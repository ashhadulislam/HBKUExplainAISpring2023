{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7521a99c",
      "metadata": {
        "id": "7521a99c"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from scipy.stats import entropy\n",
        "\n",
        "\n",
        "# import some data to play with\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "     X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "\n",
        "# normalize the data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train=scaler.fit_transform(X_train)\n",
        "X_test=scaler.transform(X_test)\n",
        "# def change_weights(X_train,y_train,X_test,y_test,weights):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "957b7605",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "957b7605",
        "outputId": "a7f3642e-72a2-4045-8d7c-fa7a3b60b7c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0549201679861442\n"
          ]
        }
      ],
      "source": [
        "pk = np.array([1/5, 2/5, 2/5])  # fair coin\n",
        "H = entropy(pk)\n",
        "print(H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "47b727fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47b727fd",
        "outputId": "023f151d-55b6-4760-cd26-7ae8a1ba1d01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0.3333333333333333, 1: 0.3333333333333333, 2: 0.3333333333333333}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def calculate_probabilities(list_labels, uniq_labels):\n",
        "    '''\n",
        "    Author: Sara Nassar \n",
        "    this function calculates the probabilities of each label in the list of labels\n",
        "    it is calculated by number of labels in class A/all labels\n",
        "    number of labels in class B/all labels\n",
        "    and so on\n",
        "    '''\n",
        "    \n",
        "    # A dictionary to store the probabilities\n",
        "    probabilities = dict.fromkeys(uniq_labels, 0)\n",
        "    \n",
        "    # Total number of labels\n",
        "    total_labels = len(list_labels)\n",
        "    \n",
        "    for label in uniq_labels:\n",
        "        # Counting the number of times the label occurs in the list\n",
        "        count = list_labels.count(label)\n",
        "        \n",
        "        # Calculating the probability of the label\n",
        "        probability = count / total_labels\n",
        "        \n",
        "        # Storing the calculated probability in the dictionary\n",
        "        probabilities[label] = probability\n",
        "        \n",
        "    return probabilities    \n",
        "    \n",
        "    \n",
        "# test your function\n",
        "list_labels=[1,2,0,1,2,0]\n",
        "uniq_labels=[0,1,2]\n",
        "print(calculate_probabilities(list_labels,uniq_labels))\n",
        "# this should print somehting like 0.33,0.33,0.33\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1d72e4e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d72e4e1",
        "outputId": "b34bf869-e7d1-411f-c939-ff3eed029e5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0549201679861442\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def calc_entropy_from_probabilities(list_probas):\n",
        "    '''\n",
        "    Author: Sara Nassar \n",
        "    list_probas is the list of probabiities\n",
        "    the formula for entropy is\n",
        "    sum(-proba*log(proba))\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    entropy_value = 0\n",
        "\n",
        "    for proba in list_probas:\n",
        "        # If the probability is not zero\n",
        "        if proba != 0:\n",
        "            entropy_value += -proba * np.log(proba)\n",
        "     \n",
        "    return entropy_value\n",
        "\n",
        "\n",
        "# test your function\n",
        "list_probas=[1/5, 2/5, 2/5]\n",
        "print(calc_entropy_from_probabilities(list_probas))\n",
        "# above should print 1.054..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "48c81e54",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48c81e54",
        "outputId": "ae76ff4d-4732-4209-9f0a-94af6fe11e43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.61\n"
          ]
        }
      ],
      "source": [
        "def information_gain(old_entropy,new_entropies,count_items):\n",
        "    '''\n",
        "    Author: Sara Nassar \n",
        "    from the list of new entropies, calculate the overall new entropy\n",
        "    \n",
        "    formula is something like:\n",
        "    overall_new_entropy = entropy1*proportion1 + entropy2*proportion2+ entropy3*proportion3 ...\n",
        "    \n",
        "    igain=old_entropy-overall_new_entropy\n",
        "    '''\n",
        "    \n",
        "    overall_new_entropy = 0\n",
        "    \n",
        "    # Calculating the total number of items\n",
        "    total_items = sum(count_items)\n",
        "    \n",
        "    for i in range(len(new_entropies)):\n",
        "        # Calculating the proportion of items in the current partition\n",
        "        proportion = count_items[i] / total_items\n",
        "        \n",
        "        # Adding the entropy of the current partition weighted by its proportion to the overall new entropy\n",
        "        overall_new_entropy += new_entropies[i] * proportion\n",
        "        \n",
        "    # Calculating the information gain\n",
        "    information_gain = old_entropy - overall_new_entropy\n",
        "    \n",
        "    return information_gain\n",
        "\n",
        "#test your function\n",
        "old_entropy=1\n",
        "new_entropies=[0,0.65]\n",
        "count_items=[4,6]\n",
        "print(information_gain(old_entropy,new_entropies,count_items))\n",
        "# above should print 0.61\n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d776e0d3",
      "metadata": {
        "id": "d776e0d3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def initialize_weights(number_features):\n",
        "    '''\n",
        "    the first set of weights corresponding to the features\n",
        "    For now, it defaults to 2\n",
        "    '''\n",
        "    \n",
        "    weights=np.array([np.random.uniform() for i in range(number_features)])\n",
        "    return weights\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "abd742b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abd742b8",
        "outputId": "61767753-28fd-40a8-c3b5-c9867f1c74b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.98375591 0.16105189 0.58112152 0.29729853]\n"
          ]
        }
      ],
      "source": [
        "num_feats=X_train.shape[1]\n",
        "print(initialize_weights(num_feats))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6b92e940",
      "metadata": {
        "id": "6b92e940"
      },
      "outputs": [],
      "source": [
        "def get_entropy_from_groups(new_entropies,count_items):\n",
        "    overall_new_entropy = 0\n",
        "    \n",
        "    # Calculating the total number of items\n",
        "    total_items = sum(count_items)\n",
        "    \n",
        "    for i in range(len(new_entropies)):\n",
        "        # Calculating the proportion of items in the current partition\n",
        "        proportion = count_items[i] / total_items\n",
        "        \n",
        "        # Adding the entropy of the current partition weighted by its proportion to the overall new entropy\n",
        "        overall_new_entropy += new_entropies[i] * proportion\n",
        "        \n",
        "    return overall_new_entropy    \n",
        "\n",
        "def get_entropy(threshold,res,y_test):\n",
        "\n",
        "    # make two groups\n",
        "    group1=[]\n",
        "    group2=[]\n",
        "\n",
        "    for i in range(res.shape[0]):\n",
        "        if res[i]<threshold:\n",
        "            group1.append(y_test[i])\n",
        "        else:\n",
        "            group2.append(y_test[i])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    proba_gr1=calculate_probabilities(group1,np.unique(group1).tolist())\n",
        "    proba_gr1=list(proba_gr1.values()) \n",
        "    entropy_group1=calc_entropy_from_probabilities(proba_gr1)\n",
        "    count_group1=len(proba_gr1)\n",
        "\n",
        "    proba_gr2=calculate_probabilities(group2,np.unique(group2).tolist())\n",
        "    proba_gr2=list(proba_gr2.values()) \n",
        "    entropy_group2=calc_entropy_from_probabilities(proba_gr2)\n",
        "    count_group2=len(proba_gr2)\n",
        "\n",
        "    new_entropies=[entropy_group1,entropy_group2]\n",
        "    count_items=[count_group1,count_group2]\n",
        "    overall_new_entropy=get_entropy_from_groups(new_entropies,count_items)\n",
        "    return overall_new_entropy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5e48b48",
      "metadata": {
        "id": "b5e48b48"
      },
      "source": [
        "### Task4: PSO Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb62b133",
      "metadata": {
        "id": "cb62b133"
      },
      "source": [
        "#### Modified the entropy function to get a vector of entropies for n particles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "18e9e710",
      "metadata": {
        "id": "18e9e710"
      },
      "outputs": [],
      "source": [
        "def objective_fn(param1,param2,X,y):\n",
        "    '''\n",
        "    param1 and param2 are the parameters that we want to optimize\n",
        "    say param1 is the weight vector and  param2 is the threshold\n",
        "    '''\n",
        "    # Authon Adnan\n",
        "    # multiply the weights with each feature and calculate the sum\n",
        "    res=np.sum(X * param1, axis=1)  \n",
        "#     print(res)\n",
        "    #calculate entropy: hint: use the get_entropy function\n",
        "    entropy= get_entropy(param2,res,y)\n",
        "    # you only need to pass the threshold, res vector and the y\n",
        "    return entropy\n",
        "    \n",
        "    \n",
        "def objective_fn_vector(params1,params2,X,y):\n",
        "    '''\n",
        "    params1 is an array of weight vectors\n",
        "    params2 is an array of thresholds\n",
        "    '''\n",
        "    results=[]\n",
        "    for i in range(params1.shape[0]):\n",
        "        param1=params1[i]\n",
        "        param2=params2[i]\n",
        "        # call the objective_fn above to get the entropy\n",
        "        res=objective_fn(param1,param2,X,y)\n",
        "#         print(param2,res)\n",
        "        results.append(res)\n",
        "    \n",
        "    return np.array(results)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b8571ba0",
      "metadata": {
        "id": "b8571ba0"
      },
      "outputs": [],
      "source": [
        "### Below we just randomly assign 100 particles and see if we can find the global minimum.\n",
        "### THis is just to check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "dcff4c71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcff4c71",
        "outputId": "19c8bb99-0aa2-435f-e471-f12b12d68d70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of params 1 (weights) (100, 4)\n",
            "Shape of params 2 (thresholds) (100,)\n"
          ]
        }
      ],
      "source": [
        "# Authon Adnan\n",
        "params1=initialize_weights(400).reshape(100,4)\n",
        "# call the initialize_weights function above\n",
        "\n",
        "params2=np.random.uniform(low=0, high=1, size=100)\n",
        "# use the np.random.uniform() function\n",
        "\n",
        "# we have a list of 100 weight vectors (params1) and 100 thresholds (params2)\n",
        "# convert them to array\n",
        "params1=np.array(params1)\n",
        "params2=np.array(params2)\n",
        "\n",
        "print(\"Shape of params 1 (weights)\",params1.shape)\n",
        "print(\"Shape of params 2 (thresholds)\",params2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4656f045",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4656f045",
        "outputId": "e2e6963e-a770-481c-ff66-d6137a90f760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "param1_min [0.44601051 0.04738174 0.63234538 0.89406097] param2_min 0.6863086208482562\n"
          ]
        }
      ],
      "source": [
        "# Authon Adnan\n",
        "z = objective_fn_vector(params1, params2, X_train, y_train)\n",
        "# Find the global minimum that is using the minimum if params1 and params2\n",
        "param1_min =  params1[z.argmin()]# use z.argmin() to access best params1\n",
        "param2_min = params2[z.argmin()]# use z.argmin() to access best params2\n",
        "\n",
        "print(\"param1_min\",param1_min,\"param2_min\",param2_min)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "21552c35",
      "metadata": {
        "id": "21552c35"
      },
      "outputs": [],
      "source": [
        "### Setting up the particles and other parameters now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "57902fa4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57902fa4",
        "outputId": "1d0fb487-85e4-46f8-8155-6ace939725b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params1 shape is  (20, 4) params2 shape is  (20,)\n"
          ]
        }
      ],
      "source": [
        "# Authon Adnan\n",
        "# Hyper-parameter of the algorithm\n",
        "c1 = c2 = 0.1\n",
        "w1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\n",
        "w2 = 0.8 \n",
        "# Create particles\n",
        "n_particles = 20\n",
        "np.random.seed(100)\n",
        "params1= initialize_weights(n_particles*4).reshape(n_particles,4)# a vector of shape n_particles,4\n",
        "# call the initialize_weights function above\n",
        "\n",
        "params2=np.random.uniform(low=0, high=1, size=n_particles)# a vector of shape n_particles\n",
        "# use the np.random.uniform() function\n",
        "\n",
        "params1=np.array(params1)\n",
        "params2=np.array(params2)\n",
        "\n",
        "print(\"params1 shape is \",params1.shape,\"params2 shape is \",params2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1afe28a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1afe28a7",
        "outputId": "c620d632-144d-4116-ec36-b903b5d4ad94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pbest obj value for 20 particles are as follows [0.67013703 0.82232957 0.64329013 0.70573338 0.73886477 0.82232957\n",
            " 1.09729975 0.5237323  0.77244152 0.4620281  0.86703698 0.81919055\n",
            " 1.09729975 0.81919055 0.73355763 0.74030523 0.82232957 0.73805779\n",
            " 1.09729975 0.68309963]\n",
            "gbest obj value among all 20 particles is as follows 0.4620281046196322\n"
          ]
        }
      ],
      "source": [
        "# Authon Adnan\n",
        "# define velocity of each weight of every particle\n",
        "V_param1 =  initialize_weights(n_particles*4).reshape(n_particles,4) # shape is same as params1\n",
        "# once again can use initialize_weights function\n",
        "\n",
        "#define velocity of each threshold of every particle\n",
        "V_param2 =np.random.uniform(low=0, high=1, size=n_particles) # shape is same as params2\n",
        "# once again use np.random.uniform() function\n",
        "\n",
        "# Initialize objective values\n",
        "pbest = (params1,params2)\n",
        "pbest_obj = objective_fn_vector(params1, params2, X_train, y_train)\n",
        "gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()])\n",
        "gbest_obj = pbest_obj.min()\n",
        "\n",
        "print(\"pbest obj value for 20 particles are as follows\",pbest_obj)\n",
        "print(\"gbest obj value among all 20 particles is as follows\",gbest_obj)\n",
        "# note that gbest_obj should be the minimim of all pbest_obj"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96fabbf7",
      "metadata": {
        "id": "96fabbf7"
      },
      "source": [
        "### the update function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e4ff1c7b",
      "metadata": {
        "id": "e4ff1c7b"
      },
      "outputs": [],
      "source": [
        "def update():\n",
        "    \"Function to do one iteration of particle swarm optimization\"\n",
        "    # Authon Adnan\n",
        "    global V_param1,V_param2, params1,params2, pbest, pbest_obj, gbest, gbest_obj\n",
        "    # these have been already initialized in the previous cells\n",
        "    \n",
        "    # Update params\n",
        "    r11,r12, r2 = np.random.rand(3)\n",
        "    V_param1=w1*V_param1+c1*r11*(pbest[0] - params1)+ c2*r2*(gbest[0]-params1)\n",
        "    V_param2=w2*V_param2+c1*r12*(pbest[1] - params2)+ c2*r2*(gbest[1]-params2)    \n",
        "#     V = w * V + c1*r11*(pbest - params1) + c2*r2*(gbest.reshape(-1,1)-X)\n",
        "    params1 = params1 + V_param1\n",
        "    params2 = params2 + V_param2\n",
        "    \n",
        "    obj = objective_fn_vector(params1, params2, X_train, y_train)\n",
        "    for i in range(pbest[0].shape[0]):\n",
        "        if pbest_obj[i]>=obj[i]:\n",
        "            # update the three lines below\n",
        "            pbest[0][i]= params1[i]# # update pbest[0][i] with value of params1[i]\n",
        "            pbest[1][i]= params2[i]# update pbest[1][i] \n",
        "            pbest_obj[i]= obj[i] # also update pbest_obj[i]\n",
        "\n",
        "            \n",
        "    gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()]) # update gbest to contain the best from params1 and params 2\n",
        "    gbest_obj = pbest_obj.min() # update gbest to get the minimum of pbest_obj\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "dd1fa6d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd1fa6d7",
        "outputId": "33dd4630-d267-4a6b-eaf4-5589ea115a63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PSO found best solution at f((array([0.37630232, 0.75778144, 1.95092286, 0.57775224]), 1.1529946675246525))=0.4161039895073432\n",
            "Global optimal at f([array([0.44601051, 0.04738174, 0.63234538, 0.89406097]), 0.6863086208482562])=0.46083383331270966\n"
          ]
        }
      ],
      "source": [
        "for i in range(100):\n",
        "    update()\n",
        "print(\"PSO found best solution at f({})={}\".format(gbest, gbest_obj))\n",
        "print(\"Global optimal at f({})={}\".format([param1_min,param2_min], objective_fn(param1_min, param2_min, X_train, y_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8f66ba78",
      "metadata": {
        "id": "8f66ba78"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}