{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f354de5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlib\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# import some data to play with\u001b[39;00m\n\u001b[1;32m     10\u001b[0m iris \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mload_iris()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lib'"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from scipy.stats import entropy\n",
    "import lib\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# def change_weights(X_train,y_train,X_test,y_test,weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df837c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk = np.array([1/5, 2/5, 2/5])  # fair coin\n",
    "H = entropy(pk)\n",
    "print(H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af7deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "H=-0.2*(np.log(0.2))-0.4*(np.log(0.4))-0.4*(np.log(0.4))\n",
    "print(H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9927bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities(list_labels,uniq_labels):\n",
    "    '''\n",
    "    this function calculates the probabilities of each label in the list of labels\n",
    "    it is calculated by number of labels in class A/all labels\n",
    "    number of labels in class B/all labels\n",
    "    and so on\n",
    "    '''\n",
    "    \n",
    "    # put your code here\n",
    "    probabilities = {}\n",
    "    total = len(list_labels)\n",
    "    for label in uniq_labels:\n",
    "        count = list_labels.count(label)\n",
    "        probability = count / total\n",
    "        probabilities[label] = probability\n",
    "    return probabilities\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# test your function\n",
    "list_labels=[1,2,0,1,2,0]\n",
    "uniq_labels=[0,1,2]\n",
    "print(calculate_probabilities(list_labels,uniq_labels))\n",
    "# this should print somehting like 0.33,0.33,0.33\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df19ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def calc_entropy_from_probabilities(list_probas):\n",
    "    '''\n",
    "    list_probas is the list of probabiities\n",
    "    the formula for entropy is\n",
    "    sum(-proba*log(proba))\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # for now returning the same list\n",
    "    # put your code to calculate entropy\n",
    "    entropy_value = 0\n",
    "    for proba in list_probas:\n",
    "        if proba > 0:\n",
    "            entropy_value += -proba * np.log(proba)\n",
    "    return entropy_value\n",
    "    return list_proba\n",
    "\n",
    "# test your function\n",
    "list_probas=[1/5, 2/5, 2/5]\n",
    "print(calc_entropy_from_probabilities(list_probas))\n",
    "# above should print 1.054...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd0d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(old_entropy,new_entropies,count_items):\n",
    "    '''\n",
    "    from the list of new entropies, calculate the overall new entropy\n",
    "    \n",
    "    formula is something like:\n",
    "    overall_new_entropy = entropy1*proportion1 + entropy2*proportion2+ entropy3*proportion3 ...\n",
    "    \n",
    "    igain=old_entropy-overall_new_entropy\n",
    "    '''\n",
    "    \n",
    "    overall_entropy = 0 \n",
    "    numberOfItems = sum (count_items) # will be 10 (4+6)\n",
    "    for i in range(len(new_entropies)):\n",
    "        ratios = count_items[i]/numberOfItems\n",
    "        overall_entropy += new_entropies[i] * ratios\n",
    "        \n",
    "    \n",
    "    \n",
    "    igain = old_entropy - overall_entropy    \n",
    "    return igain\n",
    "\n",
    "\n",
    "#test your function\n",
    "old_entropy=1\n",
    "new_entropies=[0,0.65]\n",
    "count_items=[4,6]\n",
    "print(information_gain(old_entropy,new_entropies,count_items))\n",
    "# above should print 0.61\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804813f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats=X_train.shape[1]\n",
    "def initialize_weights(number_features):\n",
    "    '''\n",
    "    the first set of weights corresponding to the features\n",
    "    defaults to 1\n",
    "    '''\n",
    "    \n",
    "    weights=np.array([2 for i in range(number_features)])\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a26c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(initialize_weights(num_feats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b315a52a",
   "metadata": {},
   "source": [
    "# Task 2: Iteration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2119e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    " # initialization\n",
    "#Author: Abdul-Rahman Abdel-Fattah\n",
    "# step 1 calculate the probabilities of 0, 1 and 2 in the y_test array\n",
    "proba_init= calculate_probabilities(y_test.tolist(),np.unique(y_test).tolist())        #get the probabilities for y_test\n",
    "print(\"Initial proba=\",proba_init)\n",
    "\n",
    "# step 2 calculate the initial entropy of y_test, using the probability values\n",
    "# you might have to convert the dictionary to a list\n",
    "# get only the probability values\n",
    "list_probas= list(proba_init.values()) #get list from dictionary proba_init\n",
    "print(list_probas)\n",
    "entropy_init= calc_entropy_from_probabilities(list_probas)\n",
    "print(\"Initial entropy = \",entropy_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8180adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author: Abdul-Rahman Abdel-Fattah\n",
    "wt_init=initialize_weights(num_feats)\n",
    "# right now the initialize_weights function only returns 2,2,2 \n",
    "print(wt_init)\n",
    "\n",
    "# multiply the weights with each feature and calculate the sum\n",
    "res= np.sum( wt_init * X_test , axis =1)# use np.sum() function\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca063085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy_from_groups(new_entropies,count_items):\n",
    "    overall_new_entropy = 0\n",
    "    \n",
    "    # Calculating the total number of items\n",
    "    total_items = sum(count_items)\n",
    "    \n",
    "    for i in range(len(new_entropies)):\n",
    "        # Calculating the proportion of items in the current partition\n",
    "        proportion = count_items[i] / total_items\n",
    "        \n",
    "        # Adding the entropy of the current partition weighted by its proportion to the overall new entropy\n",
    "        overall_new_entropy += new_entropies[i] * proportion\n",
    "        \n",
    "    return overall_new_entropy    \n",
    "\n",
    "def get_entropy(threshold,res,y_test):\n",
    "\n",
    "    # make two groups\n",
    "    group1=[]\n",
    "    group2=[]\n",
    "\n",
    "    for i in range(res.shape[0]):\n",
    "        if res[i]<threshold:\n",
    "            group1.append(y_test[i])\n",
    "        else:\n",
    "            group2.append(y_test[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    proba_gr1=calculate_probabilities(group1,np.unique(group1).tolist())\n",
    "    proba_gr1=list(proba_gr1.values()) \n",
    "    entropy_group1=calc_entropy_from_probabilities(proba_gr1)\n",
    "    count_group1=len(proba_gr1)\n",
    "\n",
    "    proba_gr2=calculate_probabilities(group2,np.unique(group2).tolist())\n",
    "    proba_gr2=list(proba_gr2.values()) \n",
    "    entropy_group2=calc_entropy_from_probabilities(proba_gr2)\n",
    "    count_group2=len(proba_gr2)\n",
    "\n",
    "    new_entropies=[entropy_group1,entropy_group2]\n",
    "    count_items=[count_group1,count_group2]\n",
    "    overall_new_entropy=get_entropy_from_groups(new_entropies,count_items)\n",
    "    return overall_new_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2340d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*#Author: Abdul-Rahman Abdel-Fattah\n",
    "##generating the probabilities of group 1\n",
    "## calculate entropy of \n",
    "##proba_gr1=calculate_probabilities(group1,np.unique(group1).tolist())\n",
    "##proba_gr1=list(proba_gr1.values()) \n",
    "##entropy_group1=calc_entropy_from_probabilities(proba_gr1)\n",
    "##count_group1=len(proba_gr1)\n",
    "\n",
    "#proba_gr2=calculate_probabilities(group2,np.unique(group2).tolist())\n",
    "#proba_gr2=list(proba_gr2.values()) \n",
    "#entropy_group2=calc_entropy_from_probabilities(proba_gr2)\n",
    "#count_group2=len(proba_gr2)\n",
    "\n",
    "#new_entropies=[entropy_group1,entropy_group2]\n",
    "#count_items=[count_group1,count_group2]\n",
    "#overall_new_entropy=get_entropy_from_groups(new_entropies,count_items)\n",
    "#return overall_new_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2ef314",
   "metadata": {},
   "source": [
    "**Task 4** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c1731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_fn(param1,param2,X,y):\n",
    "    '''\n",
    "    param1 and param2 are the parameters that we want to optimize\n",
    "    say param1 is the weight vector and  param2 is the threshold\n",
    "    '''\n",
    "    # multiply the weights with each feature and calculate the sum\n",
    "    res=np.sum(X * param1, axis=1)  \n",
    "#     print(res)\n",
    "    #calculate entropy: hint: use the get_entropy function\n",
    "    entropy= get_entropy(param2, res, y)\n",
    "    # call the get_entropy function with the correct parameters.\n",
    "    # you only need to pass the threshold, res vector and the y\n",
    "    return entropy\n",
    "def objective_fn_vector(params1,params2,X,y):\n",
    "    '''\n",
    "    params1 is an array of weight vectors\n",
    "    params2 is an array of thresholds\n",
    "    '''\n",
    "    results=[]\n",
    "    for i in range(params1.shape[0]):\n",
    "        param1 = params1[i]\n",
    "        param2 = params2[i]\n",
    "        # call the objective_fn above to get the entropy\n",
    "        res=objective_fn(param1,param2,X,y)\n",
    "#         print(param2,res)\n",
    "        results.append(res)\n",
    "    \n",
    "    return np.array(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2331562",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vectors = 100\n",
    "n_features = 4\n",
    "params1 = [initialize_weights(n_features) for i in range(n_vectors)]\n",
    "\n",
    "params2= np.random.uniform(low=-1, high=1, size=n_vectors)\n",
    "# a vector of shape 100\n",
    "# use the np.random.uniform() function\n",
    "\n",
    "# we have a list of 100 weight vectors (params1) and 100 thresholds (params2)\n",
    "# convert them to array\n",
    "params1=np.array(params1)\n",
    "params2=np.array(params2)\n",
    "\n",
    "print(\"Shape of params 1 (weights)\",params1.shape)\n",
    "print(\"Shape of params 2 (thresholds)\",params2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0676a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = objective_fn_vector(params1, params2, X_train, y_train)\n",
    "# Find the global minimum that is using the minimum if params1 and params2\n",
    "min_idx = np.argmin(z)\n",
    "\n",
    "# Get the corresponding values of params1 and params2\n",
    "param1_min = params1[min_idx]\n",
    "param2_min = params2[min_idx]\n",
    "\n",
    "print(\"param1_min:\", param1_min)\n",
    "print(\"param2_min:\", param2_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25006bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter of the algorithm\n",
    "c1 = c2 = 0.1\n",
    "w1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\n",
    "w2 = 0.8 \n",
    "# Create particles\n",
    "n_particles = 20\n",
    "np.random.seed(100)\n",
    "params1 = np.array([initialize_weights(X_train.shape[1]) for i in range(n_particles)])\n",
    "\n",
    "# Initialize threshold values\n",
    "params2 = np.random.uniform(size=n_particles)\n",
    "\n",
    "params1=np.array(params1)\n",
    "params2=np.array(params2)\n",
    "\n",
    "print(\"params1 shape is \",params1.shape,\"params2 shape is \",params2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7459f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_param1 = np.array([initialize_weights(X_train.shape[1]) for i in range(n_particles)])\n",
    "\n",
    "#define velocity of each threshold of every particle\n",
    "V_param2 = np.random.uniform(size=n_particles)\n",
    "\n",
    "# Initialize objective values\n",
    "pbest = (params1,params2)\n",
    "pbest_obj = objective_fn_vector(params1, params2, X_train, y_train)\n",
    "gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()])\n",
    "gbest_obj = pbest_obj.min()\n",
    "\n",
    "print(\"pbest obj value for 20 particles are as follows\",pbest_obj)\n",
    "print(\"gbest obj value among all 20 particles is as follows\",gbest_obj)\n",
    "# note that gbest_obj should be the minimim of all pbest_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e46a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    \"Function to do one iteration of particle swarm optimization\"\n",
    "    global V_param1,V_param2, params1,params2, pbest, pbest_obj, gbest, gbest_obj\n",
    "    # these have been already initialized in the previous cells\n",
    "    \n",
    "    # Update params\n",
    "    r11,r12, r2 = np.random.rand(3)\n",
    "    V_param1=w1*V_param1+c1*r11*(pbest[0] - params1)+ c2*r2*(gbest[0]-params1)\n",
    "    V_param2=w2*V_param2+c1*r12*(pbest[1] - params2)+ c2*r2*(gbest[1]-params2)    \n",
    "#     V = w * V + c1*r11*(pbest - params1) + c2*r2*(gbest.reshape(-1,1)-X)\n",
    "    params1 = params1 + V_param1\n",
    "    params2 = params2 + V_param2\n",
    "    \n",
    "    obj = objective_fn_vector(params1, params2, X_train, y_train)\n",
    "    for i in range(pbest[0].shape[0]):\n",
    "        if pbest_obj[i]>=obj[i]:\n",
    "            # update the three lines below\n",
    "            pbest[0][i] = params1[i] # update pbest[0][i] with value of params1[i]\n",
    "            pbest[1][i] = params2[i] # update pbest[1][i] with value of params2[i]\n",
    "            pbest_obj[i] = obj[i] \n",
    "\n",
    "            \n",
    "    gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()]) # update gbest to contain the best from params1 and params 2\n",
    "    gbest_obj = pbest_obj.min() # update gbest to get the minimum of pbest_obj\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec83adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    update()\n",
    "print(\"PSO found best solution at f({})={}\".format(gbest, gbest_obj))\n",
    "print(\"Global optimal at f({})={}\".format([param1_min,param2_min], objective_fn(param1_min, param2_min, X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56fd626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "bcan = sklearn.datasets.load_breast_cancer()\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "print(\"Shape of training data is \",X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0087e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "params1=[lib.initialize_weights(X_train.shape[1]) for i in range(100)]# a vector of shape 100,4\n",
    "# call the initialize_weights function above\n",
    "\n",
    "params2=[np.random.uniform() for i in range(100)]# a vector of shape 100\n",
    "# use the np.random.uniform() function\n",
    "# initialize the params1 and params2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa1792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params1=np.array(params1)\n",
    "params2=np.array(params2)\n",
    "\n",
    "print(\"Shape of params 1 (weights)\",params1.shape)\n",
    "print(\"Shape of params 2 (thresholds)\",params2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aac611",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = lib.objective_fn_vector(params1, params2, X_train, y_train)\n",
    "# Find the global minimum\n",
    "param1_min = params1[z.argmin()] # use z.argmin()\n",
    "param2_min = params2[z.argmin()] # use z.argmin()\n",
    "\n",
    "print(\"param1_min\",param1_min,\"param2_min\",param2_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81000e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter of the algorithm\n",
    "c1 = c2 = 0.1\n",
    "w1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\n",
    "w2 = 0.8 \n",
    "# Create particles\n",
    "n_particles = 20\n",
    "np.random.seed(100)\n",
    "params1=[lib.initialize_weights(X_train.shape[1]) for i in range(n_particles)] # a vector of shape n_particles,4\n",
    "# call the initialize_weights function above\n",
    "\n",
    "params2=[np.random.uniform() for i in range(n_particles)]# a vector of shape n_particles\n",
    "# use the np.random.uniform() function\n",
    "\n",
    "params1=np.array(params1)\n",
    "params2=np.array(params2)\n",
    "\n",
    "print(\"params1 shape is \",params1.shape,\"params2 shape is \",params2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define velocity of each weight of every particle\n",
    "V_param1 = [lib.initialize_weights(X_train.shape[1])*0.1 for i in range(n_particles)] # shape is same as params1\n",
    "# once again can use initialize_weights function\n",
    "\n",
    "#define velocity of each threshold of every particle\n",
    "V_param2 = np.array([np.random.uniform()*0.1 for i in range(n_particles)])# shape is same as params2\n",
    "# once again use np.random.uniform() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044aff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbest = (params1,params2)\n",
    "pbest_obj = lib.objective_fn_vector(params1, params2, X_train, y_train)\n",
    "gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()])\n",
    "gbest_obj = pbest_obj.min()\n",
    "\n",
    "print(\"pbest obj value for 20 particles are as follows\",pbest_obj)\n",
    "print(\"gbest obj value among all 20 particles is as follows\",gbest_obj)\n",
    "# note that gbest_obj should be the minimim of all pbest_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    \"Function to do one iteration of particle swarm optimization\"\n",
    "    global V_param1,V_param2, params1,params2, pbest, pbest_obj, gbest, gbest_obj\n",
    "    # these have been already initialized in the previous cells\n",
    "    \n",
    "    # Update params\n",
    "    r11,r12, r2 = np.random.rand(3)\n",
    "    V_param1=w1*V_param1+c1*r11*(pbest[0] - params1)+ c2*r2*(gbest[0]-params1)\n",
    "    V_param2=w2*V_param2+c1*r12*(pbest[1] - params2)+ c2*r2*(gbest[1]-params2)    \n",
    "#     V = w * V + c1*r11*(pbest - params1) + c2*r2*(gbest.reshape(-1,1)-X)\n",
    "    params1 = params1 + V_param1\n",
    "    params2 = params2 + V_param2\n",
    "    \n",
    "    obj = lib.objective_fn_vector(params1, params2, X_train, y_train)\n",
    "    for i in range(pbest[0].shape[0]):\n",
    "        if pbest_obj[i]>=obj[i]:\n",
    "            \n",
    "            pbest[0][i]=params1[i] # update pbest[0][i] with value of params1[i]\n",
    "            pbest[1][i]=params2[i] # update pbest[1][i] \n",
    "            pbest_obj[i]=obj[i]    # also update pbest_obj[i]\n",
    "\n",
    "            \n",
    "    gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()]) # update gbest to contain the best from params1 and params 2\n",
    "    gbest_obj = pbest_obj.min() # update gbest to get the minimum of pbest_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449ce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    update()\n",
    "print(\"PSO found best solution at f({})={}\".format(gbest, gbest_obj))\n",
    "print(\"Global optimal at f({})={}\".format([param1_min,param2_min], lib.objective_fn(param1_min, param2_min, X_train, y_train)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
