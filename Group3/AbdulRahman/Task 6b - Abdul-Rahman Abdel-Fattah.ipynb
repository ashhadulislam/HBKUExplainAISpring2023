{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebc3cf8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:50.522222Z",
     "iopub.status.busy": "2023-03-30T15:51:50.521749Z",
     "iopub.status.idle": "2023-03-30T15:51:50.529342Z",
     "shell.execute_reply": "2023-03-30T15:51:50.528148Z",
     "shell.execute_reply.started": "2023-03-30T15:51:50.522166Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import entropy\n",
    "from sklearn.datasets import load_breast_cancer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "669f5e65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:50.531836Z",
     "iopub.status.busy": "2023-03-30T15:51:50.531487Z",
     "iopub.status.idle": "2023-03-30T15:51:50.542886Z",
     "shell.execute_reply": "2023-03-30T15:51:50.541758Z",
     "shell.execute_reply.started": "2023-03-30T15:51:50.531796Z"
    }
   },
   "outputs": [],
   "source": [
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "# def change_weights(X_train,y_train,X_test,y_test,weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1306faa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:50.544655Z",
     "iopub.status.busy": "2023-03-30T15:51:50.544294Z",
     "iopub.status.idle": "2023-03-30T15:51:50.551924Z",
     "shell.execute_reply": "2023-03-30T15:51:50.550395Z",
     "shell.execute_reply.started": "2023-03-30T15:51:50.544621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0549201679861442\n"
     ]
    }
   ],
   "source": [
    "pk = np.array([1/5, 2/5, 2/5])  # fair coin\n",
    "H = entropy(pk)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb88a76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:50.555274Z",
     "iopub.status.busy": "2023-03-30T15:51:50.554801Z",
     "iopub.status.idle": "2023-03-30T15:51:50.565086Z",
     "shell.execute_reply": "2023-03-30T15:51:50.563520Z",
     "shell.execute_reply.started": "2023-03-30T15:51:50.555236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.3333333333333333, 1: 0.3333333333333333, 2: 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_probabilities(list_labels, uniq_labels):\n",
    "    '''\n",
    "   this function calculates the probabilities of each label in the list of labels\n",
    "    it is calculated by number of labels in class A/all labels\n",
    "    number of labels in class B/all labels\n",
    "    and so on\n",
    "    '''\n",
    "    \n",
    "    # A dictionary to store the probabilities\n",
    "    probabilities = dict.fromkeys(uniq_labels, 0)\n",
    "    \n",
    "    # Total number of labels\n",
    "    total_labels = len(list_labels)\n",
    "    \n",
    "    for label in uniq_labels:\n",
    "        # Counting the number of times the label occurs in the list\n",
    "        count = list_labels.count(label)\n",
    "        \n",
    "        # Calculating the probability of the label\n",
    "        probability = count / total_labels\n",
    "        \n",
    "        # Storing the calculated probability in the dictionary\n",
    "        probabilities[label] = probability\n",
    "        \n",
    "    return probabilities    \n",
    "    \n",
    "    \n",
    "# test your function\n",
    "list_labels=[1,2,0,1,2,0]\n",
    "uniq_labels=[0,1,2]\n",
    "print(calculate_probabilities(list_labels,uniq_labels))\n",
    "# this should print somehting like 0.33,0.33,0.33\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "350f6e2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:50.567035Z",
     "iopub.status.busy": "2023-03-30T15:51:50.566377Z",
     "iopub.status.idle": "2023-03-30T15:51:50.575572Z",
     "shell.execute_reply": "2023-03-30T15:51:50.574321Z",
     "shell.execute_reply.started": "2023-03-30T15:51:50.566989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0549201679861442\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calc_entropy_from_probabilities(list_probas):\n",
    "    '''\n",
    "    list_probas is the list of probabiities\n",
    "    the formula for entropy is\n",
    "    sum(-proba*log(proba))\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    entropy_value = 0\n",
    "\n",
    "    for proba in list_probas:\n",
    "        # If the probability is not zero\n",
    "        if proba != 0:\n",
    "            entropy_value += -proba * np.log(proba)\n",
    "     \n",
    "    return entropy_value\n",
    "\n",
    "\n",
    "# test your function\n",
    "list_probas=[1/5, 2/5, 2/5]\n",
    "print(calc_entropy_from_probabilities(list_probas))\n",
    "# above should print 1.054..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1b769dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:50.578786Z",
     "iopub.status.busy": "2023-03-30T15:51:50.578313Z",
     "iopub.status.idle": "2023-03-30T15:51:50.590342Z",
     "shell.execute_reply": "2023-03-30T15:51:50.589506Z",
     "shell.execute_reply.started": "2023-03-30T15:51:50.578741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61\n"
     ]
    }
   ],
   "source": [
    "def information_gain(old_entropy, new_entropies, count_items):\n",
    "    '''\n",
    "   \n",
    "    from the list of new entropies, calculate the overall new entropy\n",
    "    \n",
    "    formula is something like:\n",
    "    overall_new_entropy = entropy1*proportion1 + entropy2*proportion2+ entropy3*proportion3 ...\n",
    "    \n",
    "    igain=old_entropy-overall_new_entropy\n",
    "    '''\n",
    "    \n",
    "    overall_new_entropy = 0\n",
    "    \n",
    "    # Calculating the total number of items\n",
    "    total_items = sum(count_items.values())\n",
    "    \n",
    "    for i in range(len(new_entropies)):\n",
    "        if i in count_items:\n",
    "            # Calculating the proportion of items in the current partition\n",
    "            proportion = count_items[i] / total_items\n",
    "        \n",
    "            # Adding the entropy of the current partition weighted by its proportion to the overall new entropy\n",
    "            overall_new_entropy += new_entropies[i] * proportion\n",
    "        \n",
    "    # Calculating the information gain\n",
    "    information_gain = old_entropy - overall_new_entropy\n",
    "    \n",
    "    return information_gain\n",
    "\n",
    "\n",
    "old_entropy=1\n",
    "new_entropies=[0,0.65]\n",
    "count_items=[4,6]\n",
    "count_items = {i: count_items[i] for i in range(len(count_items))}\n",
    "print(information_gain(old_entropy,new_entropies,count_items)) # should print 0.61\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "097f663e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:50.592640Z",
     "iopub.status.busy": "2023-03-30T15:51:50.592003Z",
     "iopub.status.idle": "2023-03-30T15:51:50.601069Z",
     "shell.execute_reply": "2023-03-30T15:51:50.599811Z",
     "shell.execute_reply.started": "2023-03-30T15:51:50.592599Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def initialize_weights(number_features):\n",
    "    '''\n",
    "    the first set of weights corresponding to the features\n",
    "    For now, it defaults to 2\n",
    "    '''\n",
    "    \n",
    "    weights=np.array([np.random.uniform() for i in range(number_features)])\n",
    "    return weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12587128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:50.604364Z",
     "iopub.status.busy": "2023-03-30T15:51:50.603159Z",
     "iopub.status.idle": "2023-03-30T15:51:50.613278Z",
     "shell.execute_reply": "2023-03-30T15:51:50.612412Z",
     "shell.execute_reply.started": "2023-03-30T15:51:50.604307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25001521 0.6102131  0.59356411 0.01289052]\n"
     ]
    }
   ],
   "source": [
    "num_feats=X_train.shape[1]\n",
    "print(initialize_weights(num_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd95de91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:50.615409Z",
     "iopub.status.busy": "2023-03-30T15:51:50.615073Z",
     "iopub.status.idle": "2023-03-30T15:51:50.625948Z",
     "shell.execute_reply": "2023-03-30T15:51:50.625050Z",
     "shell.execute_reply.started": "2023-03-30T15:51:50.615366Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_entropy_from_groups(new_entropies,count_items):\n",
    "    overall_new_entropy = 0\n",
    "    \n",
    "    # Calculating the total number of items\n",
    "    total_items = sum(count_items)\n",
    "    \n",
    "    for i in range(len(new_entropies)):\n",
    "        # Calculating the proportion of items in the current partition\n",
    "        proportion = count_items[i] / total_items\n",
    "        \n",
    "        # Adding the entropy of the current partition weighted by its proportion to the overall new entropy\n",
    "        overall_new_entropy += new_entropies[i] * proportion\n",
    "        \n",
    "    return overall_new_entropy    \n",
    "\n",
    "def get_entropy(threshold,res,y_test):\n",
    "\n",
    "    # make two groups\n",
    "    group1=[]\n",
    "    group2=[]\n",
    "\n",
    "    for i in range(res.shape[0]):\n",
    "        if res[i]<threshold:\n",
    "            group1.append(y_test[i])\n",
    "        else:\n",
    "            group2.append(y_test[i])\n",
    "\n",
    "\n",
    "    proba_gr1=calculate_probabilities(group1,np.unique(group1).tolist())\n",
    "    proba_gr1=list(proba_gr1.values()) \n",
    "    entropy_group1=calc_entropy_from_probabilities(proba_gr1)\n",
    "    count_group1=len(proba_gr1)\n",
    "\n",
    "    proba_gr2=calculate_probabilities(group2,np.unique(group2).tolist())\n",
    "    proba_gr2=list(proba_gr2.values()) \n",
    "    entropy_group2=calc_entropy_from_probabilities(proba_gr2)\n",
    "    count_group2=len(proba_gr2)\n",
    "\n",
    "    new_entropies=[entropy_group1,entropy_group2]\n",
    "    count_items=[count_group1,count_group2]\n",
    "    overall_new_entropy=get_entropy_from_groups(new_entropies,count_items)\n",
    "    return overall_new_entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95755c2",
   "metadata": {},
   "source": [
    "### Task4: PSO Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13813b3b",
   "metadata": {},
   "source": [
    "#### Modified the entropy function to get a vector of entropies for n particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af36ec4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:50.628130Z",
     "iopub.status.busy": "2023-03-30T15:51:50.627014Z",
     "iopub.status.idle": "2023-03-30T15:51:50.635662Z",
     "shell.execute_reply": "2023-03-30T15:51:50.634444Z",
     "shell.execute_reply.started": "2023-03-30T15:51:50.628070Z"
    }
   },
   "outputs": [],
   "source": [
    " def objective_fn(param1, param2, X, y):\n",
    "    '''\n",
    "    param1 and param2 are the parameters that we want to optimize\n",
    "    say param1 is the weight vector and  param2 is the threshold\n",
    "    '''\n",
    "    # Multiply the weights with each feature and calculate the sum\n",
    "    res = np.sum(X * param1, axis=1)\n",
    "    \n",
    "    # Calculate entropy using the get_entropy function\n",
    "    entropy = get_entropy(param2, res, y)\n",
    "    \n",
    "    return entropy   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d085ee7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:50.639233Z",
     "iopub.status.busy": "2023-03-30T15:51:50.638890Z",
     "iopub.status.idle": "2023-03-30T15:51:50.646794Z",
     "shell.execute_reply": "2023-03-30T15:51:50.645609Z",
     "shell.execute_reply.started": "2023-03-30T15:51:50.639201Z"
    }
   },
   "outputs": [],
   "source": [
    "def objective_fn_vector(params1, params2, X, y):\n",
    "    '''\n",
    "    params1 is an array of weight vectors\n",
    "    params2 is an array of thresholds\n",
    "    '''\n",
    "    results = []\n",
    "    for i in range(params1.shape[0]):\n",
    "        param1 = params1[i] # get ith set of weights\n",
    "        param2 = params2[i] # get ith threshold\n",
    "        # call the objective_fn above to get the entropy\n",
    "        res = objective_fn(param1, param2, X, y)\n",
    "        #print(param2,res)\n",
    "        results.append(res)\n",
    "    \n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4d0c3a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:50.649586Z",
     "iopub.status.busy": "2023-03-30T15:51:50.648462Z",
     "iopub.status.idle": "2023-03-30T15:51:50.661419Z",
     "shell.execute_reply": "2023-03-30T15:51:50.660167Z",
     "shell.execute_reply.started": "2023-03-30T15:51:50.649544Z"
    }
   },
   "outputs": [],
   "source": [
    "### Below we just randomly assign 100 particles and see if we can find the global minimum.\n",
    "### THis is just to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0293708f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:50.663987Z",
     "iopub.status.busy": "2023-03-30T15:51:50.663338Z",
     "iopub.status.idle": "2023-03-30T15:51:50.676920Z",
     "shell.execute_reply": "2023-03-30T15:51:50.675691Z",
     "shell.execute_reply.started": "2023-03-30T15:51:50.663946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of params 1 (weights): (100, 4)\n",
      "Shape of params 2 (thresholds): (100,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Below we just randomly assign 100 particles and see if we can find the global minimum.\n",
    "'''\n",
    "num_particles = 100\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "params1 = []\n",
    "for i in range(num_particles):\n",
    "    weights = initialize_weights(num_features)\n",
    "    params1.append(weights)\n",
    "\n",
    "    \n",
    "# we have a list of 100 weight vectors (params1) and 100 thresholds (params2)\n",
    "# convert them to array\n",
    "params1 = np.array(params1)\n",
    "params2 = np.random.uniform(size=num_particles)\n",
    "\n",
    "print(\"Shape of params 1 (weights):\", params1.shape)\n",
    "print(\"Shape of params 2 (thresholds):\", params2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "283e4a03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:50.679871Z",
     "iopub.status.busy": "2023-03-30T15:51:50.678363Z",
     "iopub.status.idle": "2023-03-30T15:51:50.705036Z",
     "shell.execute_reply": "2023-03-30T15:51:50.703687Z",
     "shell.execute_reply.started": "2023-03-30T15:51:50.679825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param1_min [0.47604967 0.0841604  0.74688747 0.17380328] param2_min 0.3357010581120441\n"
     ]
    }
   ],
   "source": [
    "z = objective_fn_vector(params1, params2, X_train, y_train)\n",
    "# Find the global minimum that is using the minimum if params1 and params2\n",
    "global_min_idx = np.argmin(z)\n",
    "param1_min = params1[global_min_idx] # use z.argmin() to access best params1 (global_min_weight)\n",
    "param2_min = params2[global_min_idx] # use z.argmin() to access best params2 (global_min_threshold)\n",
    "\n",
    "print(\"param1_min\",param1_min,\"param2_min\",param2_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79dc5ee2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:50.706931Z",
     "iopub.status.busy": "2023-03-30T15:51:50.706566Z",
     "iopub.status.idle": "2023-03-30T15:51:50.716107Z",
     "shell.execute_reply": "2023-03-30T15:51:50.715026Z",
     "shell.execute_reply.started": "2023-03-30T15:51:50.706895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params1 shape is  (20, 4) params2 shape is  (20,)\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameter of the algorithm\n",
    "c1 = c2 = 0.1\n",
    "w1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\n",
    "w2 = 0.8 \n",
    "\n",
    "# Create particles\n",
    "n_particles = 20\n",
    "np.random.seed(100)\n",
    "params1 = np.array([initialize_weights(X_train.shape[1]) for i in range(n_particles)])\n",
    "params2 = np.random.uniform(size=n_particles)\n",
    "\n",
    "params1 = np.array(params1)\n",
    "params2 = np.array(params2)\n",
    "\n",
    "print(\"params1 shape is \",params1.shape,\"params2 shape is \",params2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21db38a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:50.717892Z",
     "iopub.status.busy": "2023-03-30T15:51:50.717579Z",
     "iopub.status.idle": "2023-03-30T15:51:50.735825Z",
     "shell.execute_reply": "2023-03-30T15:51:50.734629Z",
     "shell.execute_reply.started": "2023-03-30T15:51:50.717862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pbest obj value for 20 particles are as follows [0.67013703 0.82232957 0.64329013 0.70573338 0.73886477 0.82232957\n",
      " 1.09729975 0.5237323  0.77244152 0.4620281  0.86703698 0.81919055\n",
      " 1.09729975 0.81919055 0.73355763 0.74030523 0.82232957 0.73805779\n",
      " 1.09729975 0.68309963]\n",
      "gbest obj value among all 20 particles is as follows 0.4620281046196322\n"
     ]
    }
   ],
   "source": [
    "# Define velocity of each weight of every particle\n",
    "V_param1 = np.array([initialize_weights(X_train.shape[1]) for i in range(n_particles)])\n",
    "\n",
    "# Define velocity of each threshold of every particle\n",
    "V_param2 = np.random.uniform(size=n_particles)\n",
    "\n",
    "# Initialize objective values\n",
    "pbest = (np.array([param.copy() for param in params1]), params2.copy())\n",
    "pbest_obj = objective_fn_vector(params1, params2, X_train, y_train)\n",
    "gbest = (params1[pbest_obj.argmin()], params2[pbest_obj.argmin()])\n",
    "gbest_obj = pbest_obj.min()\n",
    "\n",
    "print(\"pbest obj value for 20 particles are as follows\",pbest_obj)\n",
    "print(\"gbest obj value among all 20 particles is as follows\",gbest_obj)\n",
    "# Note that gbest_obj should be the minimum of all pbest_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2c2d0f",
   "metadata": {},
   "source": [
    "### the update function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa3a63bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:50.738744Z",
     "iopub.status.busy": "2023-03-30T15:51:50.737563Z",
     "iopub.status.idle": "2023-03-30T15:51:50.750527Z",
     "shell.execute_reply": "2023-03-30T15:51:50.749228Z",
     "shell.execute_reply.started": "2023-03-30T15:51:50.738696Z"
    }
   },
   "outputs": [],
   "source": [
    "def update():\n",
    "    \"Function to do one iteration of particle swarm optimization\"\n",
    "    global V_param1, V_param2, params1, params2, pbest, pbest_obj, gbest, gbest_obj\n",
    "    # these have been already initialized in the previous cells\n",
    "    \n",
    "    # Update params\n",
    "    r11, r12, r2 = np.random.rand(3)\n",
    "    V_param1 = w1 * V_param1 + c1 * r11 * (pbest[0] - params1) + c2 * r2 * (gbest[0] - params1)\n",
    "    V_param2 = w2 * V_param2 + c1 * r12 * (pbest[1] - params2) + c2 * r2 * (gbest[1] - params2)    \n",
    "   #V = w * V + c1*r11*(pbest - params1) + c2*r2*(gbest.reshape(-1,1)-X)\n",
    "    params1 = params1 + V_param1\n",
    "    params2 = params2 + V_param2\n",
    "    \n",
    "    obj = objective_fn_vector(params1, params2, X_train, y_train)\n",
    "    for i in range(pbest[0].shape[0]):\n",
    "        if pbest_obj[i] >= obj[i]:\n",
    "            pbest[0][i] = params1[i]\n",
    "            pbest[1][i] = params2[i]\n",
    "            pbest_obj[i] = obj[i]\n",
    "            \n",
    "    gbest = (params1[pbest_obj.argmin()], params2[pbest_obj.argmin()])\n",
    "    gbest_obj = pbest_obj.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccd0ffd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:50.752663Z",
     "iopub.status.busy": "2023-03-30T15:51:50.752026Z",
     "iopub.status.idle": "2023-03-30T15:51:51.045632Z",
     "shell.execute_reply": "2023-03-30T15:51:51.044395Z",
     "shell.execute_reply.started": "2023-03-30T15:51:50.752622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSO found best solution at f((array([1.62223357, 0.17131464, 0.91164882, 0.74848758]), 0.9622442630641292))=0.4161039895073432\n",
      "Global optimal at f([array([0.55252942, 0.1611271 , 0.56854896, 0.48570987]), 0.3357010581120441])=0.7280271855450309\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    update()\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "param1_min = initialize_weights(num_features) \n",
    "\n",
    "print(\"PSO found best solution at f({})={}\".format(gbest, gbest_obj))\n",
    "print(\"Global optimal at f({})={}\".format([param1_min,param2_min], objective_fn(param1_min, param2_min, X_train, y_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8c332e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:51.048173Z",
     "iopub.status.busy": "2023-03-30T15:51:51.047335Z",
     "iopub.status.idle": "2023-03-30T15:51:51.066737Z",
     "shell.execute_reply": "2023-03-30T15:51:51.065592Z",
     "shell.execute_reply.started": "2023-03-30T15:51:51.048131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data is  (381, 30)\n"
     ]
    }
   ],
   "source": [
    "# import some data to play with\n",
    "#load the breast cancer dataset \n",
    "bcan = load_breast_cancer()\n",
    "X = bcan.data\n",
    "y = bcan.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "print(\"Shape of training data is \",X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7dd1408",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:51.069066Z",
     "iopub.status.busy": "2023-03-30T15:51:51.068596Z",
     "iopub.status.idle": "2023-03-30T15:51:51.087783Z",
     "shell.execute_reply": "2023-03-30T15:51:51.086270Z",
     "shell.execute_reply.started": "2023-03-30T15:51:51.069017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of params 1 (weights) (100, 30)\n",
      "Shape of params 2 (thresholds) (100,)\n"
     ]
    }
   ],
   "source": [
    "n_particles = 100\n",
    "params1 = []\n",
    "params2 = []\n",
    "for i in range(n_particles):\n",
    "    params1.append(initialize_weights(X_train.shape[1]))\n",
    "    params2.append(np.random.uniform())\n",
    "params1 = np.array(params1)\n",
    "params2 = np.array(params2)\n",
    "print(\"Shape of params 1 (weights)\",params1.shape)\n",
    "print(\"Shape of params 2 (thresholds)\",params2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bc8edb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:51.090622Z",
     "iopub.status.busy": "2023-03-30T15:51:51.089757Z",
     "iopub.status.idle": "2023-03-30T15:51:51.147483Z",
     "shell.execute_reply": "2023-03-30T15:51:51.146264Z",
     "shell.execute_reply.started": "2023-03-30T15:51:51.090573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param1_min [0.12751954 0.54369217 0.20049061 0.67016086 0.55811222 0.23237832\n",
      " 0.51558849 0.31499197 0.84659255 0.44737628 0.10008522 0.90159209\n",
      " 0.85608825 0.28632987 0.25901477 0.06637137 0.31763167 0.051843\n",
      " 0.94418832 0.7172173  0.5536592  0.35974477 0.1591823  0.43295804\n",
      " 0.27936218 0.96103763 0.09813216 0.40699555 0.00837645 0.56805893] param2_min 0.5766097683672401\n"
     ]
    }
   ],
   "source": [
    "z = objective_fn_vector(params1, params2, X_train, y_train)\n",
    "global_min_idx = z.argmin()\n",
    "param1_min = params1[global_min_idx]\n",
    "param2_min = params2[global_min_idx]\n",
    "global_min_obj_val = z[global_min_idx]\n",
    "print(\"param1_min\",param1_min,\"param2_min\",param2_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dacd0ea6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:51.152190Z",
     "iopub.status.busy": "2023-03-30T15:51:51.151812Z",
     "iopub.status.idle": "2023-03-30T15:51:51.162924Z",
     "shell.execute_reply": "2023-03-30T15:51:51.161721Z",
     "shell.execute_reply.started": "2023-03-30T15:51:51.152156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params1 shape is  (20, 30) params2 shape is  (20,)\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameter of the algorithm\n",
    "c1 = c2 = 0.1\n",
    "w1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\n",
    "w2 = 0.8 \n",
    "# Create particles\n",
    "n_particles = 20\n",
    "params1 = []\n",
    "params2 = []\n",
    "for i in range(n_particles):\n",
    "    params1.append(initialize_weights(X_train.shape[1]))\n",
    "    params2.append(np.random.uniform())\n",
    "\n",
    "params1=np.array(params1)\n",
    "params2=np.array(params2)\n",
    "\n",
    "print(\"params1 shape is \",params1.shape,\"params2 shape is \",params2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff128d62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:51.165307Z",
     "iopub.status.busy": "2023-03-30T15:51:51.164368Z",
     "iopub.status.idle": "2023-03-30T15:51:51.174009Z",
     "shell.execute_reply": "2023-03-30T15:51:51.172841Z",
     "shell.execute_reply.started": "2023-03-30T15:51:51.165269Z"
    }
   },
   "outputs": [],
   "source": [
    "# define velocity of each weight of every particle\n",
    "n_particles = 20\n",
    "V_param1 = []\n",
    "V_param2 = []\n",
    "for i in range(n_particles):\n",
    "    V_param1.append(initialize_weights(X_train.shape[1]))\n",
    "    V_param2.append(np.random.uniform())\n",
    "\n",
    "V_param1=np.array(V_param1)\n",
    "V_param2=np.array(V_param2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bcb2734",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:51.176121Z",
     "iopub.status.busy": "2023-03-30T15:51:51.175524Z",
     "iopub.status.idle": "2023-03-30T15:51:51.195700Z",
     "shell.execute_reply": "2023-03-30T15:51:51.194493Z",
     "shell.execute_reply.started": "2023-03-30T15:51:51.176085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pbest obj value for 20 particles are as follows [0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461\n",
      " 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461\n",
      " 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461]\n",
      "gbest obj value among all 20 particles is as follows 0.6643460978641622\n"
     ]
    }
   ],
   "source": [
    "# Initialize objective values\n",
    "pbest = (params1.copy(), params2.copy())\n",
    "pbest_obj = objective_fn_vector(params1, params2, X_train, y_train)\n",
    "\n",
    "gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()])\n",
    "gbest_obj = pbest_obj.min()\n",
    "\n",
    "print(\"pbest obj value for 20 particles are as follows\",pbest_obj)\n",
    "print(\"gbest obj value among all 20 particles is as follows\",gbest_obj)\n",
    "# note that gbest_obj should be the minimim of all pbest_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f9b4fa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:51.198151Z",
     "iopub.status.busy": "2023-03-30T15:51:51.197130Z",
     "iopub.status.idle": "2023-03-30T15:51:51.933937Z",
     "shell.execute_reply": "2023-03-30T15:51:51.933035Z",
     "shell.execute_reply.started": "2023-03-30T15:51:51.198111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSO found best solution at f((array([0.81478416, 2.16135537, 0.80949853, 3.47002409, 1.53346952,\n",
      "       3.21786451, 4.69918283, 9.56614919, 1.34898383, 1.05069426,\n",
      "       0.27373452, 0.35291515, 1.29812582, 2.00260603, 0.70941125,\n",
      "       4.12196654, 2.14378816, 1.07077156, 2.79188037, 1.16440568,\n",
      "       3.67590249, 0.20751861, 0.72134805, 0.32680045, 0.46749854,\n",
      "       0.53694585, 0.43963777, 0.399247  , 0.34559994, 1.23349266]), 1.8817097915254228))=0.6643460978641622\n",
      "Global optimal at f([array([0.12751954, 0.54369217, 0.20049061, 0.67016086, 0.55811222,\n",
      "       0.23237832, 0.51558849, 0.31499197, 0.84659255, 0.44737628,\n",
      "       0.10008522, 0.90159209, 0.85608825, 0.28632987, 0.25901477,\n",
      "       0.06637137, 0.31763167, 0.051843  , 0.94418832, 0.7172173 ,\n",
      "       0.5536592 , 0.35974477, 0.1591823 , 0.43295804, 0.27936218,\n",
      "       0.96103763, 0.09813216, 0.40699555, 0.00837645, 0.56805893]), 0.5766097683672401])=0.6643460978641622\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    update()\n",
    "print(\"PSO found best solution at f({})={}\".format(gbest, gbest_obj))\n",
    "print(\"Global optimal at f({})={}\".format([param1_min,param2_min], objective_fn(param1_min, param2_min, X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adb33df5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:51.935739Z",
     "iopub.status.busy": "2023-03-30T15:51:51.935065Z",
     "iopub.status.idle": "2023-03-30T15:51:51.942029Z",
     "shell.execute_reply": "2023-03-30T15:51:51.940610Z",
     "shell.execute_reply.started": "2023-03-30T15:51:51.935700Z"
    }
   },
   "outputs": [],
   "source": [
    "max_tree_size=128\n",
    "all_optimized_weights_list=[None for i in range(max_tree_size)]\n",
    "all_optimized_thresh_list=[None for i in range(max_tree_size)]\n",
    "all_dataset_sizes_list=[None for i in range(max_tree_size)]\n",
    "all_IG_list=[None for i in range(max_tree_size)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a7b918d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:51.943806Z",
     "iopub.status.busy": "2023-03-30T15:51:51.943379Z",
     "iopub.status.idle": "2023-03-30T15:51:51.957537Z",
     "shell.execute_reply": "2023-03-30T15:51:51.956188Z",
     "shell.execute_reply.started": "2023-03-30T15:51:51.943769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381, 30)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a47d5e0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:51.959757Z",
     "iopub.status.busy": "2023-03-30T15:51:51.959315Z",
     "iopub.status.idle": "2023-03-30T15:51:51.970758Z",
     "shell.execute_reply": "2023-03-30T15:51:51.969371Z",
     "shell.execute_reply.started": "2023-03-30T15:51:51.959705Z"
    }
   },
   "outputs": [],
   "source": [
    "def update(V_param1,V_param2, params1,params2, pbest, pbest_obj, gbest, gbest_obj):\n",
    "    \"Function to do one iteration of particle swarm optimization\"\n",
    "    # these have been already initialized in the previous cells\n",
    "    \n",
    "    # Update params\n",
    "    r11,r12, r2 = np.random.rand(3)\n",
    "    V_param1=w1*V_param1+c1*r11*(pbest[0] - params1)+ c2*r2*(gbest[0]-params1)\n",
    "    V_param2=w2*V_param2+c1*r12*(pbest[1] - params2)+ c2*r2*(gbest[1]-params2)    \n",
    "#     V = w * V + c1*r11*(pbest - params1) + c2*r2*(gbest.reshape(-1,1)-X)\n",
    "    params1 = params1 + V_param1\n",
    "    params2 = params2 + V_param2\n",
    "    \n",
    "    obj = objective_fn_vector(params1, params2, X_train, y_train)\n",
    "    for i in range(pbest[0].shape[0]):\n",
    "        if pbest_obj[i]>=obj[i]:\n",
    "            \n",
    "            pbest[0][i]=params1[i] # update pbest[0][i] with value of params1[i]\n",
    "            pbest[1][i]=params2[i] # update pbest[1][i] \n",
    "            pbest_obj[i]=obj[i]    # also update pbest_obj[i]\n",
    "\n",
    "            \n",
    "    gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()]) # update gbest to contain the best from params1 and params 2\n",
    "    gbest_obj = pbest_obj.min() # update gbest to get the minimum of pbest_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0239be2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:51.973348Z",
     "iopub.status.busy": "2023-03-30T15:51:51.972902Z",
     "iopub.status.idle": "2023-03-30T15:51:51.995462Z",
     "shell.execute_reply": "2023-03-30T15:51:51.994332Z",
     "shell.execute_reply.started": "2023-03-30T15:51:51.973308Z"
    }
   },
   "outputs": [],
   "source": [
    "max_tree_size=128\n",
    "all_optimized_weights_list=[None for i in range(max_tree_size)]\n",
    "all_optimized_thresh_list=[None for i in range(max_tree_size)]\n",
    "all_dataset_sizes_list=[None for i in range(max_tree_size)]\n",
    "all_IG_list=[None for i in range(max_tree_size)]\n",
    "\n",
    "threshold_dict = {}\n",
    "\n",
    "def find_best_params(train_x,train_y,test_x,test_y,node_number):\n",
    "      \n",
    "    '''\n",
    "    recursive function to get the best set of weights\n",
    "    '''\n",
    "    print(\"node_number\",node_number,\"data shape\",train_x.shape)\n",
    "    # exit condition 1: if the node_number is more than the maximum tree size, return\n",
    "    if node_number>=max_tree_size:\n",
    "        return all_optimized_weights_list, all_optimized_thresh_list, all_dataset_sizes_list, all_IG_list\n",
    "    # exit condition 2: if the training dataset has one or less rows, return\n",
    "    if train_x.shape[0]<=1:\n",
    "        return all_optimized_weights_list, all_optimized_thresh_list, all_dataset_sizes_list, all_IG_list\n",
    "    # exit condition 3: if the train_y has values from only one class (only 0s or only 1s and so on)\n",
    "    if len(np.unique(train_y))==1:\n",
    "        return all_optimized_weights_list, all_optimized_thresh_list, all_dataset_sizes_list, all_IG_list\n",
    "\n",
    "\n",
    "    # Hyper-parameter of the algorithm\n",
    "    c1 = c2 = 0.1\n",
    "    w1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\n",
    "    w2 = 0.8 \n",
    "    # Create particles\n",
    "    n_particles = 20\n",
    "    np.random.seed(100)\n",
    "    params1=[initialize_weights(X_train.shape[1]) for i in range(n_particles)] # a vector of shape n_particles,n_features\n",
    "    # call the initialize_weights function above\n",
    "\n",
    "    params2=[np.random.uniform() for i in range(n_particles)]# a vector of shape n_particles\n",
    "    # use the np.random.uniform() function\n",
    "\n",
    "    params1=np.array(params1)\n",
    "    params2=np.array(params2)\n",
    "\n",
    "    # define velocity of each weight of every particle\n",
    "    V_param1 = [initialize_weights(X_train.shape[1])*0.1 for i in range(n_particles)] # shape is same as params1\n",
    "    # once again can use initialize_weights function\n",
    "\n",
    "    #define velocity of each threshold of every particle\n",
    "    V_param2 = np.array([np.random.uniform()*0.1 for i in range(n_particles)])# shape is same as params2\n",
    "    # once again use np.random.uniform\n",
    "\n",
    "        # Initialize objective values\n",
    "    pbest = (params1,params2)\n",
    "    pbest_obj = objective_fn_vector(params1, params2, X_train, y_train)\n",
    "    gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()])\n",
    "    gbest_obj = pbest_obj.min()\n",
    "\n",
    "    new_ys = np.dot(train_x, gbest[0])\n",
    "    # normalize the new_ys\n",
    "    new_ys = (new_ys - np.min(new_ys)) / (np.max(new_ys) - np.min(new_ys))\n",
    "\n",
    "    for i in range(100):\n",
    "        update(V_param1, V_param2, params1, params2, pbest, pbest_obj, gbest, gbest_obj)\n",
    "        # calculate new_ys inside the loop\n",
    "        new_ys = np.dot(train_x, gbest[0])\n",
    "        # normalize the new_ys\n",
    "        new_ys = (new_ys - np.min(new_ys)) / (np.max(new_ys) - np.min(new_ys))\n",
    "        # calculate the threshold using the 50th percentile of new_ys\n",
    "        threshold = np.percentile(new_ys, 50) \n",
    "        #Half of the new_ys values will be greater than the threshold and half will be lower \n",
    "\n",
    "    unique, counts = np.unique(train_y, return_counts=True)\n",
    "    count_items = dict(zip(unique, counts))\n",
    "\n",
    "    # add the achieved optimized values to the lists    \n",
    "    all_optimized_weights_list[node_number]=gbest[0]\n",
    "    all_optimized_thresh_list[node_number]=threshold\n",
    "    all_dataset_sizes_list[node_number]=train_x.shape[0]\n",
    "    all_IG_list[node_number] = information_gain(train_y, new_ys, count_items)\n",
    "\n",
    "    # Save threshold value to dictionary with node number as key\n",
    "    threshold_dict[node_number] = threshold\n",
    "\n",
    "    # chop the data into two parts: left\n",
    "    train_x_left = train_x[new_ys >= threshold]\n",
    "    train_y_left = train_y[new_ys >= threshold]\n",
    "    left_child_node_num = node_number * 2 + 1\n",
    "\n",
    "    # chop the data into two parts: right\n",
    "    train_x_right = train_x[new_ys < threshold]\n",
    "    train_y_right = train_y[new_ys < threshold]\n",
    "    right_child_node_num = node_number * 2 + 2\n",
    "\n",
    "    # exit condition 4: return if information gain is 0\n",
    "    if np.allclose(information_gain(train_y, new_ys, count_items), 0):\n",
    "        return\n",
    "\n",
    "    print(\"Left\",train_x_left.shape)\n",
    "    print(\"Right\",train_x_right.shape)\n",
    "    # make the recursion call for left\n",
    "    find_best_params(train_x_left, train_y_left, test_x, test_y, left_child_node_num)\n",
    "\n",
    "    # make the recursion call for right\n",
    "    find_best_params(train_x_right, train_y_right, test_x, test_y, right_child_node_num)\n",
    "\n",
    "   # return all_optimized_weights_list, all_optimized_thresh_list, all_dataset_sizes_list, all_IG_list, threshold_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee68f691",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:51:51.998095Z",
     "iopub.status.busy": "2023-03-30T15:51:51.996929Z",
     "iopub.status.idle": "2023-03-30T15:52:37.253598Z",
     "shell.execute_reply": "2023-03-30T15:52:37.252263Z",
     "shell.execute_reply.started": "2023-03-30T15:51:51.998055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_number 0 data shape (381, 30)\n",
      "Left (191, 30)\n",
      "Right (190, 30)\n",
      "node_number 1 data shape (191, 30)\n",
      "Left (96, 30)\n",
      "Right (95, 30)\n",
      "node_number 3 data shape (96, 30)\n",
      "Left (48, 30)\n",
      "Right (48, 30)\n",
      "node_number 7 data shape (48, 30)\n",
      "Left (24, 30)\n",
      "Right (24, 30)\n",
      "node_number 15 data shape (24, 30)\n",
      "Left (12, 30)\n",
      "Right (12, 30)\n",
      "node_number 31 data shape (12, 30)\n",
      "node_number 32 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 65 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 131 data shape (3, 30)\n",
      "node_number 132 data shape (3, 30)\n",
      "node_number 66 data shape (6, 30)\n",
      "node_number 16 data shape (24, 30)\n",
      "node_number 8 data shape (48, 30)\n",
      "Left (24, 30)\n",
      "Right (24, 30)\n",
      "node_number 17 data shape (24, 30)\n",
      "Left (12, 30)\n",
      "Right (12, 30)\n",
      "node_number 35 data shape (12, 30)\n",
      "node_number 36 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 73 data shape (6, 30)\n",
      "node_number 74 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 149 data shape (3, 30)\n",
      "node_number 150 data shape (3, 30)\n",
      "node_number 18 data shape (24, 30)\n",
      "node_number 4 data shape (95, 30)\n",
      "Left (48, 30)\n",
      "Right (47, 30)\n",
      "node_number 9 data shape (48, 30)\n",
      "Left (24, 30)\n",
      "Right (24, 30)\n",
      "node_number 19 data shape (24, 30)\n",
      "Left (12, 30)\n",
      "Right (12, 30)\n",
      "node_number 39 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 79 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 159 data shape (3, 30)\n",
      "node_number 160 data shape (3, 30)\n",
      "node_number 80 data shape (6, 30)\n",
      "node_number 40 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 81 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 163 data shape (3, 30)\n",
      "node_number 164 data shape (3, 30)\n",
      "node_number 82 data shape (6, 30)\n",
      "node_number 20 data shape (24, 30)\n",
      "Left (12, 30)\n",
      "Right (12, 30)\n",
      "node_number 41 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 83 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 167 data shape (3, 30)\n",
      "node_number 168 data shape (3, 30)\n",
      "node_number 84 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 169 data shape (3, 30)\n",
      "node_number 170 data shape (3, 30)\n",
      "node_number 42 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 85 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 171 data shape (3, 30)\n",
      "node_number 172 data shape (3, 30)\n",
      "node_number 86 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 173 data shape (3, 30)\n",
      "node_number 174 data shape (3, 30)\n",
      "node_number 10 data shape (47, 30)\n",
      "Left (24, 30)\n",
      "Right (23, 30)\n",
      "node_number 21 data shape (24, 30)\n",
      "Left (12, 30)\n",
      "Right (12, 30)\n",
      "node_number 43 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 87 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 175 data shape (3, 30)\n",
      "node_number 176 data shape (3, 30)\n",
      "node_number 88 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 177 data shape (3, 30)\n",
      "node_number 178 data shape (3, 30)\n",
      "node_number 44 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 89 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 179 data shape (3, 30)\n",
      "node_number 180 data shape (3, 30)\n",
      "node_number 90 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 181 data shape (3, 30)\n",
      "node_number 182 data shape (3, 30)\n",
      "node_number 22 data shape (23, 30)\n",
      "Left (12, 30)\n",
      "Right (11, 30)\n",
      "node_number 45 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 91 data shape (6, 30)\n",
      "node_number 92 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 185 data shape (3, 30)\n",
      "node_number 186 data shape (3, 30)\n",
      "node_number 46 data shape (11, 30)\n",
      "Left (6, 30)\n",
      "Right (5, 30)\n",
      "node_number 93 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 187 data shape (3, 30)\n",
      "node_number 188 data shape (3, 30)\n",
      "node_number 94 data shape (5, 30)\n",
      "Left (3, 30)\n",
      "Right (2, 30)\n",
      "node_number 189 data shape (3, 30)\n",
      "node_number 190 data shape (2, 30)\n",
      "node_number 2 data shape (190, 30)\n",
      "Left (95, 30)\n",
      "Right (95, 30)\n",
      "node_number 5 data shape (95, 30)\n",
      "Left (48, 30)\n",
      "Right (47, 30)\n",
      "node_number 11 data shape (48, 30)\n",
      "Left (24, 30)\n",
      "Right (24, 30)\n",
      "node_number 23 data shape (24, 30)\n",
      "Left (12, 30)\n",
      "Right (12, 30)\n",
      "node_number 47 data shape (12, 30)\n",
      "node_number 48 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 97 data shape (6, 30)\n",
      "node_number 98 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 197 data shape (3, 30)\n",
      "node_number 198 data shape (3, 30)\n",
      "node_number 24 data shape (24, 30)\n",
      "node_number 12 data shape (47, 30)\n",
      "Left (24, 30)\n",
      "Right (23, 30)\n",
      "node_number 25 data shape (24, 30)\n",
      "Left (12, 30)\n",
      "Right (12, 30)\n",
      "node_number 51 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 103 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 207 data shape (3, 30)\n",
      "node_number 208 data shape (3, 30)\n",
      "node_number 104 data shape (6, 30)\n",
      "node_number 52 data shape (12, 30)\n",
      "node_number 26 data shape (23, 30)\n",
      "Left (12, 30)\n",
      "Right (11, 30)\n",
      "node_number 53 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 107 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 215 data shape (3, 30)\n",
      "node_number 216 data shape (3, 30)\n",
      "node_number 108 data shape (6, 30)\n",
      "node_number 54 data shape (11, 30)\n",
      "node_number 6 data shape (95, 30)\n",
      "Left (48, 30)\n",
      "Right (47, 30)\n",
      "node_number 13 data shape (48, 30)\n",
      "Left (24, 30)\n",
      "Right (24, 30)\n",
      "node_number 27 data shape (24, 30)\n",
      "Left (12, 30)\n",
      "Right (12, 30)\n",
      "node_number 55 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 111 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 223 data shape (3, 30)\n",
      "node_number 224 data shape (3, 30)\n",
      "node_number 112 data shape (6, 30)\n",
      "node_number 56 data shape (12, 30)\n",
      "node_number 28 data shape (24, 30)\n",
      "node_number 14 data shape (47, 30)\n"
     ]
    }
   ],
   "source": [
    "node_number=0\n",
    "find_best_params(X_train,y_train,X_test,y_test,node_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0be85bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:52:37.256190Z",
     "iopub.status.busy": "2023-03-30T15:52:37.255078Z",
     "iopub.status.idle": "2023-03-30T15:52:37.266702Z",
     "shell.execute_reply": "2023-03-30T15:52:37.265358Z",
     "shell.execute_reply.started": "2023-03-30T15:52:37.256139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.22441467237833546,\n",
       " 0.25040664249973654,\n",
       " 0.6070197815984368,\n",
       " 0.16999422935636366,\n",
       " 0.3870857323197978,\n",
       " 0.40154230043943406,\n",
       " 0.6953655013125565,\n",
       " 0.18290547726336936,\n",
       " 0.5243978726934435,\n",
       " 0.49439422020590257,\n",
       " 0.4199079226819826,\n",
       " 0.5180354180900657,\n",
       " 0.5023437334491847,\n",
       " 0.49085075550996815,\n",
       " None,\n",
       " 0.26670256775705414,\n",
       " None,\n",
       " 0.5365141637823336,\n",
       " None,\n",
       " 0.5925103386771338,\n",
       " 0.5738675330691438,\n",
       " 0.453015413122601,\n",
       " 0.47245793766868893,\n",
       " 0.3931328210882388,\n",
       " None,\n",
       " 0.5966365029521083,\n",
       " 0.4190398195863026,\n",
       " 0.5754684486333079,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 0.5332669248888764,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 0.3492993970535977,\n",
       " None,\n",
       " None,\n",
       " 0.3287786683014051,\n",
       " 0.49219480860396525,\n",
       " 0.5177772246091644,\n",
       " 0.6177374815344967,\n",
       " 0.5371743156798217,\n",
       " 0.5430055792077058,\n",
       " 0.6885497405414811,\n",
       " 0.3282434638570763,\n",
       " None,\n",
       " 0.4347875268433588,\n",
       " None,\n",
       " None,\n",
       " 0.7266951260248022,\n",
       " None,\n",
       " 0.5651702088281272,\n",
       " None,\n",
       " 0.5007759882503201,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 0.462721269194764,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 0.407986836368041,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 0.08014449008621545,\n",
       " None,\n",
       " 0.805762728072178,\n",
       " None,\n",
       " 0.8094481882564852,\n",
       " 0.5262198191314693,\n",
       " 0.2599982143014793,\n",
       " 0.7354076126010943,\n",
       " 0.6757965260478538,\n",
       " 0.3760213283765001,\n",
       " 0.41480723609955844,\n",
       " 0.5725988150819651,\n",
       " None,\n",
       " 0.35087138922971006,\n",
       " 0.4575149018345144,\n",
       " 0.647799845925609,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 0.6658385302805891,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 0.7260821886978004,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 0.6036707989285783,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 0.3456316754849498,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_optimized_thresh_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b173245",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T15:52:37.269060Z",
     "iopub.status.busy": "2023-03-30T15:52:37.268630Z",
     "iopub.status.idle": "2023-03-30T15:52:37.275627Z",
     "shell.execute_reply": "2023-03-30T15:52:37.274514Z",
     "shell.execute_reply.started": "2023-03-30T15:52:37.269020Z"
    }
   },
   "outputs": [],
   "source": [
    "#thresh=all_optimized_thresh_list[0]\n",
    "#print(thresh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
