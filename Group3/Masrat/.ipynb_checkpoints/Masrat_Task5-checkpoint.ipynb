{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65eea450",
   "metadata": {
    "id": "65eea450"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlib\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lib'"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import lib \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8589f94",
   "metadata": {
    "id": "d8589f94"
   },
   "source": [
    "### make sure you import with correct file name above\n",
    "\n",
    "```\n",
    "import filename as lib\n",
    "```\n",
    "(without the .py extension)\n",
    "\n",
    "if filename is lib.py\n",
    "```\n",
    "import lib as lib\n",
    "```\n",
    "\n",
    "if filename is res.py\n",
    "```\n",
    "import res as lib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c553beb",
   "metadata": {
    "id": "6c553beb"
   },
   "outputs": [],
   "source": [
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b42c8",
   "metadata": {
    "id": "584b42c8",
    "outputId": "41cfc7dc-e7ae-4357-9d10-70ae86ca3006"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0549201679861442\n"
     ]
    }
   ],
   "source": [
    "pk = np.array([1/5, 2/5, 2/5])  # fair coin\n",
    "H = entropy(pk)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d5b1e0",
   "metadata": {
    "id": "a7d5b1e0",
    "outputId": "5b308a2b-b1eb-4832-9d0e-c77da37ea47e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.3333333333333333, 1: 0.3333333333333333, 2: 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "# test your function\n",
    "list_labels=[1,2,0,1,2,0]\n",
    "uniq_labels=[0,1,2]\n",
    "print(lib.calculate_probabilities(list_labels,uniq_labels))\n",
    "# this should print somehting like 0.33,0.33,0.33\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d62286",
   "metadata": {
    "id": "13d62286",
    "outputId": "4ae010f2-bbb1-46c5-e8e9-e526dfae22c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0549201679861442\n"
     ]
    }
   ],
   "source": [
    "# test your function\n",
    "list_probas=[1/5, 2/5, 2/5]\n",
    "print(lib.calc_entropy_from_probabilities(list_probas))\n",
    "# above should print 1.054..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1b8ac1",
   "metadata": {
    "id": "9a1b8ac1",
    "outputId": "10b5754d-6598-47b2-ae5d-2b8a7782195a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#test your function\n",
    "old_entropy=1\n",
    "new_entropies=[0,0.65]\n",
    "count_items=[4,6]\n",
    "print(lib.information_gain(old_entropy,new_entropies,count_items))\n",
    "# above should print 0.61\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d8bd4",
   "metadata": {
    "id": "9a7d8bd4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e2fea1",
   "metadata": {
    "id": "59e2fea1",
    "outputId": "94f4d35f-bc30-4d28-809d-3b5b0d935f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39200437 0.41372806 0.37198109 0.69543609]\n"
     ]
    }
   ],
   "source": [
    "num_feats=X_train.shape[1]\n",
    "print(lib.initialize_weights(num_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2b8cf0",
   "metadata": {
    "id": "5a2b8cf0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd7b59b0",
   "metadata": {
    "id": "bd7b59b0"
   },
   "source": [
    "### Task4: PSO Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bec610",
   "metadata": {
    "id": "25bec610"
   },
   "source": [
    "#### Modified the entropy function to get a vector of entropies for n particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989af75",
   "metadata": {
    "id": "b989af75"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d970ff7",
   "metadata": {
    "id": "9d970ff7"
   },
   "outputs": [],
   "source": [
    "### Below we just randomly assign 100 particles and see if we can find the global minimum.\n",
    "### THis is just to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251094b5",
   "metadata": {
    "id": "251094b5",
    "outputId": "ec6d0676-bde8-45eb-d475-80f180ca95cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of params 1 (weights) (100, 4)\n",
      "Shape of params 2 (thresholds) (100,)\n"
     ]
    }
   ],
   "source": [
    "params1=[lib.initialize_weights(X_train.shape[1]) for i in range(100)]# a vector of shape 100,4\n",
    "# call the initialize_weights function above\n",
    "\n",
    "params2=[np.random.uniform() for i in range(100)]# a vector of shape 100\n",
    "# use the np.random.uniform() function\n",
    "\n",
    "# we have a list of 100 weight vectors (params1) and 100 thresholds (params2)\n",
    "# convert them to array\n",
    "params1=np.array(params1)\n",
    "params2=np.array(params2)\n",
    "\n",
    "print(\"Shape of params 1 (weights)\",params1.shape)\n",
    "print(\"Shape of params 2 (thresholds)\",params2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e84de3",
   "metadata": {
    "id": "b4e84de3",
    "outputId": "98f59960-dc1f-4ca5-e04e-dddfb8eaf2c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param1_min [0.37354781 0.32839402 0.55265166 0.94437739] param2_min 0.5986985252524131\n"
     ]
    }
   ],
   "source": [
    "z = lib.objective_fn_vector(params1, params2, X_train, y_train)\n",
    "# Find the global minimum\n",
    "param1_min = params1[z.argmin()] # use z.argmin()\n",
    "param2_min = params2[z.argmin()] # use z.argmin()\n",
    "\n",
    "print(\"param1_min\",param1_min,\"param2_min\",param2_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa02705d",
   "metadata": {
    "id": "fa02705d"
   },
   "outputs": [],
   "source": [
    "### Setting up the particles and other parameters now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e777c594",
   "metadata": {
    "id": "e777c594",
    "outputId": "38640de8-6d35-49b1-dab7-7c0f83b434b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params1 shape is  (20, 4) params2 shape is  (20,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyper-parameter of the algorithm\n",
    "c1 = c2 = 0.1\n",
    "w1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\n",
    "w2 = 0.8 \n",
    "# Create particles\n",
    "n_particles = 20\n",
    "np.random.seed(100)\n",
    "params1=[lib.initialize_weights(X_train.shape[1]) for i in range(n_particles)] # a vector of shape n_particles,4\n",
    "# call the initialize_weights function above\n",
    "\n",
    "params2=[np.random.uniform() for i in range(n_particles)]# a vector of shape n_particles\n",
    "# use the np.random.uniform() function\n",
    "\n",
    "params1=np.array(params1)\n",
    "params2=np.array(params2)\n",
    "\n",
    "print(\"params1 shape is \",params1.shape,\"params2 shape is \",params2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be72213",
   "metadata": {
    "id": "3be72213"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02386d",
   "metadata": {
    "id": "6c02386d",
    "outputId": "0ece2f18-0ad1-47dd-ba07-c1a647e83a2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pbest obj value for 20 particles are as follows [0.67013703 0.82232957 0.64329013 0.70573338 0.73886477 0.82232957\n",
      " 1.09729975 0.5237323  0.77244152 0.4620281  0.86703698 0.81919055\n",
      " 1.09729975 0.81919055 0.73355763 0.74030523 0.82232957 0.73805779\n",
      " 1.09729975 0.68309963]\n",
      "gbest obj value among all 20 particles is as follows 0.4620281046196322\n"
     ]
    }
   ],
   "source": [
    "# define velocity of each weight of every particle\n",
    "V_param1 = [lib.initialize_weights(X_train.shape[1])*0.1 for i in range(n_particles)] # shape is same as params1\n",
    "# once again can use initialize_weights function\n",
    "\n",
    "#define velocity of each threshold of every particle\n",
    "V_param2 = np.array([np.random.uniform()*0.1 for i in range(n_particles)])# shape is same as params2\n",
    "# once again use np.random.uniform() function\n",
    "\n",
    "# Initialize objective values\n",
    "pbest = (params1,params2)\n",
    "pbest_obj = lib.objective_fn_vector(params1, params2, X_train, y_train)\n",
    "gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()])\n",
    "gbest_obj = pbest_obj.min()\n",
    "\n",
    "print(\"pbest obj value for 20 particles are as follows\",pbest_obj)\n",
    "print(\"gbest obj value among all 20 particles is as follows\",gbest_obj)\n",
    "# note that gbest_obj should be the minimim of all pbest_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a4bc73",
   "metadata": {
    "id": "37a4bc73"
   },
   "source": [
    "### the update function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7e37e6",
   "metadata": {
    "id": "cc7e37e6"
   },
   "outputs": [],
   "source": [
    "def update():\n",
    "    \"Function to do one iteration of particle swarm optimization\"\n",
    "    global V_param1,V_param2, params1,params2, pbest, pbest_obj, gbest, gbest_obj\n",
    "    # these have been already initialized in the previous cells\n",
    "    \n",
    "    # Update params\n",
    "    r11,r12, r2 = np.random.rand(3)\n",
    "    V_param1=w1*V_param1+c1*r11*(pbest[0] - params1)+ c2*r2*(gbest[0]-params1)\n",
    "    V_param2=w2*V_param2+c1*r12*(pbest[1] - params2)+ c2*r2*(gbest[1]-params2)    \n",
    "#     V = w * V + c1*r11*(pbest - params1) + c2*r2*(gbest.reshape(-1,1)-X)\n",
    "    params1 = params1 + V_param1\n",
    "    params2 = params2 + V_param2\n",
    "    \n",
    "    obj = lib.objective_fn_vector(params1, params2, X_train, y_train)\n",
    "    for i in range(pbest[0].shape[0]):\n",
    "        if pbest_obj[i]>=obj[i]:\n",
    "            \n",
    "            pbest[0][i]=params1[i] # update pbest[0][i] with value of params1[i]\n",
    "            pbest[1][i]=params2[i] # update pbest[1][i] \n",
    "            pbest_obj[i]=obj[i]    # also update pbest_obj[i]\n",
    "\n",
    "            \n",
    "    gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()]) # update gbest to contain the best from params1 and params 2\n",
    "    gbest_obj = pbest_obj.min() # update gbest to get the minimum of pbest_obj\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c029808",
   "metadata": {
    "id": "2c029808",
    "outputId": "c9ace597-4209-4094-cdc1-870fd9fd9435"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSO found best solution at f((array([0.82357244, 0.30594878, 0.56947532, 1.02209275]), 0.8070045185863748))=0.4161039895073432\n",
      "Global optimal at f([array([0.37354781, 0.32839402, 0.55265166, 0.94437739]), 0.5986985252524131])=0.4620281046196322\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    update()\n",
    "print(\"PSO found best solution at f({})={}\".format(gbest, gbest_obj))\n",
    "print(\"Global optimal at f({})={}\".format([param1_min,param2_min], lib.objective_fn(param1_min, param2_min, X_train, y_train)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832613d1",
   "metadata": {
    "id": "832613d1"
   },
   "source": [
    "### Try to resolve the error above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d2c15d",
   "metadata": {
    "id": "d9d2c15d"
   },
   "source": [
    "### Try a different dataset (preferable binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a0ace",
   "metadata": {
    "id": "f68a0ace",
    "outputId": "e42cb84f-38ea-4667-d0a0-46308393f13c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapr of training data is  (381, 30)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "#load the breast cancer dataset \n",
    "bcan = sklearn.datasets.load_breast_cancer()\n",
    "X = bcan.data\n",
    "y = bcan.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "print(\"Shapr of training data is \",X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27948481",
   "metadata": {
    "id": "27948481"
   },
   "outputs": [],
   "source": [
    "params1=[lib.initialize_weights(X_train.shape[1]) for i in range(100)]# a vector of shape 100,4\n",
    "# call the initialize_weights function above\n",
    "\n",
    "params2=[np.random.uniform() for i in range(100)]# a vector of shape 100\n",
    "# use the np.random.uniform() function\n",
    "# initialize the params1 and params2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52998992",
   "metadata": {
    "id": "52998992",
    "outputId": "f48ac081-eeee-4ec5-c65e-ff4b21d3e8cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of params 1 (weights) (100, 30)\n",
      "Shape of params 2 (thresholds) (100,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# call the initialize_weights function above\n",
    "\n",
    "# use the np.random.uniform() function\n",
    "\n",
    "\n",
    "# we have a list of 100 weight vectors (params1) and 100 thresholds (params2)\n",
    "# convert them to array\n",
    "params1=np.array(params1)\n",
    "params2=np.array(params2)\n",
    "\n",
    "print(\"Shape of params 1 (weights)\",params1.shape)\n",
    "print(\"Shape of params 2 (thresholds)\",params2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcdfb5f",
   "metadata": {
    "id": "fbcdfb5f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a6b1d5",
   "metadata": {
    "id": "64a6b1d5",
    "outputId": "d2bd492b-de1e-458a-e16e-87c590e3f9a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param1_min [0.18245493 0.05052287 0.25256524 0.92907513 0.68984837 0.31908874\n",
      " 0.49173868 0.35238822 0.85396625 0.25328364 0.2837559  0.1907728\n",
      " 0.54990109 0.52196241 0.14893894 0.68850451 0.91048032 0.48135845\n",
      " 0.57524243 0.55891037 0.87255076 0.01371472 0.01663933 0.86819341\n",
      " 0.59229002 0.57706058 0.02868421 0.94438384 0.56813455 0.88657561] param2_min 0.9859728952134363\n"
     ]
    }
   ],
   "source": [
    "z = lib.objective_fn_vector(params1, params2, X_train, y_train)\n",
    "# Find the global minimum\n",
    "param1_min = params1[z.argmin()] # use z.argmin()\n",
    "param2_min = params2[z.argmin()] # use z.argmin()\n",
    "\n",
    "print(\"param1_min\",param1_min,\"param2_min\",param2_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8138ba39",
   "metadata": {
    "id": "8138ba39",
    "outputId": "8f7d1c7e-b424-42fc-a0d0-4f145c7edd4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params1 shape is  (20, 30) params2 shape is  (20,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyper-parameter of the algorithm\n",
    "c1 = c2 = 0.1\n",
    "w1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\n",
    "w2 = 0.8 \n",
    "# Create particles\n",
    "n_particles = 20\n",
    "np.random.seed(100)\n",
    "params1=[lib.initialize_weights(X_train.shape[1]) for i in range(n_particles)] # a vector of shape n_particles,4\n",
    "# call the initialize_weights function above\n",
    "\n",
    "params2=[np.random.uniform() for i in range(n_particles)]# a vector of shape n_particles\n",
    "# use the np.random.uniform() function\n",
    "\n",
    "params1=np.array(params1)\n",
    "params2=np.array(params2)\n",
    "\n",
    "print(\"params1 shape is \",params1.shape,\"params2 shape is \",params2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5622d0eb",
   "metadata": {
    "id": "5622d0eb"
   },
   "outputs": [],
   "source": [
    "# define velocity of each weight of every particle\n",
    "V_param1 = [lib.initialize_weights(X_train.shape[1])*0.1 for i in range(n_particles)] # shape is same as params1\n",
    "# once again can use initialize_weights function\n",
    "\n",
    "#define velocity of each threshold of every particle\n",
    "V_param2 = np.array([np.random.uniform()*0.1 for i in range(n_particles)])# shape is same as params2\n",
    "# once again use np.random.uniform() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72744a20",
   "metadata": {
    "id": "72744a20",
    "outputId": "be51b02c-d76b-405c-bbed-8769f7a2b8df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pbest obj value for 20 particles are as follows [0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461\n",
      " 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461\n",
      " 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461]\n",
      "gbest obj value among all 20 particles is as follows 0.6643460978641622\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize objective values\n",
    "pbest = (params1,params2)\n",
    "pbest_obj = lib.objective_fn_vector(params1, params2, X_train, y_train)\n",
    "gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()])\n",
    "gbest_obj = pbest_obj.min()\n",
    "\n",
    "print(\"pbest obj value for 20 particles are as follows\",pbest_obj)\n",
    "print(\"gbest obj value among all 20 particles is as follows\",gbest_obj)\n",
    "# note that gbest_obj should be the minimim of all pbest_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a42fa29",
   "metadata": {
    "id": "1a42fa29"
   },
   "outputs": [],
   "source": [
    "def update():\n",
    "    \"Function to do one iteration of particle swarm optimization\"\n",
    "    global V_param1,V_param2, params1,params2, pbest, pbest_obj, gbest, gbest_obj\n",
    "    # these have been already initialized in the previous cells\n",
    "    \n",
    "    # Update params\n",
    "    r11,r12, r2 = np.random.rand(3)\n",
    "    V_param1=w1*V_param1+c1*r11*(pbest[0] - params1)+ c2*r2*(gbest[0]-params1)\n",
    "    V_param2=w2*V_param2+c1*r12*(pbest[1] - params2)+ c2*r2*(gbest[1]-params2)    \n",
    "#     V = w * V + c1*r11*(pbest - params1) + c2*r2*(gbest.reshape(-1,1)-X)\n",
    "    params1 = params1 + V_param1\n",
    "    params2 = params2 + V_param2\n",
    "    \n",
    "    obj = lib.objective_fn_vector(params1, params2, X_train, y_train)\n",
    "    for i in range(pbest[0].shape[0]):\n",
    "        if pbest_obj[i]>=obj[i]:\n",
    "            \n",
    "            pbest[0][i]=params1[i] # update pbest[0][i] with value of params1[i]\n",
    "            pbest[1][i]=params2[i] # update pbest[1][i] \n",
    "            pbest_obj[i]=obj[i]    # also update pbest_obj[i]\n",
    "\n",
    "            \n",
    "    gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()]) # update gbest to contain the best from params1 and params 2\n",
    "    gbest_obj = pbest_obj.min() # update gbest to get the minimum of pbest_obj\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e2e7ef",
   "metadata": {
    "id": "38e2e7ef",
    "outputId": "20310465-32ee-4590-8aad-0963083b676d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSO found best solution at f((array([0.73967237, 0.397978  , 0.52943064, 0.85073834, 0.00519341,\n",
      "       0.223283  , 0.69099158, 0.83329627, 0.20659199, 0.65730114,\n",
      "       0.90826225, 0.21069073, 0.19250957, 0.28918705, 0.4864996 ,\n",
      "       0.98468547, 0.87774426, 2.28774533, 0.85622285, 0.29948077,\n",
      "       1.07324093, 1.08387372, 0.83011504, 0.53068515, 0.20038449,\n",
      "       0.38772061, 0.03103212, 0.90301547, 0.94961941, 0.03664681]), 0.6462061718275887))=0.6643460978641622\n",
      "Global optimal at f([array([0.18245493, 0.05052287, 0.25256524, 0.92907513, 0.68984837,\n",
      "       0.31908874, 0.49173868, 0.35238822, 0.85396625, 0.25328364,\n",
      "       0.2837559 , 0.1907728 , 0.54990109, 0.52196241, 0.14893894,\n",
      "       0.68850451, 0.91048032, 0.48135845, 0.57524243, 0.55891037,\n",
      "       0.87255076, 0.01371472, 0.01663933, 0.86819341, 0.59229002,\n",
      "       0.57706058, 0.02868421, 0.94438384, 0.56813455, 0.88657561]), 0.9859728952134363])=0.44322120624369\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    update()\n",
    "print(\"PSO found best solution at f({})={}\".format(gbest, gbest_obj))\n",
    "print(\"Global optimal at f({})={}\".format([param1_min,param2_min], lib.objective_fn(param1_min, param2_min, X_train, y_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee4622",
   "metadata": {
    "id": "7eee4622"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2767927f",
   "metadata": {
    "id": "2767927f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
