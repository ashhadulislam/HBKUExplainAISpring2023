{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.stats import entropy\nfrom sklearn.datasets import load_breast_cancer","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.129589Z","iopub.execute_input":"2023-03-02T07:34:52.130775Z","iopub.status.idle":"2023-03-02T07:34:52.136530Z","shell.execute_reply.started":"2023-03-02T07:34:52.130727Z","shell.execute_reply":"2023-03-02T07:34:52.135421Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"# import some data to play with\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n     X, y, test_size=0.33, random_state=42)\n\n\n# normalize the data\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\n# def change_weights(X_train,y_train,X_test,y_test,weights):","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.138222Z","iopub.execute_input":"2023-03-02T07:34:52.138674Z","iopub.status.idle":"2023-03-02T07:34:52.151957Z","shell.execute_reply.started":"2023-03-02T07:34:52.138639Z","shell.execute_reply":"2023-03-02T07:34:52.150715Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"pk = np.array([1/5, 2/5, 2/5])  # fair coin\nH = entropy(pk)\nprint(H)","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.154143Z","iopub.execute_input":"2023-03-02T07:34:52.155277Z","iopub.status.idle":"2023-03-02T07:34:52.162710Z","shell.execute_reply.started":"2023-03-02T07:34:52.155226Z","shell.execute_reply":"2023-03-02T07:34:52.161448Z"},"trusted":true},"execution_count":118,"outputs":[{"name":"stdout","text":"1.0549201679861442\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef calculate_probabilities(list_labels, uniq_labels):\n    '''\n    Author: Sara Nassar \n    this function calculates the probabilities of each label in the list of labels\n    it is calculated by number of labels in class A/all labels\n    number of labels in class B/all labels\n    and so on\n    '''\n    \n    # A dictionary to store the probabilities\n    probabilities = dict.fromkeys(uniq_labels, 0)\n    \n    # Total number of labels\n    total_labels = len(list_labels)\n    \n    for label in uniq_labels:\n        # Counting the number of times the label occurs in the list\n        count = list_labels.count(label)\n        \n        # Calculating the probability of the label\n        probability = count / total_labels\n        \n        # Storing the calculated probability in the dictionary\n        probabilities[label] = probability\n        \n    return probabilities    \n    \n    \n# test your function\nlist_labels=[1,2,0,1,2,0]\nuniq_labels=[0,1,2]\nprint(calculate_probabilities(list_labels,uniq_labels))\n# this should print somehting like 0.33,0.33,0.33\n","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.164129Z","iopub.execute_input":"2023-03-02T07:34:52.164864Z","iopub.status.idle":"2023-03-02T07:34:52.174500Z","shell.execute_reply.started":"2023-03-02T07:34:52.164815Z","shell.execute_reply":"2023-03-02T07:34:52.173185Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stdout","text":"{0: 0.3333333333333333, 1: 0.3333333333333333, 2: 0.3333333333333333}\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef calc_entropy_from_probabilities(list_probas):\n    '''\n    Author: Sara Nassar \n    list_probas is the list of probabiities\n    the formula for entropy is\n    sum(-proba*log(proba))\n    \n    '''\n    \n    entropy_value = 0\n\n    for proba in list_probas:\n        # If the probability is not zero\n        if proba != 0:\n            entropy_value += -proba * np.log(proba)\n     \n    return entropy_value\n\n\n# test your function\nlist_probas=[1/5, 2/5, 2/5]\nprint(calc_entropy_from_probabilities(list_probas))\n# above should print 1.054...","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.176289Z","iopub.execute_input":"2023-03-02T07:34:52.176694Z","iopub.status.idle":"2023-03-02T07:34:52.190709Z","shell.execute_reply.started":"2023-03-02T07:34:52.176649Z","shell.execute_reply":"2023-03-02T07:34:52.189203Z"},"trusted":true},"execution_count":120,"outputs":[{"name":"stdout","text":"1.0549201679861442\n","output_type":"stream"}]},{"cell_type":"code","source":"def information_gain(old_entropy,new_entropies,count_items):\n    '''\n    Author: Sara Nassar \n    from the list of new entropies, calculate the overall new entropy\n    \n    formula is something like:\n    overall_new_entropy = entropy1*proportion1 + entropy2*proportion2+ entropy3*proportion3 ...\n    \n    igain=old_entropy-overall_new_entropy\n    '''\n    \n    overall_new_entropy = 0\n    \n    # Calculating the total number of items\n    total_items = sum(count_items)\n    \n    for i in range(len(new_entropies)):\n        # Calculating the proportion of items in the current partition\n        proportion = count_items[i] / total_items\n        \n        # Adding the entropy of the current partition weighted by its proportion to the overall new entropy\n        overall_new_entropy += new_entropies[i] * proportion\n        \n    # Calculating the information gain\n    information_gain = old_entropy - overall_new_entropy\n    \n    return information_gain\n\n#test your function\nold_entropy=1\nnew_entropies=[0,0.65]\ncount_items=[4,6]\nprint(information_gain(old_entropy,new_entropies,count_items))\n# above should print 0.61\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.194249Z","iopub.execute_input":"2023-03-02T07:34:52.194634Z","iopub.status.idle":"2023-03-02T07:34:52.203691Z","shell.execute_reply.started":"2023-03-02T07:34:52.194600Z","shell.execute_reply":"2023-03-02T07:34:52.202448Z"},"trusted":true},"execution_count":121,"outputs":[{"name":"stdout","text":"0.61\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef initialize_weights(number_features):\n    '''\n    the first set of weights corresponding to the features\n    For now, it defaults to 2\n    '''\n    \n    weights=np.array([np.random.uniform() for i in range(number_features)])\n    return weights\n    ","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.204975Z","iopub.execute_input":"2023-03-02T07:34:52.205722Z","iopub.status.idle":"2023-03-02T07:34:52.217838Z","shell.execute_reply.started":"2023-03-02T07:34:52.205675Z","shell.execute_reply":"2023-03-02T07:34:52.216835Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"num_feats=X_train.shape[1]\nprint(initialize_weights(num_feats))","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.220205Z","iopub.execute_input":"2023-03-02T07:34:52.220662Z","iopub.status.idle":"2023-03-02T07:34:52.227901Z","shell.execute_reply.started":"2023-03-02T07:34:52.220614Z","shell.execute_reply":"2023-03-02T07:34:52.226567Z"},"trusted":true},"execution_count":123,"outputs":[{"name":"stdout","text":"[0.41709074 0.69559103 0.42484724 0.85811423]\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_entropy_from_groups(new_entropies,count_items):\n    overall_new_entropy = 0\n    \n    # Calculating the total number of items\n    total_items = sum(count_items)\n    \n    for i in range(len(new_entropies)):\n        # Calculating the proportion of items in the current partition\n        proportion = count_items[i] / total_items\n        \n        # Adding the entropy of the current partition weighted by its proportion to the overall new entropy\n        overall_new_entropy += new_entropies[i] * proportion\n        \n    return overall_new_entropy    \n\ndef get_entropy(threshold,res,y_test):\n\n    # make two groups\n    group1=[]\n    group2=[]\n\n    for i in range(res.shape[0]):\n        if res[i]<threshold:\n            group1.append(y_test[i])\n        else:\n            group2.append(y_test[i])\n\n\n\n\n    proba_gr1=calculate_probabilities(group1,np.unique(group1).tolist())\n    proba_gr1=list(proba_gr1.values()) \n    entropy_group1=calc_entropy_from_probabilities(proba_gr1)\n    count_group1=len(proba_gr1)\n\n    proba_gr2=calculate_probabilities(group2,np.unique(group2).tolist())\n    proba_gr2=list(proba_gr2.values()) \n    entropy_group2=calc_entropy_from_probabilities(proba_gr2)\n    count_group2=len(proba_gr2)\n\n    new_entropies=[entropy_group1,entropy_group2]\n    count_items=[count_group1,count_group2]\n    overall_new_entropy=get_entropy_from_groups(new_entropies,count_items)\n    return overall_new_entropy\n","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.229416Z","iopub.execute_input":"2023-03-02T07:34:52.230205Z","iopub.status.idle":"2023-03-02T07:34:52.240961Z","shell.execute_reply.started":"2023-03-02T07:34:52.230167Z","shell.execute_reply":"2023-03-02T07:34:52.240050Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"markdown","source":"### Task4: PSO Implementation","metadata":{}},{"cell_type":"markdown","source":"#### Modified the entropy function to get a vector of entropies for n particles","metadata":{}},{"cell_type":"code","source":" def objective_fn(param1, param2, X, y):\n    '''\n    Author: Sara Nassar\n    param1 and param2 are the parameters that we want to optimize\n    say param1 is the weight vector and  param2 is the threshold\n    '''\n    # Multiply the weights with each feature and calculate the sum\n    res = np.sum(X * param1, axis=1)\n    \n    # Calculate entropy using the get_entropy function\n    entropy = get_entropy(param2, res, y)\n    \n    return entropy   \n\n    \ndef objective_fn_vector(params1, params2, X, y):\n    '''\n    Author: Sara Nassar\n    params1 is an array of weight vectors\n    params2 is an array of thresholds\n    '''\n    results = []\n    for i in range(params1.shape[0]):\n        param1 = params1[i] # get ith set of weights\n        param2 = params2[i] # get ith threshold\n        # call the objective_fn above to get the entropy\n        res = objective_fn(param1, param2, X, y)\n        #print(param2,res)\n        results.append(res)\n    \n    return np.array(results)","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.243844Z","iopub.execute_input":"2023-03-02T07:34:52.244164Z","iopub.status.idle":"2023-03-02T07:34:52.257499Z","shell.execute_reply.started":"2023-03-02T07:34:52.244135Z","shell.execute_reply":"2023-03-02T07:34:52.256099Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"### Below we just randomly assign 100 particles and see if we can find the global minimum.\n### THis is just to check","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.258892Z","iopub.execute_input":"2023-03-02T07:34:52.259641Z","iopub.status.idle":"2023-03-02T07:34:52.273418Z","shell.execute_reply.started":"2023-03-02T07:34:52.259602Z","shell.execute_reply":"2023-03-02T07:34:52.272125Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"'''\nAuthor: Sara Nassar\nBelow we just randomly assign 100 particles and see if we can find the global minimum.\n'''\nnum_particles = 100\nnum_features = X_train.shape[1]\n\nparams1 = []\nfor i in range(num_particles):\n    weights = initialize_weights(num_features)\n    params1.append(weights)\n\n    \n# we have a list of 100 weight vectors (params1) and 100 thresholds (params2)\n# convert them to array\nparams1 = np.array(params1)\nparams2 = np.random.uniform(size=num_particles)\n\nprint(\"Shape of params 1 (weights):\", params1.shape)\nprint(\"Shape of params 2 (thresholds):\", params2.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.274723Z","iopub.execute_input":"2023-03-02T07:34:52.275077Z","iopub.status.idle":"2023-03-02T07:34:52.287524Z","shell.execute_reply.started":"2023-03-02T07:34:52.275043Z","shell.execute_reply":"2023-03-02T07:34:52.286429Z"},"trusted":true},"execution_count":127,"outputs":[{"name":"stdout","text":"Shape of params 1 (weights): (100, 4)\nShape of params 2 (thresholds): (100,)\n","output_type":"stream"}]},{"cell_type":"code","source":"z = objective_fn_vector(params1, params2, X_train, y_train)\n# Find the global minimum that is using the minimum if params1 and params2\nglobal_min_idx = np.argmin(z)\nparam1_min = params1[global_min_idx] # use z.argmin() to access best params1\nparam2_min = params2[global_min_idx] # use z.argmin() to access best params2\n\nprint(\"param1_min\",param1_min,\"param2_min\",param2_min)","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.289768Z","iopub.execute_input":"2023-03-02T07:34:52.290986Z","iopub.status.idle":"2023-03-02T07:34:52.315730Z","shell.execute_reply.started":"2023-03-02T07:34:52.290940Z","shell.execute_reply":"2023-03-02T07:34:52.314332Z"},"trusted":true},"execution_count":128,"outputs":[{"name":"stdout","text":"param1_min [0.035627   0.49239265 0.95237685 0.81057376] param2_min 0.6723314084399745\n","output_type":"stream"}]},{"cell_type":"code","source":"### Setting up the particles and other parameters now","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.317270Z","iopub.execute_input":"2023-03-02T07:34:52.317963Z","iopub.status.idle":"2023-03-02T07:34:52.322752Z","shell.execute_reply.started":"2023-03-02T07:34:52.317921Z","shell.execute_reply":"2023-03-02T07:34:52.321431Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"# Hyper-parameter of the algorithm\nc1 = c2 = 0.1\nw1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\nw2 = 0.8 \n\n# Create particles\nn_particles = 20\nnp.random.seed(100)\nparams1 = np.array([initialize_weights(X_train.shape[1]) for i in range(n_particles)])\nparams2 = np.random.uniform(size=n_particles)\n\nparams1 = np.array(params1)\nparams2 = np.array(params2)\n\nprint(\"params1 shape is \",params1.shape,\"params2 shape is \",params2.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.324220Z","iopub.execute_input":"2023-03-02T07:34:52.324575Z","iopub.status.idle":"2023-03-02T07:34:52.336134Z","shell.execute_reply.started":"2023-03-02T07:34:52.324543Z","shell.execute_reply":"2023-03-02T07:34:52.334856Z"},"trusted":true},"execution_count":130,"outputs":[{"name":"stdout","text":"params1 shape is  (20, 4) params2 shape is  (20,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define velocity of each weight of every particle\nV_param1 = np.array([initialize_weights(X_train.shape[1]) for i in range(n_particles)])\n\n# Define velocity of each threshold of every particle\nV_param2 = np.random.uniform(size=n_particles)\n\n# Initialize objective values\npbest = (params1, params2)\npbest_obj = objective_fn_vector(params1, params2, X_train, y_train)\ngbest = (params1[pbest_obj.argmin()], params2[pbest_obj.argmin()])\ngbest_obj = pbest_obj.min()\n\nprint(\"pbest obj value for 20 particles are as follows\",pbest_obj)\nprint(\"gbest obj value among all 20 particles is as follows\",gbest_obj)\n# Note that gbest_obj should be the minimum of all pbest_obj","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.338316Z","iopub.execute_input":"2023-03-02T07:34:52.339791Z","iopub.status.idle":"2023-03-02T07:34:52.353458Z","shell.execute_reply.started":"2023-03-02T07:34:52.339731Z","shell.execute_reply":"2023-03-02T07:34:52.352544Z"},"trusted":true},"execution_count":131,"outputs":[{"name":"stdout","text":"pbest obj value for 20 particles are as follows [0.67013703 0.82232957 0.64329013 0.70573338 0.73886477 0.82232957\n 1.09729975 0.5237323  0.77244152 0.4620281  0.86703698 0.81919055\n 1.09729975 0.81919055 0.73355763 0.74030523 0.82232957 0.73805779\n 1.09729975 0.68309963]\ngbest obj value among all 20 particles is as follows 0.4620281046196322\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### the update function","metadata":{}},{"cell_type":"code","source":"def update():\n    \"Function to do one iteration of particle swarm optimization\"\n    global V_param1, V_param2, params1, params2, pbest, pbest_obj, gbest, gbest_obj\n    # these have been already initialized in the previous cells\n    \n    # Update params\n    r11, r12, r2 = np.random.rand(3)\n    V_param1 = w1 * V_param1 + c1 * r11 * (pbest[0] - params1) + c2 * r2 * (gbest[0] - params1)\n    V_param2 = w2 * V_param2 + c1 * r12 * (pbest[1] - params2) + c2 * r2 * (gbest[1] - params2)    \n   #V = w * V + c1*r11*(pbest - params1) + c2*r2*(gbest.reshape(-1,1)-X)\n    params1 = params1 + V_param1\n    params2 = params2 + V_param2\n    \n    obj = objective_fn_vector(params1, params2, X_train, y_train)\n    for i in range(pbest[0].shape[0]):\n        if pbest_obj[i] >= obj[i]:\n            pbest[0][i] = params1[i]\n            pbest[1][i] = params2[i]\n            pbest_obj[i] = obj[i]\n            \n    gbest = (params1[pbest_obj.argmin()], params2[pbest_obj.argmin()])\n    gbest_obj = pbest_obj.min()","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.354619Z","iopub.execute_input":"2023-03-02T07:34:52.355443Z","iopub.status.idle":"2023-03-02T07:34:52.365773Z","shell.execute_reply.started":"2023-03-02T07:34:52.355394Z","shell.execute_reply":"2023-03-02T07:34:52.364640Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"for i in range(100):\n    update()\nprint(\"PSO found best solution at f({})={}\".format(gbest, gbest_obj))\nprint(\"Global optimal at f({})={}\".format([param1_min,param2_min], objective_fn(param1_min, param2_min, X_train, y_train)))\n","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.367277Z","iopub.execute_input":"2023-03-02T07:34:52.367874Z","iopub.status.idle":"2023-03-02T07:34:52.664580Z","shell.execute_reply.started":"2023-03-02T07:34:52.367834Z","shell.execute_reply":"2023-03-02T07:34:52.663415Z"},"trusted":true},"execution_count":133,"outputs":[{"name":"stdout","text":"PSO found best solution at f((array([ 0.89609881, -0.78802764,  1.12817761,  0.69381515]), 0.6364592103247197))=0.4161039895073432\nGlobal optimal at f([array([0.035627  , 0.49239265, 0.95237685, 0.81057376]), 0.6723314084399745])=0.4620281046196322\n","output_type":"stream"}]},{"cell_type":"code","source":"# import some data to play with\n#load the breast cancer dataset \nbcan = load_breast_cancer()\nX = bcan.data\ny = bcan.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n     X, y, test_size=0.33, random_state=42)\n\n\n# normalize the data\nscaler = MinMaxScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\nprint(\"Shape of training data is \",X_train.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.666219Z","iopub.execute_input":"2023-03-02T07:34:52.666644Z","iopub.status.idle":"2023-03-02T07:34:52.681702Z","shell.execute_reply.started":"2023-03-02T07:34:52.666608Z","shell.execute_reply":"2023-03-02T07:34:52.680448Z"},"trusted":true},"execution_count":134,"outputs":[{"name":"stdout","text":"Shape of training data is  (381, 30)\n","output_type":"stream"}]},{"cell_type":"code","source":"n_particles = 100\nparams1 = []\nparams2 = []\nfor i in range(n_particles):\n    params1.append(initialize_weights(X_train.shape[1]))\n    params2.append(np.random.uniform())\nparams1 = np.array(params1)\nparams2 = np.array(params2)\nprint(\"Shape of params 1 (weights)\",params1.shape)\nprint(\"Shape of params 2 (thresholds)\",params2.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.686281Z","iopub.execute_input":"2023-03-02T07:34:52.686674Z","iopub.status.idle":"2023-03-02T07:34:52.705244Z","shell.execute_reply.started":"2023-03-02T07:34:52.686638Z","shell.execute_reply":"2023-03-02T07:34:52.704039Z"},"trusted":true},"execution_count":135,"outputs":[{"name":"stdout","text":"Shape of params 1 (weights) (100, 30)\nShape of params 2 (thresholds) (100,)\n","output_type":"stream"}]},{"cell_type":"code","source":"z = objective_fn_vector(params1, params2, X_train, y_train)\nglobal_min_idx = z.argmin()\nparam1_min = params1[global_min_idx]\nparam2_min = params2[global_min_idx]\nglobal_min_obj_val = z[global_min_idx]\nprint(\"param1_min\",param1_min,\"param2_min\",param2_min)","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.706700Z","iopub.execute_input":"2023-03-02T07:34:52.707070Z","iopub.status.idle":"2023-03-02T07:34:52.752028Z","shell.execute_reply.started":"2023-03-02T07:34:52.707035Z","shell.execute_reply":"2023-03-02T07:34:52.750906Z"},"trusted":true},"execution_count":136,"outputs":[{"name":"stdout","text":"param1_min [0.09646405 0.23238037 0.60906506 0.87316713 0.22716421 0.28587331\n 0.09639433 0.21543273 0.3377934  0.00175056 0.30354284 0.55109436\n 0.14945001 0.86743393 0.48016423 0.23391784 0.01983731 0.29332121\n 0.11841787 0.90289701 0.20699767 0.57530654 0.68620426 0.74267843\n 0.46333525 0.74344956 0.32585486 0.18245493 0.05052287 0.25256524] param2_min 0.9290751334299893\n","output_type":"stream"}]},{"cell_type":"code","source":"# Hyper-parameter of the algorithm\nc1 = c2 = 0.1\nw1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\nw2 = 0.8 \n# Create particles\nn_particles = 20\nparams1 = []\nparams2 = []\nfor i in range(n_particles):\n    params1.append(initialize_weights(X_train.shape[1]))\n    params2.append(np.random.uniform())\n\nparams1=np.array(params1)\nparams2=np.array(params2)\n\nprint(\"params1 shape is \",params1.shape,\"params2 shape is \",params2.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.753418Z","iopub.execute_input":"2023-03-02T07:34:52.753750Z","iopub.status.idle":"2023-03-02T07:34:52.765433Z","shell.execute_reply.started":"2023-03-02T07:34:52.753717Z","shell.execute_reply":"2023-03-02T07:34:52.764198Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stdout","text":"params1 shape is  (20, 30) params2 shape is  (20,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# define velocity of each weight of every particle\nn_particles = 20\nV_param1 = []\nV_param2 = []\nfor i in range(n_particles):\n    V_param1.append(initialize_weights(X_train.shape[1]))\n    V_param2.append(np.random.uniform())\n\nV_param1=np.array(V_param1)\nV_param2=np.array(V_param2)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.767040Z","iopub.execute_input":"2023-03-02T07:34:52.767510Z","iopub.status.idle":"2023-03-02T07:34:52.777495Z","shell.execute_reply.started":"2023-03-02T07:34:52.767463Z","shell.execute_reply":"2023-03-02T07:34:52.776063Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"# Initialize objective values\npbest = (params1.copy(), params2.copy())\npbest_obj = objective_fn_vector(params1, params2, X_train, y_train)\n\ngbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()])\ngbest_obj = pbest_obj.min()\n\nprint(\"pbest obj value for 20 particles are as follows\",pbest_obj)\nprint(\"gbest obj value among all 20 particles is as follows\",gbest_obj)\n# note that gbest_obj should be the minimim of all pbest_obj","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.779002Z","iopub.execute_input":"2023-03-02T07:34:52.779327Z","iopub.status.idle":"2023-03-02T07:34:52.799571Z","shell.execute_reply.started":"2023-03-02T07:34:52.779295Z","shell.execute_reply":"2023-03-02T07:34:52.798238Z"},"trusted":true},"execution_count":139,"outputs":[{"name":"stdout","text":"pbest obj value for 20 particles are as follows [0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461\n 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461\n 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461]\ngbest obj value among all 20 particles is as follows 0.6643460978641622\n","output_type":"stream"}]},{"cell_type":"code","source":"def update():\n    \"Function to do one iteration of particle swarm optimization\"\n    global V_param1,V_param2, params1,params2, pbest, pbest_obj, gbest, gbest_obj\n    # these have been already initialized in the previous cells\n    \n    # Update params\n    r11,r12, r2 = np.random.rand(3)\n    V_param1=w1*V_param1+c1*r11*(pbest[0] - params1)+ c2*r2*(gbest[0]-params1)\n    V_param2=w2*V_param2+c1*r12*(pbest[1] - params2)+ c2*r2*(gbest[1]-params2)    \n#     V = w * V + c1*r11*(pbest - params1) + c2*r2*(gbest.reshape(-1,1)-X)\n    params1 = params1 + V_param1\n    params2 = params2 + V_param2\n    \n    obj = objective_fn_vector(params1, params2, X_train, y_train)\n    for i in range(pbest[0].shape[0]):\n        if pbest_obj[i]>=obj[i]:\n            \n            pbest[0][i]=params1[i] # update pbest[0][i] with value of params1[i]\n            pbest[1][i]=params2[i] # update pbest[1][i] \n            pbest_obj[i]=obj[i]    # also update pbest_obj[i]\n\n            \n    gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()]) # update gbest to contain the best from params1 and params 2\n    gbest_obj = pbest_obj.min() # update gbest to get the minimum of pbest_obj\n ","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.800900Z","iopub.execute_input":"2023-03-02T07:34:52.801217Z","iopub.status.idle":"2023-03-02T07:34:52.810948Z","shell.execute_reply.started":"2023-03-02T07:34:52.801187Z","shell.execute_reply":"2023-03-02T07:34:52.809311Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"for i in range(1000):\n    update()\nprint(\"PSO found best solution at f({})={}\".format(gbest, gbest_obj))\nprint(\"Global optimal at f({})={}\".format([param1_min,param2_min], \n        objective_fn(param1_min, param2_min, X_train, y_train)))","metadata":{"execution":{"iopub.status.busy":"2023-03-02T07:34:52.812943Z","iopub.execute_input":"2023-03-02T07:34:52.813522Z","iopub.status.idle":"2023-03-02T07:34:59.402549Z","shell.execute_reply.started":"2023-03-02T07:34:52.813378Z","shell.execute_reply":"2023-03-02T07:34:59.401180Z"},"trusted":true},"execution_count":141,"outputs":[{"name":"stdout","text":"PSO found best solution at f((array([ 0.82882938, -1.37089182,  0.41875713,  0.63957963,  0.75853243,\n        2.15623174,  0.87711075,  3.55359923,  1.54238394,  3.33248445,\n        4.57515914,  6.96399765,  1.31484294,  1.05349995,  0.18102848,\n        0.5353661 ,  1.22919416,  1.95071972,  0.57930799,  4.23485133,\n        2.20263255,  1.03344666,  2.71860644,  1.05112473,  2.42302556,\n        0.31588585,  0.47904932,  0.51749065,  0.48020385,  0.49943934]), 3.7578016612026))=0.44322120624369\nGlobal optimal at f([array([0.09646405, 0.23238037, 0.60906506, 0.87316713, 0.22716421,\n       0.28587331, 0.09639433, 0.21543273, 0.3377934 , 0.00175056,\n       0.30354284, 0.55109436, 0.14945001, 0.86743393, 0.48016423,\n       0.23391784, 0.01983731, 0.29332121, 0.11841787, 0.90289701,\n       0.20699767, 0.57530654, 0.68620426, 0.74267843, 0.46333525,\n       0.74344956, 0.32585486, 0.18245493, 0.05052287, 0.25256524]), 0.9290751334299893])=0.44354386645831856\n","output_type":"stream"}]}]}