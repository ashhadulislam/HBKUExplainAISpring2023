{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.stats import entropy\nfrom sklearn.datasets import load_breast_cancer\n","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:50.521749Z","iopub.execute_input":"2023-03-30T15:51:50.522222Z","iopub.status.idle":"2023-03-30T15:51:50.529342Z","shell.execute_reply.started":"2023-03-30T15:51:50.522166Z","shell.execute_reply":"2023-03-30T15:51:50.528148Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"# import some data to play with\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n     X, y, test_size=0.33, random_state=42)\n\n\n# normalize the data\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\n# def change_weights(X_train,y_train,X_test,y_test,weights):","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:50.531487Z","iopub.execute_input":"2023-03-30T15:51:50.531836Z","iopub.status.idle":"2023-03-30T15:51:50.542886Z","shell.execute_reply.started":"2023-03-30T15:51:50.531796Z","shell.execute_reply":"2023-03-30T15:51:50.541758Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"pk = np.array([1/5, 2/5, 2/5])  # fair coin\nH = entropy(pk)\nprint(H)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:50.544294Z","iopub.execute_input":"2023-03-30T15:51:50.544655Z","iopub.status.idle":"2023-03-30T15:51:50.551924Z","shell.execute_reply.started":"2023-03-30T15:51:50.544621Z","shell.execute_reply":"2023-03-30T15:51:50.550395Z"},"trusted":true},"execution_count":132,"outputs":[{"name":"stdout","text":"1.0549201679861442\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef calculate_probabilities(list_labels, uniq_labels):\n    '''\n    Author: Sara Nassar \n    this function calculates the probabilities of each label in the list of labels\n    it is calculated by number of labels in class A/all labels\n    number of labels in class B/all labels\n    and so on\n    '''\n    \n    # A dictionary to store the probabilities\n    probabilities = dict.fromkeys(uniq_labels, 0)\n    \n    # Total number of labels\n    total_labels = len(list_labels)\n    \n    for label in uniq_labels:\n        # Counting the number of times the label occurs in the list\n        count = list_labels.count(label)\n        \n        # Calculating the probability of the label\n        probability = count / total_labels\n        \n        # Storing the calculated probability in the dictionary\n        probabilities[label] = probability\n        \n    return probabilities    \n    \n    \n# test your function\nlist_labels=[1,2,0,1,2,0]\nuniq_labels=[0,1,2]\nprint(calculate_probabilities(list_labels,uniq_labels))\n# this should print somehting like 0.33,0.33,0.33\n","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:50.554801Z","iopub.execute_input":"2023-03-30T15:51:50.555274Z","iopub.status.idle":"2023-03-30T15:51:50.565086Z","shell.execute_reply.started":"2023-03-30T15:51:50.555236Z","shell.execute_reply":"2023-03-30T15:51:50.563520Z"},"trusted":true},"execution_count":133,"outputs":[{"name":"stdout","text":"{0: 0.3333333333333333, 1: 0.3333333333333333, 2: 0.3333333333333333}\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef calc_entropy_from_probabilities(list_probas):\n    '''\n    Author: Sara Nassar \n    list_probas is the list of probabiities\n    the formula for entropy is\n    sum(-proba*log(proba))\n    \n    '''\n    \n    entropy_value = 0\n\n    for proba in list_probas:\n        # If the probability is not zero\n        if proba != 0:\n            entropy_value += -proba * np.log(proba)\n     \n    return entropy_value\n\n\n# test your function\nlist_probas=[1/5, 2/5, 2/5]\nprint(calc_entropy_from_probabilities(list_probas))\n# above should print 1.054...","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:50.566377Z","iopub.execute_input":"2023-03-30T15:51:50.567035Z","iopub.status.idle":"2023-03-30T15:51:50.575572Z","shell.execute_reply.started":"2023-03-30T15:51:50.566989Z","shell.execute_reply":"2023-03-30T15:51:50.574321Z"},"trusted":true},"execution_count":134,"outputs":[{"name":"stdout","text":"1.0549201679861442\n","output_type":"stream"}]},{"cell_type":"code","source":"def information_gain(old_entropy, new_entropies, count_items):\n    '''\n    Author: Sara Nassar \n    from the list of new entropies, calculate the overall new entropy\n    \n    formula is something like:\n    overall_new_entropy = entropy1*proportion1 + entropy2*proportion2+ entropy3*proportion3 ...\n    \n    igain=old_entropy-overall_new_entropy\n    '''\n    \n    overall_new_entropy = 0\n    \n    # Calculating the total number of items\n    total_items = sum(count_items.values())\n    \n    for i in range(len(new_entropies)):\n        if i in count_items:\n            # Calculating the proportion of items in the current partition\n            proportion = count_items[i] / total_items\n        \n            # Adding the entropy of the current partition weighted by its proportion to the overall new entropy\n            overall_new_entropy += new_entropies[i] * proportion\n        \n    # Calculating the information gain\n    information_gain = old_entropy - overall_new_entropy\n    \n    return information_gain\n\n\nold_entropy=1\nnew_entropies=[0,0.65]\ncount_items=[4,6]\ncount_items = {i: count_items[i] for i in range(len(count_items))}\nprint(information_gain(old_entropy,new_entropies,count_items)) # should print 0.61\n\n  ","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:50.578313Z","iopub.execute_input":"2023-03-30T15:51:50.578786Z","iopub.status.idle":"2023-03-30T15:51:50.590342Z","shell.execute_reply.started":"2023-03-30T15:51:50.578741Z","shell.execute_reply":"2023-03-30T15:51:50.589506Z"},"trusted":true},"execution_count":135,"outputs":[{"name":"stdout","text":"0.61\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef initialize_weights(number_features):\n    '''\n    the first set of weights corresponding to the features\n    For now, it defaults to 2\n    '''\n    \n    weights=np.array([np.random.uniform() for i in range(number_features)])\n    return weights\n    ","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:50.592003Z","iopub.execute_input":"2023-03-30T15:51:50.592640Z","iopub.status.idle":"2023-03-30T15:51:50.601069Z","shell.execute_reply.started":"2023-03-30T15:51:50.592599Z","shell.execute_reply":"2023-03-30T15:51:50.599811Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"num_feats=X_train.shape[1]\nprint(initialize_weights(num_feats))","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:50.603159Z","iopub.execute_input":"2023-03-30T15:51:50.604364Z","iopub.status.idle":"2023-03-30T15:51:50.613278Z","shell.execute_reply.started":"2023-03-30T15:51:50.604307Z","shell.execute_reply":"2023-03-30T15:51:50.612412Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stdout","text":"[0.29332121 0.11841787 0.90289701 0.20699767]\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_entropy_from_groups(new_entropies,count_items):\n    overall_new_entropy = 0\n    \n    # Calculating the total number of items\n    total_items = sum(count_items)\n    \n    for i in range(len(new_entropies)):\n        # Calculating the proportion of items in the current partition\n        proportion = count_items[i] / total_items\n        \n        # Adding the entropy of the current partition weighted by its proportion to the overall new entropy\n        overall_new_entropy += new_entropies[i] * proportion\n        \n    return overall_new_entropy    \n\ndef get_entropy(threshold,res,y_test):\n\n    # make two groups\n    group1=[]\n    group2=[]\n\n    for i in range(res.shape[0]):\n        if res[i]<threshold:\n            group1.append(y_test[i])\n        else:\n            group2.append(y_test[i])\n\n\n    proba_gr1=calculate_probabilities(group1,np.unique(group1).tolist())\n    proba_gr1=list(proba_gr1.values()) \n    entropy_group1=calc_entropy_from_probabilities(proba_gr1)\n    count_group1=len(proba_gr1)\n\n    proba_gr2=calculate_probabilities(group2,np.unique(group2).tolist())\n    proba_gr2=list(proba_gr2.values()) \n    entropy_group2=calc_entropy_from_probabilities(proba_gr2)\n    count_group2=len(proba_gr2)\n\n    new_entropies=[entropy_group1,entropy_group2]\n    count_items=[count_group1,count_group2]\n    overall_new_entropy=get_entropy_from_groups(new_entropies,count_items)\n    return overall_new_entropy\n","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:50.615073Z","iopub.execute_input":"2023-03-30T15:51:50.615409Z","iopub.status.idle":"2023-03-30T15:51:50.625948Z","shell.execute_reply.started":"2023-03-30T15:51:50.615366Z","shell.execute_reply":"2023-03-30T15:51:50.625050Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"markdown","source":"### Task4: PSO Implementation","metadata":{}},{"cell_type":"markdown","source":"#### Modified the entropy function to get a vector of entropies for n particles","metadata":{}},{"cell_type":"code","source":" def objective_fn(param1, param2, X, y):\n    '''\n    Author: Sara Nassar\n    param1 and param2 are the parameters that we want to optimize\n    say param1 is the weight vector and  param2 is the threshold\n    '''\n    # Multiply the weights with each feature and calculate the sum\n    res = np.sum(X * param1, axis=1)\n    \n    # Calculate entropy using the get_entropy function\n    entropy = get_entropy(param2, res, y)\n    \n    return entropy   \n","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:50.627014Z","iopub.execute_input":"2023-03-30T15:51:50.628130Z","iopub.status.idle":"2023-03-30T15:51:50.635662Z","shell.execute_reply.started":"2023-03-30T15:51:50.628070Z","shell.execute_reply":"2023-03-30T15:51:50.634444Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"def objective_fn_vector(params1, params2, X, y):\n    '''\n    Author: Sara Nassar\n    params1 is an array of weight vectors\n    params2 is an array of thresholds\n    '''\n    results = []\n    for i in range(params1.shape[0]):\n        param1 = params1[i] # get ith set of weights\n        param2 = params2[i] # get ith threshold\n        # call the objective_fn above to get the entropy\n        res = objective_fn(param1, param2, X, y)\n        #print(param2,res)\n        results.append(res)\n    \n    return np.array(results)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:50.638890Z","iopub.execute_input":"2023-03-30T15:51:50.639233Z","iopub.status.idle":"2023-03-30T15:51:50.646794Z","shell.execute_reply.started":"2023-03-30T15:51:50.639201Z","shell.execute_reply":"2023-03-30T15:51:50.645609Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"### Below we just randomly assign 100 particles and see if we can find the global minimum.\n### THis is just to check","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:50.648462Z","iopub.execute_input":"2023-03-30T15:51:50.649586Z","iopub.status.idle":"2023-03-30T15:51:50.661419Z","shell.execute_reply.started":"2023-03-30T15:51:50.649544Z","shell.execute_reply":"2023-03-30T15:51:50.660167Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"'''\nAuthor: Sara Nassar\nBelow we just randomly assign 100 particles and see if we can find the global minimum.\n'''\nnum_particles = 100\nnum_features = X_train.shape[1]\n\nparams1 = []\nfor i in range(num_particles):\n    weights = initialize_weights(num_features)\n    params1.append(weights)\n\n    \n# we have a list of 100 weight vectors (params1) and 100 thresholds (params2)\n# convert them to array\nparams1 = np.array(params1)\nparams2 = np.random.uniform(size=num_particles)\n\nprint(\"Shape of params 1 (weights):\", params1.shape)\nprint(\"Shape of params 2 (thresholds):\", params2.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:50.663338Z","iopub.execute_input":"2023-03-30T15:51:50.663987Z","iopub.status.idle":"2023-03-30T15:51:50.676920Z","shell.execute_reply.started":"2023-03-30T15:51:50.663946Z","shell.execute_reply":"2023-03-30T15:51:50.675691Z"},"trusted":true},"execution_count":142,"outputs":[{"name":"stdout","text":"Shape of params 1 (weights): (100, 4)\nShape of params 2 (thresholds): (100,)\n","output_type":"stream"}]},{"cell_type":"code","source":"z = objective_fn_vector(params1, params2, X_train, y_train)\n# Find the global minimum that is using the minimum if params1 and params2\nglobal_min_idx = np.argmin(z)\nparam1_min = params1[global_min_idx] # use z.argmin() to access best params1 (global_min_weight)\nparam2_min = params2[global_min_idx] # use z.argmin() to access best params2 (global_min_threshold)\n\nprint(\"param1_min\",param1_min,\"param2_min\",param2_min)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:50.678363Z","iopub.execute_input":"2023-03-30T15:51:50.679871Z","iopub.status.idle":"2023-03-30T15:51:50.705036Z","shell.execute_reply.started":"2023-03-30T15:51:50.679825Z","shell.execute_reply":"2023-03-30T15:51:50.703687Z"},"trusted":true},"execution_count":143,"outputs":[{"name":"stdout","text":"param1_min [0.8252117  0.04036757 0.78453879 0.84722558] param2_min 0.7756162615317937\n","output_type":"stream"}]},{"cell_type":"code","source":"# Hyper-parameter of the algorithm\nc1 = c2 = 0.1\nw1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\nw2 = 0.8 \n\n# Create particles\nn_particles = 20\nnp.random.seed(100)\nparams1 = np.array([initialize_weights(X_train.shape[1]) for i in range(n_particles)])\nparams2 = np.random.uniform(size=n_particles)\n\nparams1 = np.array(params1)\nparams2 = np.array(params2)\n\nprint(\"params1 shape is \",params1.shape,\"params2 shape is \",params2.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:50.706566Z","iopub.execute_input":"2023-03-30T15:51:50.706931Z","iopub.status.idle":"2023-03-30T15:51:50.716107Z","shell.execute_reply.started":"2023-03-30T15:51:50.706895Z","shell.execute_reply":"2023-03-30T15:51:50.715026Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"params1 shape is  (20, 4) params2 shape is  (20,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define velocity of each weight of every particle\nV_param1 = np.array([initialize_weights(X_train.shape[1]) for i in range(n_particles)])\n\n# Define velocity of each threshold of every particle\nV_param2 = np.random.uniform(size=n_particles)\n\n# Initialize objective values\npbest = (np.array([param.copy() for param in params1]), params2.copy())\npbest_obj = objective_fn_vector(params1, params2, X_train, y_train)\ngbest = (params1[pbest_obj.argmin()], params2[pbest_obj.argmin()])\ngbest_obj = pbest_obj.min()\n\nprint(\"pbest obj value for 20 particles are as follows\",pbest_obj)\nprint(\"gbest obj value among all 20 particles is as follows\",gbest_obj)\n# Note that gbest_obj should be the minimum of all pbest_obj","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:50.717579Z","iopub.execute_input":"2023-03-30T15:51:50.717892Z","iopub.status.idle":"2023-03-30T15:51:50.735825Z","shell.execute_reply.started":"2023-03-30T15:51:50.717862Z","shell.execute_reply":"2023-03-30T15:51:50.734629Z"},"trusted":true},"execution_count":145,"outputs":[{"name":"stdout","text":"pbest obj value for 20 particles are as follows [0.67013703 0.82232957 0.64329013 0.70573338 0.73886477 0.82232957\n 1.09729975 0.5237323  0.77244152 0.4620281  0.86703698 0.81919055\n 1.09729975 0.81919055 0.73355763 0.74030523 0.82232957 0.73805779\n 1.09729975 0.68309963]\ngbest obj value among all 20 particles is as follows 0.4620281046196322\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### the update function","metadata":{}},{"cell_type":"code","source":"def update():\n    \"Function to do one iteration of particle swarm optimization\"\n    global V_param1, V_param2, params1, params2, pbest, pbest_obj, gbest, gbest_obj\n    # these have been already initialized in the previous cells\n    \n    # Update params\n    r11, r12, r2 = np.random.rand(3)\n    V_param1 = w1 * V_param1 + c1 * r11 * (pbest[0] - params1) + c2 * r2 * (gbest[0] - params1)\n    V_param2 = w2 * V_param2 + c1 * r12 * (pbest[1] - params2) + c2 * r2 * (gbest[1] - params2)    \n   #V = w * V + c1*r11*(pbest - params1) + c2*r2*(gbest.reshape(-1,1)-X)\n    params1 = params1 + V_param1\n    params2 = params2 + V_param2\n    \n    obj = objective_fn_vector(params1, params2, X_train, y_train)\n    for i in range(pbest[0].shape[0]):\n        if pbest_obj[i] >= obj[i]:\n            pbest[0][i] = params1[i]\n            pbest[1][i] = params2[i]\n            pbest_obj[i] = obj[i]\n            \n    gbest = (params1[pbest_obj.argmin()], params2[pbest_obj.argmin()])\n    gbest_obj = pbest_obj.min()","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:50.737563Z","iopub.execute_input":"2023-03-30T15:51:50.738744Z","iopub.status.idle":"2023-03-30T15:51:50.750527Z","shell.execute_reply.started":"2023-03-30T15:51:50.738696Z","shell.execute_reply":"2023-03-30T15:51:50.749228Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"code","source":"for i in range(100):\n    update()\n\nnum_features = X_train.shape[1]\nparam1_min = initialize_weights(num_features) \n\nprint(\"PSO found best solution at f({})={}\".format(gbest, gbest_obj))\nprint(\"Global optimal at f({})={}\".format([param1_min,param2_min], objective_fn(param1_min, param2_min, X_train, y_train)))\n","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:50.752026Z","iopub.execute_input":"2023-03-30T15:51:50.752663Z","iopub.status.idle":"2023-03-30T15:51:51.045632Z","shell.execute_reply.started":"2023-03-30T15:51:50.752622Z","shell.execute_reply":"2023-03-30T15:51:51.044395Z"},"trusted":true},"execution_count":147,"outputs":[{"name":"stdout","text":"PSO found best solution at f((array([1.01229026, 0.59462971, 1.88398165, 0.60763938]), 1.248160629484322))=0.4161039895073432\nGlobal optimal at f([array([0.55252942, 0.1611271 , 0.56854896, 0.48570987]), 0.7756162615317937])=0.618501821515296\n","output_type":"stream"}]},{"cell_type":"code","source":"# import some data to play with\n#load the breast cancer dataset \nbcan = load_breast_cancer()\nX = bcan.data\ny = bcan.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n     X, y, test_size=0.33, random_state=42)\n\n\n# normalize the data\nscaler = MinMaxScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\nprint(\"Shape of training data is \",X_train.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:51.047335Z","iopub.execute_input":"2023-03-30T15:51:51.048173Z","iopub.status.idle":"2023-03-30T15:51:51.066737Z","shell.execute_reply.started":"2023-03-30T15:51:51.048131Z","shell.execute_reply":"2023-03-30T15:51:51.065592Z"},"trusted":true},"execution_count":148,"outputs":[{"name":"stdout","text":"Shape of training data is  (381, 30)\n","output_type":"stream"}]},{"cell_type":"code","source":"n_particles = 100\nparams1 = []\nparams2 = []\nfor i in range(n_particles):\n    params1.append(initialize_weights(X_train.shape[1]))\n    params2.append(np.random.uniform())\nparams1 = np.array(params1)\nparams2 = np.array(params2)\nprint(\"Shape of params 1 (weights)\",params1.shape)\nprint(\"Shape of params 2 (thresholds)\",params2.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:51.068596Z","iopub.execute_input":"2023-03-30T15:51:51.069066Z","iopub.status.idle":"2023-03-30T15:51:51.087783Z","shell.execute_reply.started":"2023-03-30T15:51:51.069017Z","shell.execute_reply":"2023-03-30T15:51:51.086270Z"},"trusted":true},"execution_count":149,"outputs":[{"name":"stdout","text":"Shape of params 1 (weights) (100, 30)\nShape of params 2 (thresholds) (100,)\n","output_type":"stream"}]},{"cell_type":"code","source":"z = objective_fn_vector(params1, params2, X_train, y_train)\nglobal_min_idx = z.argmin()\nparam1_min = params1[global_min_idx]\nparam2_min = params2[global_min_idx]\nglobal_min_obj_val = z[global_min_idx]\nprint(\"param1_min\",param1_min,\"param2_min\",param2_min)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:51.089757Z","iopub.execute_input":"2023-03-30T15:51:51.090622Z","iopub.status.idle":"2023-03-30T15:51:51.147483Z","shell.execute_reply.started":"2023-03-30T15:51:51.090573Z","shell.execute_reply":"2023-03-30T15:51:51.146264Z"},"trusted":true},"execution_count":150,"outputs":[{"name":"stdout","text":"param1_min [0.12751954 0.54369217 0.20049061 0.67016086 0.55811222 0.23237832\n 0.51558849 0.31499197 0.84659255 0.44737628 0.10008522 0.90159209\n 0.85608825 0.28632987 0.25901477 0.06637137 0.31763167 0.051843\n 0.94418832 0.7172173  0.5536592  0.35974477 0.1591823  0.43295804\n 0.27936218 0.96103763 0.09813216 0.40699555 0.00837645 0.56805893] param2_min 0.5766097683672401\n","output_type":"stream"}]},{"cell_type":"code","source":"# Hyper-parameter of the algorithm\nc1 = c2 = 0.1\nw1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\nw2 = 0.8 \n# Create particles\nn_particles = 20\nparams1 = []\nparams2 = []\nfor i in range(n_particles):\n    params1.append(initialize_weights(X_train.shape[1]))\n    params2.append(np.random.uniform())\n\nparams1=np.array(params1)\nparams2=np.array(params2)\n\nprint(\"params1 shape is \",params1.shape,\"params2 shape is \",params2.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:51.151812Z","iopub.execute_input":"2023-03-30T15:51:51.152190Z","iopub.status.idle":"2023-03-30T15:51:51.162924Z","shell.execute_reply.started":"2023-03-30T15:51:51.152156Z","shell.execute_reply":"2023-03-30T15:51:51.161721Z"},"trusted":true},"execution_count":151,"outputs":[{"name":"stdout","text":"params1 shape is  (20, 30) params2 shape is  (20,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# define velocity of each weight of every particle\nn_particles = 20\nV_param1 = []\nV_param2 = []\nfor i in range(n_particles):\n    V_param1.append(initialize_weights(X_train.shape[1]))\n    V_param2.append(np.random.uniform())\n\nV_param1=np.array(V_param1)\nV_param2=np.array(V_param2)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:51.164368Z","iopub.execute_input":"2023-03-30T15:51:51.165307Z","iopub.status.idle":"2023-03-30T15:51:51.174009Z","shell.execute_reply.started":"2023-03-30T15:51:51.165269Z","shell.execute_reply":"2023-03-30T15:51:51.172841Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"code","source":"# Initialize objective values\npbest = (params1.copy(), params2.copy())\npbest_obj = objective_fn_vector(params1, params2, X_train, y_train)\n\ngbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()])\ngbest_obj = pbest_obj.min()\n\nprint(\"pbest obj value for 20 particles are as follows\",pbest_obj)\nprint(\"gbest obj value among all 20 particles is as follows\",gbest_obj)\n# note that gbest_obj should be the minimim of all pbest_obj","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:51.175524Z","iopub.execute_input":"2023-03-30T15:51:51.176121Z","iopub.status.idle":"2023-03-30T15:51:51.195700Z","shell.execute_reply.started":"2023-03-30T15:51:51.176085Z","shell.execute_reply":"2023-03-30T15:51:51.194493Z"},"trusted":true},"execution_count":153,"outputs":[{"name":"stdout","text":"pbest obj value for 20 particles are as follows [0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461\n 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461\n 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461]\ngbest obj value among all 20 particles is as follows 0.6643460978641622\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in range(100):\n    update()\nprint(\"PSO found best solution at f({})={}\".format(gbest, gbest_obj))\nprint(\"Global optimal at f({})={}\".format([param1_min,param2_min], objective_fn(param1_min, param2_min, X_train, y_train)))","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:51.197130Z","iopub.execute_input":"2023-03-30T15:51:51.198151Z","iopub.status.idle":"2023-03-30T15:51:51.933937Z","shell.execute_reply.started":"2023-03-30T15:51:51.198111Z","shell.execute_reply":"2023-03-30T15:51:51.933035Z"},"trusted":true},"execution_count":154,"outputs":[{"name":"stdout","text":"PSO found best solution at f((array([0.81478416, 2.16135537, 0.80949853, 3.47002409, 1.53346952,\n       3.21786451, 4.69918283, 9.56614919, 1.34898383, 1.05069426,\n       0.27373452, 0.35291515, 1.29812582, 2.00260603, 0.70941125,\n       4.12196654, 2.14378816, 1.07077156, 2.79188037, 1.16440568,\n       3.67590249, 0.20751861, 0.72134805, 0.32680045, 0.46749854,\n       0.53694585, 0.43963777, 0.399247  , 0.34559994, 1.23349266]), 1.8817097915254228))=0.6643460978641622\nGlobal optimal at f([array([0.12751954, 0.54369217, 0.20049061, 0.67016086, 0.55811222,\n       0.23237832, 0.51558849, 0.31499197, 0.84659255, 0.44737628,\n       0.10008522, 0.90159209, 0.85608825, 0.28632987, 0.25901477,\n       0.06637137, 0.31763167, 0.051843  , 0.94418832, 0.7172173 ,\n       0.5536592 , 0.35974477, 0.1591823 , 0.43295804, 0.27936218,\n       0.96103763, 0.09813216, 0.40699555, 0.00837645, 0.56805893]), 0.5766097683672401])=0.6643460978641622\n","output_type":"stream"}]},{"cell_type":"code","source":"max_tree_size=128\nall_optimized_weights_list=[None for i in range(max_tree_size)]\nall_optimized_thresh_list=[None for i in range(max_tree_size)]\nall_dataset_sizes_list=[None for i in range(max_tree_size)]\nall_IG_list=[None for i in range(max_tree_size)]\n        ","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:51.935065Z","iopub.execute_input":"2023-03-30T15:51:51.935739Z","iopub.status.idle":"2023-03-30T15:51:51.942029Z","shell.execute_reply.started":"2023-03-30T15:51:51.935700Z","shell.execute_reply":"2023-03-30T15:51:51.940610Z"},"trusted":true},"execution_count":155,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:51.943379Z","iopub.execute_input":"2023-03-30T15:51:51.943806Z","iopub.status.idle":"2023-03-30T15:51:51.957537Z","shell.execute_reply.started":"2023-03-30T15:51:51.943769Z","shell.execute_reply":"2023-03-30T15:51:51.956188Z"},"trusted":true},"execution_count":156,"outputs":[{"execution_count":156,"output_type":"execute_result","data":{"text/plain":"(381, 30)"},"metadata":{}}]},{"cell_type":"code","source":"def update(V_param1,V_param2, params1,params2, pbest, pbest_obj, gbest, gbest_obj):\n    \"Function to do one iteration of particle swarm optimization\"\n    # these have been already initialized in the previous cells\n    \n    # Update params\n    r11,r12, r2 = np.random.rand(3)\n    V_param1=w1*V_param1+c1*r11*(pbest[0] - params1)+ c2*r2*(gbest[0]-params1)\n    V_param2=w2*V_param2+c1*r12*(pbest[1] - params2)+ c2*r2*(gbest[1]-params2)    \n#     V = w * V + c1*r11*(pbest - params1) + c2*r2*(gbest.reshape(-1,1)-X)\n    params1 = params1 + V_param1\n    params2 = params2 + V_param2\n    \n    obj = objective_fn_vector(params1, params2, X_train, y_train)\n    for i in range(pbest[0].shape[0]):\n        if pbest_obj[i]>=obj[i]:\n            \n            pbest[0][i]=params1[i] # update pbest[0][i] with value of params1[i]\n            pbest[1][i]=params2[i] # update pbest[1][i] \n            pbest_obj[i]=obj[i]    # also update pbest_obj[i]\n\n            \n    gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()]) # update gbest to contain the best from params1 and params 2\n    gbest_obj = pbest_obj.min() # update gbest to get the minimum of pbest_obj","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:51.959315Z","iopub.execute_input":"2023-03-30T15:51:51.959757Z","iopub.status.idle":"2023-03-30T15:51:51.970758Z","shell.execute_reply.started":"2023-03-30T15:51:51.959705Z","shell.execute_reply":"2023-03-30T15:51:51.969371Z"},"trusted":true},"execution_count":157,"outputs":[]},{"cell_type":"code","source":"max_tree_size=128\nall_optimized_weights_list=[None for i in range(max_tree_size)]\nall_optimized_thresh_list=[None for i in range(max_tree_size)]\nall_dataset_sizes_list=[None for i in range(max_tree_size)]\nall_IG_list=[None for i in range(max_tree_size)]\n\nthreshold_dict = {}\n\ndef find_best_params(train_x,train_y,test_x,test_y,node_number):\n      \n    '''\n    recursive function to get the best set of weights\n    '''\n    print(\"node_number\",node_number,\"data shape\",train_x.shape)\n    # exit condition 1: if the node_number is more than the maximum tree size, return\n    if node_number>=max_tree_size:\n        return all_optimized_weights_list, all_optimized_thresh_list, all_dataset_sizes_list, all_IG_list\n    # exit condition 2: if the training dataset has one or less rows, return\n    if train_x.shape[0]<=1:\n        return all_optimized_weights_list, all_optimized_thresh_list, all_dataset_sizes_list, all_IG_list\n    # exit condition 3: if the train_y has values from only one class (only 0s or only 1s and so on)\n    if len(np.unique(train_y))==1:\n        return all_optimized_weights_list, all_optimized_thresh_list, all_dataset_sizes_list, all_IG_list\n\n\n    # Hyper-parameter of the algorithm\n    c1 = c2 = 0.1\n    w1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\n    w2 = 0.8 \n    # Create particles\n    n_particles = 20\n    np.random.seed(100)\n    params1=[initialize_weights(X_train.shape[1]) for i in range(n_particles)] # a vector of shape n_particles,n_features\n    # call the initialize_weights function above\n\n    params2=[np.random.uniform() for i in range(n_particles)]# a vector of shape n_particles\n    # use the np.random.uniform() function\n\n    params1=np.array(params1)\n    params2=np.array(params2)\n\n    # define velocity of each weight of every particle\n    V_param1 = [initialize_weights(X_train.shape[1])*0.1 for i in range(n_particles)] # shape is same as params1\n    # once again can use initialize_weights function\n\n    #define velocity of each threshold of every particle\n    V_param2 = np.array([np.random.uniform()*0.1 for i in range(n_particles)])# shape is same as params2\n    # once again use np.random.uniform\n\n        # Initialize objective values\n    pbest = (params1,params2)\n    pbest_obj = objective_fn_vector(params1, params2, X_train, y_train)\n    gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()])\n    gbest_obj = pbest_obj.min()\n\n    new_ys = np.dot(train_x, gbest[0])\n    # normalize the new_ys\n    new_ys = (new_ys - np.min(new_ys)) / (np.max(new_ys) - np.min(new_ys))\n\n    for i in range(100):\n        update(V_param1, V_param2, params1, params2, pbest, pbest_obj, gbest, gbest_obj)\n        # calculate new_ys inside the loop\n        new_ys = np.dot(train_x, gbest[0])\n        # normalize the new_ys\n        new_ys = (new_ys - np.min(new_ys)) / (np.max(new_ys) - np.min(new_ys))\n        # calculate the threshold using the 50th percentile of new_ys\n        threshold = np.percentile(new_ys, 50) \n        #Half of the new_ys values will be greater than the threshold and half will be lower \n\n    unique, counts = np.unique(train_y, return_counts=True)\n    count_items = dict(zip(unique, counts))\n\n    # add the achieved optimized values to the lists    \n    all_optimized_weights_list[node_number]=gbest[0]\n    all_optimized_thresh_list[node_number]=threshold\n    all_dataset_sizes_list[node_number]=train_x.shape[0]\n    all_IG_list[node_number] = information_gain(train_y, new_ys, count_items)\n\n    # Save threshold value to dictionary with node number as key\n    threshold_dict[node_number] = threshold\n\n    # chop the data into two parts: left\n    train_x_left = train_x[new_ys >= threshold]\n    train_y_left = train_y[new_ys >= threshold]\n    left_child_node_num = node_number * 2 + 1\n\n    # chop the data into two parts: right\n    train_x_right = train_x[new_ys < threshold]\n    train_y_right = train_y[new_ys < threshold]\n    right_child_node_num = node_number * 2 + 2\n\n    # exit condition 4: return if information gain is 0\n    if np.allclose(information_gain(train_y, new_ys, count_items), 0):\n        return\n\n    print(\"Left\",train_x_left.shape)\n    print(\"Right\",train_x_right.shape)\n    # make the recursion call for left\n    find_best_params(train_x_left, train_y_left, test_x, test_y, left_child_node_num)\n\n    # make the recursion call for right\n    find_best_params(train_x_right, train_y_right, test_x, test_y, right_child_node_num)\n\n   # return all_optimized_weights_list, all_optimized_thresh_list, all_dataset_sizes_list, all_IG_list, threshold_dict\n","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:51.972902Z","iopub.execute_input":"2023-03-30T15:51:51.973348Z","iopub.status.idle":"2023-03-30T15:51:51.995462Z","shell.execute_reply.started":"2023-03-30T15:51:51.973308Z","shell.execute_reply":"2023-03-30T15:51:51.994332Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"code","source":"node_number=0\nfind_best_params(X_train,y_train,X_test,y_test,node_number)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:51:51.996929Z","iopub.execute_input":"2023-03-30T15:51:51.998095Z","iopub.status.idle":"2023-03-30T15:52:37.253598Z","shell.execute_reply.started":"2023-03-30T15:51:51.998055Z","shell.execute_reply":"2023-03-30T15:52:37.252263Z"},"trusted":true},"execution_count":159,"outputs":[{"name":"stdout","text":"node_number 0 data shape (381, 30)\nLeft (191, 30)\nRight (190, 30)\nnode_number 1 data shape (191, 30)\nLeft (96, 30)\nRight (95, 30)\nnode_number 3 data shape (96, 30)\nLeft (48, 30)\nRight (48, 30)\nnode_number 7 data shape (48, 30)\nLeft (24, 30)\nRight (24, 30)\nnode_number 15 data shape (24, 30)\nLeft (12, 30)\nRight (12, 30)\nnode_number 31 data shape (12, 30)\nnode_number 32 data shape (12, 30)\nLeft (6, 30)\nRight (6, 30)\nnode_number 65 data shape (6, 30)\nLeft (3, 30)\nRight (3, 30)\nnode_number 131 data shape (3, 30)\nnode_number 132 data shape (3, 30)\nnode_number 66 data shape (6, 30)\nnode_number 16 data shape (24, 30)\nnode_number 8 data shape (48, 30)\nLeft (24, 30)\nRight (24, 30)\nnode_number 17 data shape (24, 30)\nLeft (12, 30)\nRight (12, 30)\nnode_number 35 data shape (12, 30)\nnode_number 36 data shape (12, 30)\nLeft (6, 30)\nRight (6, 30)\nnode_number 73 data shape (6, 30)\nnode_number 74 data shape (6, 30)\nLeft (3, 30)\nRight (3, 30)\nnode_number 149 data shape (3, 30)\nnode_number 150 data shape (3, 30)\nnode_number 18 data shape (24, 30)\nnode_number 4 data shape (95, 30)\nLeft (48, 30)\nRight (47, 30)\nnode_number 9 data shape (48, 30)\nLeft (24, 30)\nRight (24, 30)\nnode_number 19 data shape (24, 30)\nLeft (12, 30)\nRight (12, 30)\nnode_number 39 data shape (12, 30)\nLeft (6, 30)\nRight (6, 30)\nnode_number 79 data shape (6, 30)\nLeft (3, 30)\nRight (3, 30)\nnode_number 159 data shape (3, 30)\nnode_number 160 data shape (3, 30)\nnode_number 80 data shape (6, 30)\nnode_number 40 data shape (12, 30)\nLeft (6, 30)\nRight (6, 30)\nnode_number 81 data shape (6, 30)\nLeft (3, 30)\nRight (3, 30)\nnode_number 163 data shape (3, 30)\nnode_number 164 data shape (3, 30)\nnode_number 82 data shape (6, 30)\nnode_number 20 data shape (24, 30)\nLeft (12, 30)\nRight (12, 30)\nnode_number 41 data shape (12, 30)\nLeft (6, 30)\nRight (6, 30)\nnode_number 83 data shape (6, 30)\nLeft (3, 30)\nRight (3, 30)\nnode_number 167 data shape (3, 30)\nnode_number 168 data shape (3, 30)\nnode_number 84 data shape (6, 30)\nLeft (3, 30)\nRight (3, 30)\nnode_number 169 data shape (3, 30)\nnode_number 170 data shape (3, 30)\nnode_number 42 data shape (12, 30)\nLeft (6, 30)\nRight (6, 30)\nnode_number 85 data shape (6, 30)\nLeft (3, 30)\nRight (3, 30)\nnode_number 171 data shape (3, 30)\nnode_number 172 data shape (3, 30)\nnode_number 86 data shape (6, 30)\nLeft (3, 30)\nRight (3, 30)\nnode_number 173 data shape (3, 30)\nnode_number 174 data shape (3, 30)\nnode_number 10 data shape (47, 30)\nLeft (24, 30)\nRight (23, 30)\nnode_number 21 data shape (24, 30)\nLeft (12, 30)\nRight (12, 30)\nnode_number 43 data shape (12, 30)\nLeft (6, 30)\nRight (6, 30)\nnode_number 87 data shape (6, 30)\nLeft (3, 30)\nRight (3, 30)\nnode_number 175 data shape (3, 30)\nnode_number 176 data shape (3, 30)\nnode_number 88 data shape (6, 30)\nLeft (3, 30)\nRight (3, 30)\nnode_number 177 data shape (3, 30)\nnode_number 178 data shape (3, 30)\nnode_number 44 data shape (12, 30)\nLeft (6, 30)\nRight (6, 30)\nnode_number 89 data shape (6, 30)\nLeft (3, 30)\nRight (3, 30)\nnode_number 179 data shape (3, 30)\nnode_number 180 data shape (3, 30)\nnode_number 90 data shape (6, 30)\nLeft (3, 30)\nRight (3, 30)\nnode_number 181 data shape (3, 30)\nnode_number 182 data shape (3, 30)\nnode_number 22 data shape (23, 30)\nLeft (12, 30)\nRight (11, 30)\nnode_number 45 data shape (12, 30)\nLeft (6, 30)\nRight (6, 30)\nnode_number 91 data shape (6, 30)\nnode_number 92 data shape (6, 30)\nLeft (3, 30)\nRight (3, 30)\nnode_number 185 data shape (3, 30)\nnode_number 186 data shape (3, 30)\nnode_number 46 data shape (11, 30)\nLeft (6, 30)\nRight (5, 30)\nnode_number 93 data shape (6, 30)\nLeft (3, 30)\nRight (3, 30)\nnode_number 187 data shape (3, 30)\nnode_number 188 data shape (3, 30)\nnode_number 94 data shape (5, 30)\nLeft (3, 30)\nRight (2, 30)\nnode_number 189 data shape (3, 30)\nnode_number 190 data shape (2, 30)\nnode_number 2 data shape (190, 30)\nLeft (95, 30)\nRight (95, 30)\nnode_number 5 data shape (95, 30)\nLeft (48, 30)\nRight (47, 30)\nnode_number 11 data shape (48, 30)\nLeft (24, 30)\nRight (24, 30)\nnode_number 23 data shape (24, 30)\nLeft (12, 30)\nRight (12, 30)\nnode_number 47 data shape (12, 30)\nnode_number 48 data shape (12, 30)\nLeft (6, 30)\nRight (6, 30)\nnode_number 97 data shape (6, 30)\nnode_number 98 data shape (6, 30)\nLeft (3, 30)\nRight (3, 30)\nnode_number 197 data shape (3, 30)\nnode_number 198 data shape (3, 30)\nnode_number 24 data shape (24, 30)\nnode_number 12 data shape (47, 30)\nLeft (24, 30)\nRight (23, 30)\nnode_number 25 data shape (24, 30)\nLeft (12, 30)\nRight (12, 30)\nnode_number 51 data shape (12, 30)\nLeft (6, 30)\nRight (6, 30)\nnode_number 103 data shape (6, 30)\nLeft (3, 30)\nRight (3, 30)\nnode_number 207 data shape (3, 30)\nnode_number 208 data shape (3, 30)\nnode_number 104 data shape (6, 30)\nnode_number 52 data shape (12, 30)\nnode_number 26 data shape (23, 30)\nLeft (12, 30)\nRight (11, 30)\nnode_number 53 data shape (12, 30)\nLeft (6, 30)\nRight (6, 30)\nnode_number 107 data shape (6, 30)\nLeft (3, 30)\nRight (3, 30)\nnode_number 215 data shape (3, 30)\nnode_number 216 data shape (3, 30)\nnode_number 108 data shape (6, 30)\nnode_number 54 data shape (11, 30)\nnode_number 6 data shape (95, 30)\nLeft (48, 30)\nRight (47, 30)\nnode_number 13 data shape (48, 30)\nLeft (24, 30)\nRight (24, 30)\nnode_number 27 data shape (24, 30)\nLeft (12, 30)\nRight (12, 30)\nnode_number 55 data shape (12, 30)\nLeft (6, 30)\nRight (6, 30)\nnode_number 111 data shape (6, 30)\nLeft (3, 30)\nRight (3, 30)\nnode_number 223 data shape (3, 30)\nnode_number 224 data shape (3, 30)\nnode_number 112 data shape (6, 30)\nnode_number 56 data shape (12, 30)\nnode_number 28 data shape (24, 30)\nnode_number 14 data shape (47, 30)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"all_optimized_thresh_list","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:52:37.255078Z","iopub.execute_input":"2023-03-30T15:52:37.256190Z","iopub.status.idle":"2023-03-30T15:52:37.266702Z","shell.execute_reply.started":"2023-03-30T15:52:37.256139Z","shell.execute_reply":"2023-03-30T15:52:37.265358Z"},"trusted":true},"execution_count":160,"outputs":[{"execution_count":160,"output_type":"execute_result","data":{"text/plain":"[0.22441467237833546,\n 0.2504066424997364,\n 0.6070197815984363,\n 0.16999422935636388,\n 0.3870857323197978,\n 0.40154230043943423,\n 0.6953655013125566,\n 0.18290547726336912,\n 0.5243978726934442,\n 0.4943942202059026,\n 0.4199079226819821,\n 0.5180354180900639,\n 0.5023437334491847,\n 0.490850755509969,\n None,\n 0.26670256775705437,\n None,\n 0.5365141637823322,\n None,\n 0.5925103386771322,\n 0.5738675330691457,\n 0.4530154131226026,\n 0.47245793766869004,\n 0.39313282108823944,\n None,\n 0.5966365029521083,\n 0.4190398195863058,\n 0.5754684486333098,\n None,\n None,\n None,\n None,\n 0.5332669248888788,\n None,\n None,\n None,\n 0.3492993970535968,\n None,\n None,\n 0.328778668301407,\n 0.49219480860396614,\n 0.517777224609163,\n 0.6177374815344943,\n 0.5371743156798247,\n 0.5430055792077073,\n 0.688549740541477,\n 0.3282434638570728,\n None,\n 0.4347875268433696,\n None,\n None,\n 0.7266951260247945,\n None,\n 0.5651702088281381,\n None,\n 0.5007759882503241,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n 0.46272126919476236,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n 0.407986836368041,\n None,\n None,\n None,\n None,\n 0.08014449008621063,\n None,\n 0.8057627280721823,\n None,\n 0.8094481882564915,\n 0.5262198191314693,\n 0.25999821430148395,\n 0.7354076126010943,\n 0.6757965260478651,\n 0.37602132837649455,\n 0.41480723609955844,\n 0.5725988150819645,\n None,\n 0.3508713892297092,\n 0.45751490183449905,\n 0.647799845925609,\n None,\n None,\n None,\n 0.6658385302805859,\n None,\n None,\n None,\n None,\n 0.7260821886978119,\n None,\n None,\n None,\n 0.6036707989285893,\n None,\n None,\n None,\n 0.3456316754849663,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None]"},"metadata":{}}]},{"cell_type":"code","source":"#thresh=all_optimized_thresh_list[0]\n#print(thresh)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T15:52:37.268630Z","iopub.execute_input":"2023-03-30T15:52:37.269060Z","iopub.status.idle":"2023-03-30T15:52:37.275627Z","shell.execute_reply.started":"2023-03-30T15:52:37.269020Z","shell.execute_reply":"2023-03-30T15:52:37.274514Z"},"trusted":true},"execution_count":161,"outputs":[]}]}