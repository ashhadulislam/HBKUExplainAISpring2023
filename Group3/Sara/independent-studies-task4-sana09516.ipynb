{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\nfrom scipy.stats import entropy\n\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n     X, y, test_size=0.33, random_state=42)\n\n\n# normalize the data\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\n# def change_weights(X_train,y_train,X_test,y_test,weights):","metadata":{"execution":{"iopub.status.busy":"2023-02-26T06:33:27.236222Z","iopub.execute_input":"2023-02-26T06:33:27.237002Z","iopub.status.idle":"2023-02-26T06:33:27.250477Z","shell.execute_reply.started":"2023-02-26T06:33:27.236943Z","shell.execute_reply":"2023-02-26T06:33:27.249013Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"pk = np.array([1/5, 2/5, 2/5])  # fair coin\nH = entropy(pk)\nprint(H)","metadata":{"execution":{"iopub.status.busy":"2023-02-26T06:33:27.253037Z","iopub.execute_input":"2023-02-26T06:33:27.253541Z","iopub.status.idle":"2023-02-26T06:33:27.271834Z","shell.execute_reply.started":"2023-02-26T06:33:27.253478Z","shell.execute_reply":"2023-02-26T06:33:27.269292Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"1.0549201679861442\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef calculate_probabilities(list_labels, uniq_labels):\n    '''\n    Author: Sara Nassar \n    this function calculates the probabilities of each label in the list of labels\n    it is calculated by number of labels in class A/all labels\n    number of labels in class B/all labels\n    and so on\n    '''\n    \n    # A dictionary to store the probabilities\n    probabilities = dict.fromkeys(uniq_labels, 0)\n    \n    # Total number of labels\n    total_labels = len(list_labels)\n    \n    for label in uniq_labels:\n        # Counting the number of times the label occurs in the list\n        count = list_labels.count(label)\n        \n        # Calculating the probability of the label\n        probability = count / total_labels\n        \n        # Storing the calculated probability in the dictionary\n        probabilities[label] = probability\n        \n    return probabilities    \n    \n    \n# test your function\nlist_labels=[1,2,0,1,2,0]\nuniq_labels=[0,1,2]\nprint(calculate_probabilities(list_labels,uniq_labels))\n# this should print somehting like 0.33,0.33,0.33\n","metadata":{"execution":{"iopub.status.busy":"2023-02-26T06:33:27.275625Z","iopub.execute_input":"2023-02-26T06:33:27.277250Z","iopub.status.idle":"2023-02-26T06:33:27.291714Z","shell.execute_reply.started":"2023-02-26T06:33:27.277187Z","shell.execute_reply":"2023-02-26T06:33:27.289754Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"{0: 0.3333333333333333, 1: 0.3333333333333333, 2: 0.3333333333333333}\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef calc_entropy_from_probabilities(list_probas):\n    '''\n    Author: Sara Nassar \n    list_probas is the list of probabiities\n    the formula for entropy is\n    sum(-proba*log(proba))\n    \n    '''\n    \n    entropy_value = 0\n\n    for proba in list_probas:\n        # If the probability is not zero\n        if proba != 0:\n            entropy_value += -proba * np.log(proba)\n     \n    return entropy_value\n\n\n# test your function\nlist_probas=[1/5, 2/5, 2/5]\nprint(calc_entropy_from_probabilities(list_probas))\n# above should print 1.054...","metadata":{"execution":{"iopub.status.busy":"2023-02-26T06:33:27.293264Z","iopub.execute_input":"2023-02-26T06:33:27.294188Z","iopub.status.idle":"2023-02-26T06:33:27.309588Z","shell.execute_reply.started":"2023-02-26T06:33:27.294141Z","shell.execute_reply":"2023-02-26T06:33:27.307894Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"1.0549201679861442\n","output_type":"stream"}]},{"cell_type":"code","source":"def information_gain(old_entropy,new_entropies,count_items):\n    '''\n    Author: Sara Nassar \n    from the list of new entropies, calculate the overall new entropy\n    \n    formula is something like:\n    overall_new_entropy = entropy1*proportion1 + entropy2*proportion2+ entropy3*proportion3 ...\n    \n    igain=old_entropy-overall_new_entropy\n    '''\n    \n    overall_new_entropy = 0\n    \n    # Calculating the total number of items\n    total_items = sum(count_items)\n    \n    for i in range(len(new_entropies)):\n        # Calculating the proportion of items in the current partition\n        proportion = count_items[i] / total_items\n        \n        # Adding the entropy of the current partition weighted by its proportion to the overall new entropy\n        overall_new_entropy += new_entropies[i] * proportion\n        \n    # Calculating the information gain\n    information_gain = old_entropy - overall_new_entropy\n    \n    return information_gain\n\n#test your function\nold_entropy=1\nnew_entropies=[0,0.65]\ncount_items=[4,6]\nprint(information_gain(old_entropy,new_entropies,count_items))\n# above should print 0.61\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-02-26T06:33:27.312568Z","iopub.execute_input":"2023-02-26T06:33:27.313002Z","iopub.status.idle":"2023-02-26T06:33:27.328431Z","shell.execute_reply.started":"2023-02-26T06:33:27.312965Z","shell.execute_reply":"2023-02-26T06:33:27.326526Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"0.61\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef initialize_weights(number_features):\n    '''\n    the first set of weights corresponding to the features\n    For now, it defaults to 2\n    '''\n    \n    weights=np.array([np.random.uniform() for i in range(number_features)])\n    return weights\n    ","metadata":{"execution":{"iopub.status.busy":"2023-02-26T06:33:27.329940Z","iopub.execute_input":"2023-02-26T06:33:27.330963Z","iopub.status.idle":"2023-02-26T06:33:27.348819Z","shell.execute_reply.started":"2023-02-26T06:33:27.330909Z","shell.execute_reply":"2023-02-26T06:33:27.347439Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"num_feats=X_train.shape[1]\nprint(initialize_weights(num_feats))","metadata":{"execution":{"iopub.status.busy":"2023-02-26T06:33:27.350557Z","iopub.execute_input":"2023-02-26T06:33:27.351344Z","iopub.status.idle":"2023-02-26T06:33:27.366676Z","shell.execute_reply.started":"2023-02-26T06:33:27.351285Z","shell.execute_reply":"2023-02-26T06:33:27.364446Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"[0.55252942 0.1611271  0.56854896 0.48570987]\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_entropy_from_groups(new_entropies,count_items):\n    overall_new_entropy = 0\n    \n    # Calculating the total number of items\n    total_items = sum(count_items)\n    \n    for i in range(len(new_entropies)):\n        # Calculating the proportion of items in the current partition\n        proportion = count_items[i] / total_items\n        \n        # Adding the entropy of the current partition weighted by its proportion to the overall new entropy\n        overall_new_entropy += new_entropies[i] * proportion\n        \n    return overall_new_entropy    \n\ndef get_entropy(threshold,res,y_test):\n\n    # make two groups\n    group1=[]\n    group2=[]\n\n    for i in range(res.shape[0]):\n        if res[i]<threshold:\n            group1.append(y_test[i])\n        else:\n            group2.append(y_test[i])\n\n\n\n\n    proba_gr1=calculate_probabilities(group1,np.unique(group1).tolist())\n    proba_gr1=list(proba_gr1.values()) \n    entropy_group1=calc_entropy_from_probabilities(proba_gr1)\n    count_group1=len(proba_gr1)\n\n    proba_gr2=calculate_probabilities(group2,np.unique(group2).tolist())\n    proba_gr2=list(proba_gr2.values()) \n    entropy_group2=calc_entropy_from_probabilities(proba_gr2)\n    count_group2=len(proba_gr2)\n\n    new_entropies=[entropy_group1,entropy_group2]\n    count_items=[count_group1,count_group2]\n    overall_new_entropy=get_entropy_from_groups(new_entropies,count_items)\n    return overall_new_entropy\n","metadata":{"execution":{"iopub.status.busy":"2023-02-26T06:33:27.369177Z","iopub.execute_input":"2023-02-26T06:33:27.370260Z","iopub.status.idle":"2023-02-26T06:33:27.384906Z","shell.execute_reply.started":"2023-02-26T06:33:27.370207Z","shell.execute_reply":"2023-02-26T06:33:27.383403Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"### Task4: PSO Implementation","metadata":{}},{"cell_type":"markdown","source":"#### Modified the entropy function to get a vector of entropies for n particles","metadata":{}},{"cell_type":"code","source":" def objective_fn(param1, param2, X, y):\n    '''\n    Author: Sara Nassar\n    param1 and param2 are the parameters that we want to optimize\n    say param1 is the weight vector and  param2 is the threshold\n    '''\n    # Multiply the weights with each feature and calculate the sum\n    res = np.sum(X * param1, axis=1)\n    \n    # Calculate entropy using the get_entropy function\n    entropy = get_entropy(param2, res, y)\n    \n    return entropy   \n\n    \ndef objective_fn_vector(params1, params2, X, y):\n    '''\n    Author: Sara Nassar\n    params1 is an array of weight vectors\n    params2 is an array of thresholds\n    '''\n    results = []\n    for i in range(params1.shape[0]):\n        param1 = params1[i] # get ith set of weights\n        param2 = params2[i] # get ith threshold\n        # call the objective_fn above to get the entropy\n        res = objective_fn(param1, param2, X, y)\n        #print(param2,res)\n        results.append(res)\n    \n    return np.array(results)","metadata":{"execution":{"iopub.status.busy":"2023-02-26T06:33:27.495287Z","iopub.execute_input":"2023-02-26T06:33:27.496734Z","iopub.status.idle":"2023-02-26T06:33:27.508002Z","shell.execute_reply.started":"2023-02-26T06:33:27.496664Z","shell.execute_reply":"2023-02-26T06:33:27.506188Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"### Below we just randomly assign 100 particles and see if we can find the global minimum.\n### THis is just to check","metadata":{"execution":{"iopub.status.busy":"2023-02-26T06:33:27.511229Z","iopub.execute_input":"2023-02-26T06:33:27.512496Z","iopub.status.idle":"2023-02-26T06:33:27.530429Z","shell.execute_reply.started":"2023-02-26T06:33:27.512428Z","shell.execute_reply":"2023-02-26T06:33:27.527913Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"'''\nAuthor: Sara Nassar\nBelow we just randomly assign 100 particles and see if we can find the global minimum.\n'''\nnum_particles = 100\nnum_features = X_train.shape[1]\n\nparams1 = []\nfor i in range(num_particles):\n    weights = initialize_weights(num_features)\n    params1.append(weights)\n\n    \n# we have a list of 100 weight vectors (params1) and 100 thresholds (params2)\n# convert them to array\nparams1 = np.array(params1)\nparams2 = np.random.uniform(size=num_particles)\n\nprint(\"Shape of params 1 (weights):\", params1.shape)\nprint(\"Shape of params 2 (thresholds):\", params2.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-26T06:33:27.533377Z","iopub.execute_input":"2023-02-26T06:33:27.534174Z","iopub.status.idle":"2023-02-26T06:33:27.550420Z","shell.execute_reply.started":"2023-02-26T06:33:27.534107Z","shell.execute_reply":"2023-02-26T06:33:27.549030Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"Shape of params 1 (weights): (100, 4)\nShape of params 2 (thresholds): (100,)\n","output_type":"stream"}]},{"cell_type":"code","source":"z = objective_fn_vector(params1, params2, X_train, y_train)\n# Find the global minimum that is using the minimum if params1 and params2\nglobal_min_idx = np.argmin(z)\nparam1_min = params1[global_min_idx] # use z.argmin() to access best params1\nparam2_min = params2[global_min_idx] # use z.argmin() to access best params2\n\nprint(\"param1_min\",param1_min,\"param2_min\",param2_min)","metadata":{"execution":{"iopub.status.busy":"2023-02-26T06:33:27.552497Z","iopub.execute_input":"2023-02-26T06:33:27.553095Z","iopub.status.idle":"2023-02-26T06:33:27.590988Z","shell.execute_reply.started":"2023-02-26T06:33:27.553042Z","shell.execute_reply":"2023-02-26T06:33:27.589382Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"param1_min [0.40984647 0.20704059 0.95602267 0.40601118] param2_min 0.5911130758518441\n","output_type":"stream"}]},{"cell_type":"code","source":"### Setting up the particles and other parameters now","metadata":{"execution":{"iopub.status.busy":"2023-02-26T06:33:27.594264Z","iopub.execute_input":"2023-02-26T06:33:27.594754Z","iopub.status.idle":"2023-02-26T06:33:27.600447Z","shell.execute_reply.started":"2023-02-26T06:33:27.594713Z","shell.execute_reply":"2023-02-26T06:33:27.598541Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# Hyper-parameter of the algorithm\nc1 = c2 = 0.1\nw1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\nw2 = 0.8 \n\n# Create particles\nn_particles = 20\nnp.random.seed(100)\nparams1 = np.array([initialize_weights(X_train.shape[1]) for i in range(n_particles)])\nparams2 = np.random.uniform(size=n_particles)\n\nparams1 = np.array(params1)\nparams2 = np.array(params2)\n\nprint(\"params1 shape is \",params1.shape,\"params2 shape is \",params2.shape)","metadata":{"execution":{"iopub.status.busy":"2023-02-26T06:33:27.604086Z","iopub.execute_input":"2023-02-26T06:33:27.604635Z","iopub.status.idle":"2023-02-26T06:33:27.619388Z","shell.execute_reply.started":"2023-02-26T06:33:27.604581Z","shell.execute_reply":"2023-02-26T06:33:27.617884Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"params1 shape is  (20, 4) params2 shape is  (20,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define velocity of each weight of every particle\nV_param1 = np.array([initialize_weights(X_train.shape[1]) for i in range(n_particles)])\n\n# Define velocity of each threshold of every particle\nV_param2 = np.random.uniform(size=n_particles)\n\n# Initialize objective values\npbest = (params1, params2)\npbest_obj = objective_fn_vector(params1, params2, X_train, y_train)\ngbest = (params1[pbest_obj.argmin()], params2[pbest_obj.argmin()])\ngbest_obj = pbest_obj.min()\n\nprint(\"pbest obj value for 20 particles are as follows\",pbest_obj)\nprint(\"gbest obj value among all 20 particles is as follows\",gbest_obj)\n# Note that gbest_obj should be the minimum of all pbest_obj","metadata":{"execution":{"iopub.status.busy":"2023-02-26T06:33:27.621503Z","iopub.execute_input":"2023-02-26T06:33:27.622875Z","iopub.status.idle":"2023-02-26T06:33:27.644384Z","shell.execute_reply.started":"2023-02-26T06:33:27.622787Z","shell.execute_reply":"2023-02-26T06:33:27.641870Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"pbest obj value for 20 particles are as follows [0.67013703 0.82232957 0.64329013 0.70573338 0.73886477 0.82232957\n 1.09729975 0.5237323  0.77244152 0.4620281  0.86703698 0.81919055\n 1.09729975 0.81919055 0.73355763 0.74030523 0.82232957 0.73805779\n 1.09729975 0.68309963]\ngbest obj value among all 20 particles is as follows 0.4620281046196322\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### the update function","metadata":{}},{"cell_type":"code","source":"def update():\n    \"Function to do one iteration of particle swarm optimization\"\n    global V_param1, V_param2, params1, params2, pbest, pbest_obj, gbest, gbest_obj\n    # these have been already initialized in the previous cells\n    \n    # Update params\n    r11, r12, r2 = np.random.rand(3)\n    V_param1 = w1 * V_param1 + c1 * r11 * (pbest[0] - params1) + c2 * r2 * (gbest[0] - params1)\n    V_param2 = w2 * V_param2 + c1 * r12 * (pbest[1] - params2) + c2 * r2 * (gbest[1] - params2)    \n#     V = w * V + c1*r11*(pbest - params1) + c2*r2*(gbest.reshape(-1,1)-X)\n    params1 = params1 + V_param1\n    params2 = params2 + V_param2\n    \n    obj = objective_fn_vector(params1, params2, X_train, y_train)\n    for i in range(pbest[0].shape[0]):\n        if pbest_obj[i] >= obj[i]:\n            pbest[0][i] = params1[i]\n            pbest[1][i] = params2[i]\n            pbest_obj[i] = obj[i]\n            \n    gbest = (params1[pbest_obj.argmin()], params2[pbest_obj.argmin()])\n    gbest_obj = pbest_obj.min()","metadata":{"execution":{"iopub.status.busy":"2023-02-26T06:33:27.646376Z","iopub.execute_input":"2023-02-26T06:33:27.647009Z","iopub.status.idle":"2023-02-26T06:33:27.662846Z","shell.execute_reply.started":"2023-02-26T06:33:27.646953Z","shell.execute_reply":"2023-02-26T06:33:27.660953Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"for i in range(100):\n    update()\nprint(\"PSO found best solution at f({})={}\".format(gbest, gbest_obj))\nprint(\"Global optimal at f({})={}\".format([param1_min,param2_min], objective_fn(param1_min, param2_min, X_train, y_train)))\n","metadata":{"execution":{"iopub.status.busy":"2023-02-26T06:33:27.665575Z","iopub.execute_input":"2023-02-26T06:33:27.666374Z","iopub.status.idle":"2023-02-26T06:33:27.989439Z","shell.execute_reply.started":"2023-02-26T06:33:27.666313Z","shell.execute_reply":"2023-02-26T06:33:27.987772Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"PSO found best solution at f((array([ 0.77394123, -0.1722639 ,  0.88045306,  0.74404172]), 0.7465800267589203))=0.4161039895073432\nGlobal optimal at f([array([0.40984647, 0.20704059, 0.95602267, 0.40601118]), 0.5911130758518441])=0.4620281046196322\n","output_type":"stream"}]}]}