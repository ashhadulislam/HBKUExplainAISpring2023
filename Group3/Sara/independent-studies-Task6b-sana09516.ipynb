{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.stats import entropy\nfrom sklearn.datasets import load_breast_cancer\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.143193Z","iopub.execute_input":"2023-03-26T19:35:15.143668Z","iopub.status.idle":"2023-03-26T19:35:15.150821Z","shell.execute_reply.started":"2023-03-26T19:35:15.143627Z","shell.execute_reply":"2023-03-26T19:35:15.149502Z"},"trusted":true},"execution_count":364,"outputs":[]},{"cell_type":"code","source":"# import some data to play with\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n     X, y, test_size=0.33, random_state=42)\n\n\n# normalize the data\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\n# def change_weights(X_train,y_train,X_test,y_test,weights):","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.152787Z","iopub.execute_input":"2023-03-26T19:35:15.153161Z","iopub.status.idle":"2023-03-26T19:35:15.168020Z","shell.execute_reply.started":"2023-03-26T19:35:15.153125Z","shell.execute_reply":"2023-03-26T19:35:15.166165Z"},"trusted":true},"execution_count":365,"outputs":[]},{"cell_type":"code","source":"pk = np.array([1/5, 2/5, 2/5])  # fair coin\nH = entropy(pk)\nprint(H)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.171988Z","iopub.execute_input":"2023-03-26T19:35:15.172651Z","iopub.status.idle":"2023-03-26T19:35:15.183757Z","shell.execute_reply.started":"2023-03-26T19:35:15.172591Z","shell.execute_reply":"2023-03-26T19:35:15.182426Z"},"trusted":true},"execution_count":366,"outputs":[{"name":"stdout","text":"1.0549201679861442\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef calculate_probabilities(list_labels, uniq_labels):\n    '''\n    Author: Sara Nassar \n    this function calculates the probabilities of each label in the list of labels\n    it is calculated by number of labels in class A/all labels\n    number of labels in class B/all labels\n    and so on\n    '''\n    \n    # A dictionary to store the probabilities\n    probabilities = dict.fromkeys(uniq_labels, 0)\n    \n    # Total number of labels\n    total_labels = len(list_labels)\n    \n    for label in uniq_labels:\n        # Counting the number of times the label occurs in the list\n        count = list_labels.count(label)\n        \n        # Calculating the probability of the label\n        probability = count / total_labels\n        \n        # Storing the calculated probability in the dictionary\n        probabilities[label] = probability\n        \n    return probabilities    \n    \n    \n# test your function\nlist_labels=[1,2,0,1,2,0]\nuniq_labels=[0,1,2]\nprint(calculate_probabilities(list_labels,uniq_labels))\n# this should print somehting like 0.33,0.33,0.33\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.185621Z","iopub.execute_input":"2023-03-26T19:35:15.186304Z","iopub.status.idle":"2023-03-26T19:35:15.199516Z","shell.execute_reply.started":"2023-03-26T19:35:15.186249Z","shell.execute_reply":"2023-03-26T19:35:15.198088Z"},"trusted":true},"execution_count":367,"outputs":[{"name":"stdout","text":"{0: 0.3333333333333333, 1: 0.3333333333333333, 2: 0.3333333333333333}\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef calc_entropy_from_probabilities(list_probas):\n    '''\n    Author: Sara Nassar \n    list_probas is the list of probabiities\n    the formula for entropy is\n    sum(-proba*log(proba))\n    \n    '''\n    \n    entropy_value = 0\n\n    for proba in list_probas:\n        # If the probability is not zero\n        if proba != 0:\n            entropy_value += -proba * np.log(proba)\n     \n    return entropy_value\n\n\n# test your function\nlist_probas=[1/5, 2/5, 2/5]\nprint(calc_entropy_from_probabilities(list_probas))\n# above should print 1.054...","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.202104Z","iopub.execute_input":"2023-03-26T19:35:15.202645Z","iopub.status.idle":"2023-03-26T19:35:15.212184Z","shell.execute_reply.started":"2023-03-26T19:35:15.202609Z","shell.execute_reply":"2023-03-26T19:35:15.210101Z"},"trusted":true},"execution_count":368,"outputs":[{"name":"stdout","text":"1.0549201679861442\n","output_type":"stream"}]},{"cell_type":"code","source":"def information_gain(old_entropy,new_entropies,count_items):\n    '''\n    Author: Sara Nassar \n    from the list of new entropies, calculate the overall new entropy\n    \n    formula is something like:\n    overall_new_entropy = entropy1*proportion1 + entropy2*proportion2+ entropy3*proportion3 ...\n    \n    igain=old_entropy-overall_new_entropy\n    '''\n    \n    overall_new_entropy = 0\n    \n    # Calculating the total number of items\n    total_items = sum(count_items)\n    \n    for i in range(len(new_entropies)):\n        # Calculating the proportion of items in the current partition\n        proportion = count_items[i] / total_items\n        \n        # Adding the entropy of the current partition weighted by its proportion to the overall new entropy\n        overall_new_entropy += new_entropies[i] * proportion\n        \n    # Calculating the information gain\n    information_gain = old_entropy - overall_new_entropy\n    \n    return information_gain\n\n#test your function\nold_entropy=1\nnew_entropies=[0,0.65]\ncount_items=[4,6]\nprint(information_gain(old_entropy,new_entropies,count_items))\n# above should print 0.61\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.214406Z","iopub.execute_input":"2023-03-26T19:35:15.214737Z","iopub.status.idle":"2023-03-26T19:35:15.233947Z","shell.execute_reply.started":"2023-03-26T19:35:15.214709Z","shell.execute_reply":"2023-03-26T19:35:15.231173Z"},"trusted":true},"execution_count":369,"outputs":[{"name":"stdout","text":"0.61\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef initialize_weights(number_features):\n    '''\n    the first set of weights corresponding to the features\n    For now, it defaults to 2\n    '''\n    \n    weights=np.array([np.random.uniform() for i in range(number_features)])\n    return weights\n    ","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.236368Z","iopub.execute_input":"2023-03-26T19:35:15.237572Z","iopub.status.idle":"2023-03-26T19:35:15.244226Z","shell.execute_reply.started":"2023-03-26T19:35:15.237521Z","shell.execute_reply":"2023-03-26T19:35:15.242817Z"},"trusted":true},"execution_count":370,"outputs":[]},{"cell_type":"code","source":"num_feats=X_train.shape[1]\nprint(initialize_weights(num_feats))","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.245558Z","iopub.execute_input":"2023-03-26T19:35:15.247543Z","iopub.status.idle":"2023-03-26T19:35:15.256649Z","shell.execute_reply.started":"2023-03-26T19:35:15.247463Z","shell.execute_reply":"2023-03-26T19:35:15.255408Z"},"trusted":true},"execution_count":371,"outputs":[{"name":"stdout","text":"[0.29332121 0.11841787 0.90289701 0.20699767]\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_entropy_from_groups(new_entropies,count_items):\n    overall_new_entropy = 0\n    \n    # Calculating the total number of items\n    total_items = sum(count_items)\n    \n    for i in range(len(new_entropies)):\n        # Calculating the proportion of items in the current partition\n        proportion = count_items[i] / total_items\n        \n        # Adding the entropy of the current partition weighted by its proportion to the overall new entropy\n        overall_new_entropy += new_entropies[i] * proportion\n        \n    return overall_new_entropy    \n\ndef get_entropy(threshold,res,y_test):\n\n    # make two groups\n    group1=[]\n    group2=[]\n\n    for i in range(res.shape[0]):\n        if res[i]<threshold:\n            group1.append(y_test[i])\n        else:\n            group2.append(y_test[i])\n\n\n\n\n    proba_gr1=calculate_probabilities(group1,np.unique(group1).tolist())\n    proba_gr1=list(proba_gr1.values()) \n    entropy_group1=calc_entropy_from_probabilities(proba_gr1)\n    count_group1=len(proba_gr1)\n\n    proba_gr2=calculate_probabilities(group2,np.unique(group2).tolist())\n    proba_gr2=list(proba_gr2.values()) \n    entropy_group2=calc_entropy_from_probabilities(proba_gr2)\n    count_group2=len(proba_gr2)\n\n    new_entropies=[entropy_group1,entropy_group2]\n    count_items=[count_group1,count_group2]\n    overall_new_entropy=get_entropy_from_groups(new_entropies,count_items)\n    return overall_new_entropy\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.257883Z","iopub.execute_input":"2023-03-26T19:35:15.259049Z","iopub.status.idle":"2023-03-26T19:35:15.269905Z","shell.execute_reply.started":"2023-03-26T19:35:15.259001Z","shell.execute_reply":"2023-03-26T19:35:15.268709Z"},"trusted":true},"execution_count":372,"outputs":[]},{"cell_type":"markdown","source":"### Task4: PSO Implementation","metadata":{}},{"cell_type":"markdown","source":"#### Modified the entropy function to get a vector of entropies for n particles","metadata":{}},{"cell_type":"code","source":" def objective_fn(param1, param2, X, y):\n    '''\n    Author: Sara Nassar\n    param1 and param2 are the parameters that we want to optimize\n    say param1 is the weight vector and  param2 is the threshold\n    '''\n    # Multiply the weights with each feature and calculate the sum\n    res = np.sum(X * param1, axis=1)\n    \n    # Calculate entropy using the get_entropy function\n    entropy = get_entropy(param2, res, y)\n    \n    return entropy   \n\n    \ndef objective_fn_vector(params1, params2, X, y):\n    '''\n    Author: Sara Nassar\n    params1 is an array of weight vectors\n    params2 is an array of thresholds\n    '''\n    results = []\n    for i in range(params1.shape[0]):\n        param1 = params1[i] # get ith set of weights\n        param2 = params2[i] # get ith threshold\n        # call the objective_fn above to get the entropy\n        res = objective_fn(param1, param2, X, y)\n        #print(param2,res)\n        results.append(res)\n    \n    return np.array(results)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.272162Z","iopub.execute_input":"2023-03-26T19:35:15.272654Z","iopub.status.idle":"2023-03-26T19:35:15.289254Z","shell.execute_reply.started":"2023-03-26T19:35:15.272614Z","shell.execute_reply":"2023-03-26T19:35:15.287777Z"},"trusted":true},"execution_count":373,"outputs":[]},{"cell_type":"code","source":"### Below we just randomly assign 100 particles and see if we can find the global minimum.\n### THis is just to check","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.295066Z","iopub.execute_input":"2023-03-26T19:35:15.296069Z","iopub.status.idle":"2023-03-26T19:35:15.306398Z","shell.execute_reply.started":"2023-03-26T19:35:15.295984Z","shell.execute_reply":"2023-03-26T19:35:15.305169Z"},"trusted":true},"execution_count":374,"outputs":[]},{"cell_type":"code","source":"'''\nAuthor: Sara Nassar\nBelow we just randomly assign 100 particles and see if we can find the global minimum.\n'''\nnum_particles = 100\nnum_features = X_train.shape[1]\n\nparams1 = []\nfor i in range(num_particles):\n    weights = initialize_weights(num_features)\n    params1.append(weights)\n\n    \n# we have a list of 100 weight vectors (params1) and 100 thresholds (params2)\n# convert them to array\nparams1 = np.array(params1)\nparams2 = np.random.uniform(size=num_particles)\n\nprint(\"Shape of params 1 (weights):\", params1.shape)\nprint(\"Shape of params 2 (thresholds):\", params2.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.307573Z","iopub.execute_input":"2023-03-26T19:35:15.308909Z","iopub.status.idle":"2023-03-26T19:35:15.322399Z","shell.execute_reply.started":"2023-03-26T19:35:15.308840Z","shell.execute_reply":"2023-03-26T19:35:15.320738Z"},"trusted":true},"execution_count":375,"outputs":[{"name":"stdout","text":"Shape of params 1 (weights): (100, 4)\nShape of params 2 (thresholds): (100,)\n","output_type":"stream"}]},{"cell_type":"code","source":"z = objective_fn_vector(params1, params2, X_train, y_train)\n# Find the global minimum that is using the minimum if params1 and params2\nglobal_min_idx = np.argmin(z)\nparam1_min = params1[global_min_idx] # use z.argmin() to access best params1\nparam2_min = params2[global_min_idx] # use z.argmin() to access best params2\n\nprint(\"param1_min\",param1_min,\"param2_min\",param2_min)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.325648Z","iopub.execute_input":"2023-03-26T19:35:15.326126Z","iopub.status.idle":"2023-03-26T19:35:15.351089Z","shell.execute_reply.started":"2023-03-26T19:35:15.326080Z","shell.execute_reply":"2023-03-26T19:35:15.349455Z"},"trusted":true},"execution_count":376,"outputs":[{"name":"stdout","text":"param1_min [0.8252117  0.04036757 0.78453879 0.84722558] param2_min 0.7756162615317937\n","output_type":"stream"}]},{"cell_type":"code","source":"### Setting up the particles and other parameters now","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.352555Z","iopub.execute_input":"2023-03-26T19:35:15.353554Z","iopub.status.idle":"2023-03-26T19:35:15.358797Z","shell.execute_reply.started":"2023-03-26T19:35:15.353514Z","shell.execute_reply":"2023-03-26T19:35:15.357073Z"},"trusted":true},"execution_count":377,"outputs":[]},{"cell_type":"code","source":"# Hyper-parameter of the algorithm\nc1 = c2 = 0.1\nw1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\nw2 = 0.8 \n\n# Create particles\nn_particles = 20\nnp.random.seed(100)\nparams1 = np.array([initialize_weights(X_train.shape[1]) for i in range(n_particles)])\nparams2 = np.random.uniform(size=n_particles)\n\nparams1 = np.array(params1)\nparams2 = np.array(params2)\n\nprint(\"params1 shape is \",params1.shape,\"params2 shape is \",params2.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.360488Z","iopub.execute_input":"2023-03-26T19:35:15.360815Z","iopub.status.idle":"2023-03-26T19:35:15.374925Z","shell.execute_reply.started":"2023-03-26T19:35:15.360783Z","shell.execute_reply":"2023-03-26T19:35:15.372555Z"},"trusted":true},"execution_count":378,"outputs":[{"name":"stdout","text":"params1 shape is  (20, 4) params2 shape is  (20,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define velocity of each weight of every particle\nV_param1 = np.array([initialize_weights(X_train.shape[1]) for i in range(n_particles)])\n\n# Define velocity of each threshold of every particle\nV_param2 = np.random.uniform(size=n_particles)\n\n# Initialize objective values\npbest = (params1, params2)\npbest_obj = objective_fn_vector(params1, params2, X_train, y_train)\ngbest = (params1[pbest_obj.argmin()], params2[pbest_obj.argmin()])\ngbest_obj = pbest_obj.min()\n\nprint(\"pbest obj value for 20 particles are as follows\",pbest_obj)\nprint(\"gbest obj value among all 20 particles is as follows\",gbest_obj)\n# Note that gbest_obj should be the minimum of all pbest_obj","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.376800Z","iopub.execute_input":"2023-03-26T19:35:15.377463Z","iopub.status.idle":"2023-03-26T19:35:15.393330Z","shell.execute_reply.started":"2023-03-26T19:35:15.377423Z","shell.execute_reply":"2023-03-26T19:35:15.392210Z"},"trusted":true},"execution_count":379,"outputs":[{"name":"stdout","text":"pbest obj value for 20 particles are as follows [0.67013703 0.82232957 0.64329013 0.70573338 0.73886477 0.82232957\n 1.09729975 0.5237323  0.77244152 0.4620281  0.86703698 0.81919055\n 1.09729975 0.81919055 0.73355763 0.74030523 0.82232957 0.73805779\n 1.09729975 0.68309963]\ngbest obj value among all 20 particles is as follows 0.4620281046196322\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### the update function","metadata":{}},{"cell_type":"code","source":"def update():\n    \"Function to do one iteration of particle swarm optimization\"\n    global V_param1, V_param2, params1, params2, pbest, pbest_obj, gbest, gbest_obj\n    # these have been already initialized in the previous cells\n    \n    # Update params\n    r11, r12, r2 = np.random.rand(3)\n    V_param1 = w1 * V_param1 + c1 * r11 * (pbest[0] - params1) + c2 * r2 * (gbest[0] - params1)\n    V_param2 = w2 * V_param2 + c1 * r12 * (pbest[1] - params2) + c2 * r2 * (gbest[1] - params2)    \n   #V = w * V + c1*r11*(pbest - params1) + c2*r2*(gbest.reshape(-1,1)-X)\n    params1 = params1 + V_param1\n    params2 = params2 + V_param2\n    \n    obj = objective_fn_vector(params1, params2, X_train, y_train)\n    for i in range(pbest[0].shape[0]):\n        if pbest_obj[i] >= obj[i]:\n            pbest[0][i] = params1[i]\n            pbest[1][i] = params2[i]\n            pbest_obj[i] = obj[i]\n            \n    gbest = (params1[pbest_obj.argmin()], params2[pbest_obj.argmin()])\n    gbest_obj = pbest_obj.min()","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.395113Z","iopub.execute_input":"2023-03-26T19:35:15.395911Z","iopub.status.idle":"2023-03-26T19:35:15.406068Z","shell.execute_reply.started":"2023-03-26T19:35:15.395873Z","shell.execute_reply":"2023-03-26T19:35:15.404098Z"},"trusted":true},"execution_count":380,"outputs":[]},{"cell_type":"code","source":"for i in range(100):\n    update()\nprint(\"PSO found best solution at f({})={}\".format(gbest, gbest_obj))\nprint(\"Global optimal at f({})={}\".format([param1_min,param2_min], objective_fn(param1_min, param2_min, X_train, y_train)))\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.410544Z","iopub.execute_input":"2023-03-26T19:35:15.411194Z","iopub.status.idle":"2023-03-26T19:35:15.632121Z","shell.execute_reply.started":"2023-03-26T19:35:15.411150Z","shell.execute_reply":"2023-03-26T19:35:15.631000Z"},"trusted":true},"execution_count":381,"outputs":[{"name":"stdout","text":"PSO found best solution at f((array([1.01229026, 0.59462971, 1.88398165, 0.60763938]), 1.248160629484322))=0.4161039895073432\nGlobal optimal at f([array([0.8252117 , 0.04036757, 0.78453879, 0.84722558]), 0.7756162615317937])=0.4620281046196322\n","output_type":"stream"}]},{"cell_type":"code","source":"# import some data to play with\n#load the breast cancer dataset \nbcan = load_breast_cancer()\nX = bcan.data\ny = bcan.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n     X, y, test_size=0.33, random_state=42)\n\n\n# normalize the data\nscaler = MinMaxScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\nprint(\"Shape of training data is \",X_train.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.634717Z","iopub.execute_input":"2023-03-26T19:35:15.635241Z","iopub.status.idle":"2023-03-26T19:35:15.657340Z","shell.execute_reply.started":"2023-03-26T19:35:15.635200Z","shell.execute_reply":"2023-03-26T19:35:15.655375Z"},"trusted":true},"execution_count":382,"outputs":[{"name":"stdout","text":"Shape of training data is  (381, 30)\n","output_type":"stream"}]},{"cell_type":"code","source":"n_particles = 100\nparams1 = []\nparams2 = []\nfor i in range(n_particles):\n    params1.append(initialize_weights(X_train.shape[1]))\n    params2.append(np.random.uniform())\nparams1 = np.array(params1)\nparams2 = np.array(params2)\nprint(\"Shape of params 1 (weights)\",params1.shape)\nprint(\"Shape of params 2 (thresholds)\",params2.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.659361Z","iopub.execute_input":"2023-03-26T19:35:15.660584Z","iopub.status.idle":"2023-03-26T19:35:15.678911Z","shell.execute_reply.started":"2023-03-26T19:35:15.660509Z","shell.execute_reply":"2023-03-26T19:35:15.677129Z"},"trusted":true},"execution_count":383,"outputs":[{"name":"stdout","text":"Shape of params 1 (weights) (100, 30)\nShape of params 2 (thresholds) (100,)\n","output_type":"stream"}]},{"cell_type":"code","source":"z = objective_fn_vector(params1, params2, X_train, y_train)\nglobal_min_idx = z.argmin()\nparam1_min = params1[global_min_idx]\nparam2_min = params2[global_min_idx]\nglobal_min_obj_val = z[global_min_idx]\nprint(\"param1_min\",param1_min,\"param2_min\",param2_min)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.680699Z","iopub.execute_input":"2023-03-26T19:35:15.681112Z","iopub.status.idle":"2023-03-26T19:35:15.721102Z","shell.execute_reply.started":"2023-03-26T19:35:15.681075Z","shell.execute_reply":"2023-03-26T19:35:15.719369Z"},"trusted":true},"execution_count":384,"outputs":[{"name":"stdout","text":"param1_min [0.09646405 0.23238037 0.60906506 0.87316713 0.22716421 0.28587331\n 0.09639433 0.21543273 0.3377934  0.00175056 0.30354284 0.55109436\n 0.14945001 0.86743393 0.48016423 0.23391784 0.01983731 0.29332121\n 0.11841787 0.90289701 0.20699767 0.57530654 0.68620426 0.74267843\n 0.46333525 0.74344956 0.32585486 0.18245493 0.05052287 0.25256524] param2_min 0.9290751334299893\n","output_type":"stream"}]},{"cell_type":"code","source":"# Hyper-parameter of the algorithm\nc1 = c2 = 0.1\nw1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\nw2 = 0.8 \n# Create particles\nn_particles = 20\nparams1 = []\nparams2 = []\nfor i in range(n_particles):\n    params1.append(initialize_weights(X_train.shape[1]))\n    params2.append(np.random.uniform())\n\nparams1=np.array(params1)\nparams2=np.array(params2)\n\nprint(\"params1 shape is \",params1.shape,\"params2 shape is \",params2.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.722727Z","iopub.execute_input":"2023-03-26T19:35:15.723828Z","iopub.status.idle":"2023-03-26T19:35:15.735492Z","shell.execute_reply.started":"2023-03-26T19:35:15.723753Z","shell.execute_reply":"2023-03-26T19:35:15.733744Z"},"trusted":true},"execution_count":385,"outputs":[{"name":"stdout","text":"params1 shape is  (20, 30) params2 shape is  (20,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# define velocity of each weight of every particle\nn_particles = 20\nV_param1 = []\nV_param2 = []\nfor i in range(n_particles):\n    V_param1.append(initialize_weights(X_train.shape[1]))\n    V_param2.append(np.random.uniform())\n\nV_param1=np.array(V_param1)\nV_param2=np.array(V_param2)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.736918Z","iopub.execute_input":"2023-03-26T19:35:15.737318Z","iopub.status.idle":"2023-03-26T19:35:15.751084Z","shell.execute_reply.started":"2023-03-26T19:35:15.737283Z","shell.execute_reply":"2023-03-26T19:35:15.749425Z"},"trusted":true},"execution_count":386,"outputs":[]},{"cell_type":"code","source":"# Initialize objective values\npbest = (params1.copy(), params2.copy())\npbest_obj = objective_fn_vector(params1, params2, X_train, y_train)\n\ngbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()])\ngbest_obj = pbest_obj.min()\n\nprint(\"pbest obj value for 20 particles are as follows\",pbest_obj)\nprint(\"gbest obj value among all 20 particles is as follows\",gbest_obj)\n# note that gbest_obj should be the minimim of all pbest_obj","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.755106Z","iopub.execute_input":"2023-03-26T19:35:15.755500Z","iopub.status.idle":"2023-03-26T19:35:15.773931Z","shell.execute_reply.started":"2023-03-26T19:35:15.755468Z","shell.execute_reply":"2023-03-26T19:35:15.773086Z"},"trusted":true},"execution_count":387,"outputs":[{"name":"stdout","text":"pbest obj value for 20 particles are as follows [0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461\n 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461\n 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461]\ngbest obj value among all 20 particles is as follows 0.6643460978641622\n","output_type":"stream"}]},{"cell_type":"code","source":"def update( V_param1,V_param2, params1,params2, pbest, pbest_obj, gbest, gbest_obj):\n    \"Function to do one iteration of particle swarm optimization\"\n    \n    # Update params\n    r11,r12, r2 = np.random.rand(3)\n    V_param1=w1*V_param1+c1*r11*(pbest[0] - params1)+ c2*r2*(gbest[0]-params1)\n    V_param2=w2*V_param2+c1*r12*(pbest[1] - params2)+ c2*r2*(gbest[1]-params2)    \n#     V = w * V + c1*r11*(pbest - params1) + c2*r2*(gbest.reshape(-1,1)-X)\n    params1 = params1 + V_param1\n    params2 = params2 + V_param2\n    \n    obj = objective_fn_vector(params1, params2, X_train, y_train)\n    for i in range(pbest[0].shape[0]):\n        if pbest_obj[i]>=obj[i]:\n            \n            pbest[0][i]=params1[i] # update pbest[0][i] with value of params1[i]\n            pbest[1][i]=params2[i] # update pbest[1][i] \n            pbest_obj[i]=obj[i]    # also update pbest_obj[i]\n\n            \n    gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()]) # update gbest to contain the best from params1 and params 2\n    gbest_obj = pbest_obj.min() # update gbest to get the minimum of pbest_obj\n ","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.775416Z","iopub.execute_input":"2023-03-26T19:35:15.775680Z","iopub.status.idle":"2023-03-26T19:35:15.786029Z","shell.execute_reply.started":"2023-03-26T19:35:15.775655Z","shell.execute_reply":"2023-03-26T19:35:15.784299Z"},"trusted":true},"execution_count":388,"outputs":[]},{"cell_type":"code","source":"for i in range(100):\n    update(V_param1,V_param2, params1,params2, pbest, pbest_obj, gbest, gbest_obj)\nprint(\"PSO found best solution at f({})={}\".format(gbest, gbest_obj))\nprint(\"Global optimal at f({})={}\".format([param1_min,param2_min], objective_fn(param1_min, param2_min, X_train, y_train)))","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:15.787058Z","iopub.execute_input":"2023-03-26T19:35:15.787401Z","iopub.status.idle":"2023-03-26T19:35:16.349539Z","shell.execute_reply.started":"2023-03-26T19:35:15.787366Z","shell.execute_reply":"2023-03-26T19:35:16.347818Z"},"trusted":true},"execution_count":389,"outputs":[{"name":"stdout","text":"PSO found best solution at f((array([0.29482165, 0.66487564, 0.18899771, 0.51658582, 0.80484256,\n       0.22853715, 0.50392967, 0.96223569, 0.79690381, 0.53795513,\n       0.07641012, 0.10222709, 0.76223726, 0.78468469, 0.26346198,\n       0.15417163, 0.59836823, 0.80791872, 0.61136397, 0.99933888,\n       0.81623974, 0.9859729 , 0.72479975, 0.54714228, 0.33726881,\n       0.14463337, 0.40559921, 0.23977194, 0.43991383, 0.46742472]), 0.056860663574915815))=0.6643460978641622\nGlobal optimal at f([array([0.09646405, 0.23238037, 0.60906506, 0.87316713, 0.22716421,\n       0.28587331, 0.09639433, 0.21543273, 0.3377934 , 0.00175056,\n       0.30354284, 0.55109436, 0.14945001, 0.86743393, 0.48016423,\n       0.23391784, 0.01983731, 0.29332121, 0.11841787, 0.90289701,\n       0.20699767, 0.57530654, 0.68620426, 0.74267843, 0.46333525,\n       0.74344956, 0.32585486, 0.18245493, 0.05052287, 0.25256524]), 0.9290751334299893])=0.44354386645831856\n","output_type":"stream"}]},{"cell_type":"code","source":"max_tree_size=128\nall_optimized_weights_list=[None for i in range(max_tree_size)]\nall_optimized_thresh_list=[None for i in range(max_tree_size)]\nall_dataset_sizes_list=[None for i in range(max_tree_size)]\nall_IG_list=[None for i in range(max_tree_size)]\n        ","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:16.352313Z","iopub.execute_input":"2023-03-26T19:35:16.352804Z","iopub.status.idle":"2023-03-26T19:35:16.359604Z","shell.execute_reply.started":"2023-03-26T19:35:16.352758Z","shell.execute_reply":"2023-03-26T19:35:16.358169Z"},"trusted":true},"execution_count":390,"outputs":[]},{"cell_type":"code","source":"def find_best_params(train_x,train_y,test_x,test_y,node_number):\n      \n    '''\n    recursive function to get the best set of weights\n    '''\n    print(\"node_number\",node_number,\"data shape\",train_x.shape)\n    # exit condition 1: if the node_number is more than the maximum tree size, return\n    if node_number>=max_tree_size:\n        return\n    # exit condition 2: if the training dataset has one or less rows, return\n    if train_x.shape[0]<=1:\n        return\n    # exit condition 3: if the train_y has values from only one class (only 0s or only 1s and so on)\n    if len(np.unique(train_y))==1:\n        return\n    \n    # use the initialized lists as global\n    global all_optimized_weights_list\n    global all_optimized_thresh_list\n    global all_dataset_sizes_list\n    global all_IG_list\n\n    # Hyper-parameter of the algorithm\n    c1 = c2 = 0.1\n    w1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\n    w2 = 0.8 \n    # Create particles\n    n_particles = 20\n    np.random.seed(100)\n    params1=[initialize_weights(X_train.shape[1]) for i in range(n_particles)] # a vector of shape n_particles,n_features\n    # call the initialize_weights function above\n\n    params2=[np.random.uniform() for i in range(n_particles)]# a vector of shape n_particles\n    # use the np.random.uniform() function\n\n    params1=np.array(params1)\n    params2=np.array(params2)\n\n    # define velocity of each weight of every particle\n    V_param1 = [initialize_weights(X_train.shape[1])*0.1 for i in range(n_particles)] # shape is same as params1\n    # once again can use initialize_weights function\n\n    #define velocity of each threshold of every particle\n    V_param2 = np.array([np.random.uniform()*0.1 for i in range(n_particles)])# shape is same as params2\n    # once again use np.random.uniform() function\n\n    # Initialize objective values\n    pbest = (params1,params2)\n    pbest_obj = objective_fn_vector(params1, params2, X_train, y_train)\n    gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()])\n    gbest_obj = pbest_obj.min()\n    \n    new_ys = None  # initialize new_ys variable\n    for i in range(100):\n        update(V_param1, V_param2, params1, params2, pbest, pbest_obj, gbest, gbest_obj)\n        # calculate new_ys inside the loop\n        new_ys = np.dot(train_x, gbest[0])\n        # normalize the new_ys\n        new_ys = (new_ys - np.min(new_ys)) / (np.max(new_ys) - np.min(new_ys))\n\n        #Error in the count items or in the number of nodes .. \n    unique, counts = np.unique(train_y, return_counts=True)\n    count_items = dict(zip(unique, counts))\n\n    # add the achieved optimized values to the lists    \n    all_optimized_weights_list[node_number]=gbest[0]\n    all_optimized_thresh_list[node_number]=gbest[1]\n    all_dataset_sizes_list[node_number]=train_x.shape[0]\n    all_IG_list[node_number] = information_gain(train_y, new_ys, count_items)\n\n   # chop the data into two parts: left\n    train_x_left = train_x[new_ys >= gbest[1]]\n    train_y_left = train_y[new_ys >= gbest[1]]\n    left_child_node_num = node_number * 2 + 1\n\n    # chop the data into two parts: right\n    train_x_right = train_x[new_ys < gbest[1]]\n    train_y_right = train_y[new_ys < gbest[1]]\n    right_child_node_num = node_number * 2 + 2\n\n    # exit condition 4: return if information gain is 0\n    # here my code has error\n    if information_gain(train_y, new_ys) == 0:\n        return\n\n    print(\"Left\", train_x_left.shape)\n    print(\"Right\", train_x_right.shape)\n\n    # make the recursion call for left\n    find_best_params(train_x_left, train_y_left, test_x, test_y, left_child_node_num)\n\n    # make the recursion call for right\n    find_best_params(train_x_right, train_y_right, test_x, test_y, right_child_node_num)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:41:55.835456Z","iopub.execute_input":"2023-03-26T19:41:55.836006Z","iopub.status.idle":"2023-03-26T19:41:55.857478Z","shell.execute_reply.started":"2023-03-26T19:41:55.835957Z","shell.execute_reply":"2023-03-26T19:41:55.854651Z"},"trusted":true},"execution_count":404,"outputs":[]},{"cell_type":"code","source":"node_number=0\nfind_best_params(X_train,y_train,X_test,y_test,node_number)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:41:58.534517Z","iopub.execute_input":"2023-03-26T19:41:58.534975Z","iopub.status.idle":"2023-03-26T19:41:59.392030Z","shell.execute_reply.started":"2023-03-26T19:41:58.534935Z","shell.execute_reply":"2023-03-26T19:41:59.390756Z"},"trusted":true},"execution_count":405,"outputs":[{"name":"stdout","text":"node_number 0 data shape (381, 30)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/1375373163.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnode_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfind_best_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnode_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_27/2523883552.py\u001b[0m in \u001b[0;36mfind_best_params\u001b[0;34m(train_x, train_y, test_x, test_y, node_number)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mall_optimized_thresh_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_number\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mall_dataset_sizes_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_number\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mall_IG_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_number\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minformation_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m    \u001b[0;31m# chop the data into two parts: left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/943559086.py\u001b[0m in \u001b[0;36minformation_gain\u001b[0;34m(old_entropy, new_entropies, count_items)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_entropies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Calculating the proportion of items in the current partition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mproportion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_items\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Adding the entropy of the current partition weighted by its proportion to the overall new entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 2"],"ename":"KeyError","evalue":"2","output_type":"error"}]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:30.375043Z","iopub.execute_input":"2023-03-26T19:35:30.375607Z","iopub.status.idle":"2023-03-26T19:35:30.386429Z","shell.execute_reply.started":"2023-03-26T19:35:30.375554Z","shell.execute_reply":"2023-03-26T19:35:30.384641Z"},"trusted":true},"execution_count":393,"outputs":[{"execution_count":393,"output_type":"execute_result","data":{"text/plain":"(381, 30)"},"metadata":{}}]},{"cell_type":"code","source":"all_optimized_thresh_list","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:37.874915Z","iopub.execute_input":"2023-03-26T19:35:37.875416Z","iopub.status.idle":"2023-03-26T19:35:37.885089Z","shell.execute_reply.started":"2023-03-26T19:35:37.875343Z","shell.execute_reply":"2023-03-26T19:35:37.883900Z"},"trusted":true},"execution_count":394,"outputs":[{"execution_count":394,"output_type":"execute_result","data":{"text/plain":"[0.4098464718143954,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None]"},"metadata":{}}]},{"cell_type":"code","source":"thresh=all_optimized_thresh_list[0]\nprint(thresh)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T19:35:58.796773Z","iopub.execute_input":"2023-03-26T19:35:58.797332Z","iopub.status.idle":"2023-03-26T19:35:58.803894Z","shell.execute_reply.started":"2023-03-26T19:35:58.797281Z","shell.execute_reply":"2023-03-26T19:35:58.802566Z"},"trusted":true},"execution_count":397,"outputs":[{"name":"stdout","text":"0.4098464718143954\n","output_type":"stream"}]}]}