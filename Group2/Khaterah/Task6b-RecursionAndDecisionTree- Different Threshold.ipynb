{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75b137d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "import lib as lib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba4c42",
   "metadata": {},
   "source": [
    "### Working on breast cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d91a34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some data to play with\n",
    "#load the breast cancer dataset \n",
    "bcan = datasets.load_breast_cancer()\n",
    "X = bcan.data\n",
    "y = bcan.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3aaecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of params 1 (weights) (100, 30)\n",
      "Shape of params 2 (thresholds) (100,)\n"
     ]
    }
   ],
   "source": [
    "# initialize params1 and params2\n",
    "\n",
    "params1=[lib.initialize_weights(X_train.shape[1]) for i in range(100)]# a vector of shape 100,4\n",
    "# call the initialize_weights function above\n",
    "\n",
    "params2=[np.random.uniform() for i in range(100)]# a vector of shape 100\n",
    "# use the np.random.uniform() function\n",
    "\n",
    "# we have a list of 100 weight vectors (params1) and 100 thresholds (params2)\n",
    "# convert them to array\n",
    "params1=np.array(params1)\n",
    "params2=np.array(params2)\n",
    "\n",
    "\n",
    "print(\"Shape of params 1 (weights)\",params1.shape)\n",
    "print(\"Shape of params 2 (thresholds)\",params2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81b9d22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param1_min [0.29332121 0.11841787 0.90289701 0.20699767 0.57530654 0.68620426\n",
      " 0.74267843 0.46333525 0.74344956 0.32585486 0.18245493 0.05052287\n",
      " 0.25256524 0.92907513 0.68984837 0.31908874 0.49173868 0.35238822\n",
      " 0.85396625 0.25328364 0.2837559  0.1907728  0.54990109 0.52196241\n",
      " 0.14893894 0.68850451 0.91048032 0.48135845 0.57524243 0.55891037] param2_min 0.48199824425601057\n",
      "0.6643460978641622\n"
     ]
    }
   ],
   "source": [
    "z = lib.objective_fn_vector(params1, params2, X_train, y_train)\n",
    "# Find the global minimum\n",
    "param1_min = params1[z.argmin()] # use z.argmin()\n",
    "param2_min = params2[z.argmin()] # use z.argmin()\n",
    "\n",
    "print(\"param1_min\",param1_min,\"param2_min\",param2_min)\n",
    "print(lib.objective_fn(param1_min, param2_min, X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d82dc762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params1 shape is  (20, 30) params2 shape is  (20,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyper-parameter of the algorithm\n",
    "c1 = c2 = 0.1\n",
    "w1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\n",
    "w2 = 0.8 \n",
    "# Create particles\n",
    "n_particles = 20\n",
    "np.random.seed(100)\n",
    "params1=[lib.initialize_weights(X_train.shape[1]) for i in range(n_particles)] # a vector of shape n_particles,4\n",
    "# call the initialize_weights function above\n",
    "\n",
    "params2=[np.random.uniform() for i in range(n_particles)]# a vector of shape n_particles\n",
    "# use the np.random.uniform() function\n",
    "\n",
    "params1=np.array(params1)\n",
    "params2=np.array(params2)\n",
    "\n",
    "print(\"params1 shape is \",params1.shape,\"params2 shape is \",params2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7442b873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pbest obj value for 20 particles are as follows [0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461\n",
      " 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461\n",
      " 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461 0.6643461]\n",
      "gbest obj value among all 20 particles is as follows 0.6643460978641622\n"
     ]
    }
   ],
   "source": [
    "# define velocity of each weight of every particle\n",
    "V_param1 = [lib.initialize_weights(X_train.shape[1])*0.1 for i in range(n_particles)] # shape is same as params1\n",
    "# once again can use initialize_weights function\n",
    "\n",
    "#define velocity of each threshold of every particle\n",
    "V_param2 = np.array([np.random.uniform()*0.1 for i in range(n_particles)])# shape is same as params2\n",
    "# once again use np.random.uniform() function\n",
    "\n",
    "# Initialize objective values\n",
    "pbest = (params1,params2)\n",
    "pbest_obj = lib.objective_fn_vector(params1, params2, X_train, y_train)\n",
    "gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()])\n",
    "gbest_obj = pbest_obj.min()\n",
    "\n",
    "print(\"pbest obj value for 20 particles are as follows\",pbest_obj)\n",
    "print(\"gbest obj value among all 20 particles is as follows\",gbest_obj)\n",
    "# note that gbest_obj should be the minimim of all pbest_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8dd5d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(V_param1,V_param2, params1,params2, pbest, pbest_obj, gbest, gbest_obj):\n",
    "    \"Function to do one iteration of particle swarm optimization\"\n",
    "    # these have been already initialized in the previous cells\n",
    "    \n",
    "    # Update params\n",
    "    r11,r12, r2 = np.random.rand(3)\n",
    "    V_param1=w1*V_param1+c1*r11*(pbest[0] - params1)+ c2*r2*(gbest[0]-params1)\n",
    "    V_param2=w2*V_param2+c1*r12*(pbest[1] - params2)+ c2*r2*(gbest[1]-params2)    \n",
    "#     V = w * V + c1*r11*(pbest - params1) + c2*r2*(gbest.reshape(-1,1)-X)\n",
    "    params1 = params1 + V_param1\n",
    "    params2 = params2 + V_param2\n",
    "    \n",
    "    obj = lib.objective_fn_vector(params1, params2, X_train, y_train)\n",
    "    for i in range(pbest[0].shape[0]):\n",
    "        if pbest_obj[i]>=obj[i]:\n",
    "            \n",
    "            pbest[0][i]=params1[i] # update pbest[0][i] with value of params1[i]\n",
    "            pbest[1][i]=params2[i] # update pbest[1][i] \n",
    "            pbest_obj[i]=obj[i]    # also update pbest_obj[i]\n",
    "\n",
    "            \n",
    "    gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()]) # update gbest to contain the best from params1 and params 2\n",
    "    gbest_obj = pbest_obj.min() # update gbest to get the minimum of pbest_obj\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd9d3cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSO found best solution at f((array([7.31640885, 8.20255845, 1.88323015, 5.06529472, 0.69221023,\n",
      "       0.52359245, 2.09916329, 7.75930097, 0.73487131, 2.48495038,\n",
      "       1.60724141, 4.57559109, 0.20111294, 2.3197124 , 4.72442403,\n",
      "       4.98683773, 3.10366565, 3.96090088, 4.46208784, 4.53378587,\n",
      "       0.43599061, 1.25557602, 2.81187997, 0.78539846, 3.59300447,\n",
      "       1.81209468, 1.32580087, 4.90588436, 5.59965104, 2.13579209]), 0.4098464718143954))=0.6643460978641622\n",
      "Global optimal at f([array([0.29332121, 0.11841787, 0.90289701, 0.20699767, 0.57530654,\n",
      "       0.68620426, 0.74267843, 0.46333525, 0.74344956, 0.32585486,\n",
      "       0.18245493, 0.05052287, 0.25256524, 0.92907513, 0.68984837,\n",
      "       0.31908874, 0.49173868, 0.35238822, 0.85396625, 0.25328364,\n",
      "       0.2837559 , 0.1907728 , 0.54990109, 0.52196241, 0.14893894,\n",
      "       0.68850451, 0.91048032, 0.48135845, 0.57524243, 0.55891037]), 0.48199824425601057])=0.6643460978641622\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    update(V_param1,V_param2, params1,params2, pbest, pbest_obj, gbest, gbest_obj)\n",
    "print(\"PSO found best solution at f({})={}\".format(gbest, gbest_obj))\n",
    "print(\"Global optimal at f({})={}\".format([param1_min,param2_min], lib.objective_fn(param1_min, param2_min, X_train, y_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55437371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the threshold values for each node. \n",
    "'''\n",
    "This is important because the decision tree algorithm uses the threshold values to determine which path to take down the tree.\n",
    "'''\n",
    "threshold_dict = {}\n",
    "# Initialize lists to store the optimized weights, thresholds, dataset sizes, and information gains for each node\n",
    "max_tree_size=128\n",
    "all_optimized_weights_list=[None for i in range(max_tree_size)]\n",
    "all_optimized_thresh_list=[None for i in range(max_tree_size)]\n",
    "all_dataset_sizes_list=[None for i in range(max_tree_size)]\n",
    "all_IG_list=[None for i in range(max_tree_size)]\n",
    "\n",
    "def find_best_params(train_x,train_y,test_x,test_y,node_number):\n",
    "      \n",
    "    '''\n",
    "    recursive function to get the best set of weights\n",
    "    '''\n",
    "    print(\"node_number\",node_number,\"data shape\",train_x.shape)\n",
    "    # exit condition 1: if the node_number is more than the maximum tree size, return\n",
    "    if node_number>=max_tree_size:\n",
    "        return \n",
    "    # exit condition 2: if the training dataset has one or less rows, return\n",
    "    if train_x.shape[0]<=1:\n",
    "        return \n",
    "    # exit condition 3: if the train_y has values from only one class (only 0s or only 1s and so on)\n",
    "    if len(np.unique(train_y))==1:\n",
    "        return\n",
    "\n",
    "    # use the initialized lists as global\n",
    "    global all_optimized_weights_list\n",
    "    global all_optimized_thresh_list\n",
    "    global all_dataset_sizes_list\n",
    "    \n",
    "    \n",
    "    # Hyper-parameter of the algorithm\n",
    "    c1 = c2 = 0.1\n",
    "    w1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\n",
    "    w2 = 0.8 \n",
    "    # Create particles\n",
    "    n_particles = 20\n",
    "    np.random.seed(100)\n",
    "    params1=[lib.initialize_weights(X_train.shape[1]) for i in range(n_particles)] # a vector of shape n_particles,n_features\n",
    "    # call the initialize_weights function above\n",
    "\n",
    "    params2=[np.random.uniform() for i in range(n_particles)]# a vector of shape n_particles\n",
    "    # use the np.random.uniform() function\n",
    "\n",
    "    params1=np.array(params1)\n",
    "    params2=np.array(params2)\n",
    "\n",
    "    # define velocity of each weight of every particle\n",
    "    V_param1 = [lib.initialize_weights(X_train.shape[1])*0.1 for i in range(n_particles)] # shape is same as params1\n",
    "    # once again can use initialize_weights function\n",
    "\n",
    "    #define velocity of each threshold of every particle\n",
    "    V_param2 = np.array([np.random.uniform()*0.1 for i in range(n_particles)])# shape is same as params2\n",
    "    # once again use np.random.uniform\n",
    "\n",
    "    # Initialize objective values\n",
    "    pbest = (params1,params2)\n",
    "    pbest_obj = lib.objective_fn_vector(params1, params2, X_train, y_train)\n",
    "    gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()])\n",
    "    gbest_obj = pbest_obj.min()\n",
    "    \n",
    "    #     print(\"pbest obj value for 20 particles are as follows\",pbest_obj)\n",
    "    #     print(\"gbest obj value among all 20 particles is as follows\",gbest_obj)  \n",
    "\n",
    "    new_ys = np.dot(train_x, gbest[0])\n",
    "    # normalize the new_ys\n",
    "    new_ys = (new_ys - np.min(new_ys)) / (np.max(new_ys) - np.min(new_ys))\n",
    "\n",
    "    for i in range(100):\n",
    "        update(V_param1, V_param2, params1, params2, pbest, pbest_obj, gbest, gbest_obj)\n",
    "        # calculate new_ys inside the loop\n",
    "    new_ys = np.dot(train_x, gbest[0])\n",
    "    # normalize the new_ys\n",
    "    new_ys = (new_ys - np.min(new_ys)) / (np.max(new_ys) - np.min(new_ys))\n",
    "    # calculate the threshold using the 50th percentile of new_ys\n",
    "    threshold = np.percentile(new_ys, 50) \n",
    "    #Half of the new_ys values will be greater than the threshold and half will be lower \n",
    "\n",
    "    unique, counts = np.unique(train_y, return_counts=True)\n",
    "    #will be used in information gain\n",
    "    count_items = dict(zip(unique, counts))\n",
    "\n",
    "    # add the achieved optimized values to the lists    \n",
    "    all_optimized_weights_list[node_number]=gbest[0]\n",
    "    all_optimized_thresh_list[node_number]=threshold\n",
    "    all_dataset_sizes_list[node_number]=train_x.shape[0]\n",
    "    all_IG_list[node_number] = lib.information_gain(train_y, new_ys, count_items)\n",
    "\n",
    "    # Save threshold value to dictionary with node number as key\n",
    "    threshold_dict[node_number] = threshold\n",
    "\n",
    "    # chop the data into two parts: left\n",
    "    train_x_left = train_x[new_ys >= threshold]\n",
    "    train_y_left = train_y[new_ys >= threshold]\n",
    "    left_child_node_num = node_number * 2 + 1\n",
    "\n",
    "    # chop the data into two parts: right\n",
    "    train_x_right = train_x[new_ys < threshold]\n",
    "    train_y_right = train_y[new_ys < threshold]\n",
    "    right_child_node_num = node_number * 2 + 2\n",
    "\n",
    "    # exit condition 4: return if information gain is 0\n",
    "    if np.allclose(lib.information_gain(train_y, new_ys, count_items), 0):\n",
    "        return\n",
    "\n",
    "    print(\"Left\",train_x_left.shape)\n",
    "    print(\"Right\",train_x_right.shape)\n",
    "    # make the recursion call for left\n",
    "    find_best_params(train_x_left, train_y_left, test_x, test_y, left_child_node_num)\n",
    "    # make the recursion call for right\n",
    "    find_best_params(train_x_right, train_y_right, test_x, test_y, right_child_node_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b0478526",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_number 0 data shape (381, 30)\n",
      "Left (191, 30)\n",
      "Right (190, 30)\n",
      "node_number 1 data shape (191, 30)\n",
      "Left (96, 30)\n",
      "Right (95, 30)\n",
      "node_number 3 data shape (96, 30)\n",
      "Left (48, 30)\n",
      "Right (48, 30)\n",
      "node_number 7 data shape (48, 30)\n",
      "Left (24, 30)\n",
      "Right (24, 30)\n",
      "node_number 15 data shape (24, 30)\n",
      "Left (12, 30)\n",
      "Right (12, 30)\n",
      "node_number 31 data shape (12, 30)\n",
      "node_number 32 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 65 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 131 data shape (3, 30)\n",
      "node_number 132 data shape (3, 30)\n",
      "node_number 66 data shape (6, 30)\n",
      "node_number 16 data shape (24, 30)\n",
      "node_number 8 data shape (48, 30)\n",
      "Left (24, 30)\n",
      "Right (24, 30)\n",
      "node_number 17 data shape (24, 30)\n",
      "Left (12, 30)\n",
      "Right (12, 30)\n",
      "node_number 35 data shape (12, 30)\n",
      "node_number 36 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 73 data shape (6, 30)\n",
      "node_number 74 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 149 data shape (3, 30)\n",
      "node_number 150 data shape (3, 30)\n",
      "node_number 18 data shape (24, 30)\n",
      "node_number 4 data shape (95, 30)\n",
      "Left (48, 30)\n",
      "Right (47, 30)\n",
      "node_number 9 data shape (48, 30)\n",
      "Left (24, 30)\n",
      "Right (24, 30)\n",
      "node_number 19 data shape (24, 30)\n",
      "Left (12, 30)\n",
      "Right (12, 30)\n",
      "node_number 39 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 79 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 159 data shape (3, 30)\n",
      "node_number 160 data shape (3, 30)\n",
      "node_number 80 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 161 data shape (3, 30)\n",
      "node_number 162 data shape (3, 30)\n",
      "node_number 40 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 81 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 163 data shape (3, 30)\n",
      "node_number 164 data shape (3, 30)\n",
      "node_number 82 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 165 data shape (3, 30)\n",
      "node_number 166 data shape (3, 30)\n",
      "node_number 20 data shape (24, 30)\n",
      "Left (12, 30)\n",
      "Right (12, 30)\n",
      "node_number 41 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 83 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 167 data shape (3, 30)\n",
      "node_number 168 data shape (3, 30)\n",
      "node_number 84 data shape (6, 30)\n",
      "node_number 42 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 85 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 171 data shape (3, 30)\n",
      "node_number 172 data shape (3, 30)\n",
      "node_number 86 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 173 data shape (3, 30)\n",
      "node_number 174 data shape (3, 30)\n",
      "node_number 10 data shape (47, 30)\n",
      "Left (24, 30)\n",
      "Right (23, 30)\n",
      "node_number 21 data shape (24, 30)\n",
      "Left (12, 30)\n",
      "Right (12, 30)\n",
      "node_number 43 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 87 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 175 data shape (3, 30)\n",
      "node_number 176 data shape (3, 30)\n",
      "node_number 88 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 177 data shape (3, 30)\n",
      "node_number 178 data shape (3, 30)\n",
      "node_number 44 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 89 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 179 data shape (3, 30)\n",
      "node_number 180 data shape (3, 30)\n",
      "node_number 90 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 181 data shape (3, 30)\n",
      "node_number 182 data shape (3, 30)\n",
      "node_number 22 data shape (23, 30)\n",
      "Left (12, 30)\n",
      "Right (11, 30)\n",
      "node_number 45 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 91 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 183 data shape (3, 30)\n",
      "node_number 184 data shape (3, 30)\n",
      "node_number 92 data shape (6, 30)\n",
      "node_number 46 data shape (11, 30)\n",
      "Left (6, 30)\n",
      "Right (5, 30)\n",
      "node_number 93 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 187 data shape (3, 30)\n",
      "node_number 188 data shape (3, 30)\n",
      "node_number 94 data shape (5, 30)\n",
      "node_number 2 data shape (190, 30)\n",
      "Left (95, 30)\n",
      "Right (95, 30)\n",
      "node_number 5 data shape (95, 30)\n",
      "Left (48, 30)\n",
      "Right (47, 30)\n",
      "node_number 11 data shape (48, 30)\n",
      "Left (24, 30)\n",
      "Right (24, 30)\n",
      "node_number 23 data shape (24, 30)\n",
      "Left (12, 30)\n",
      "Right (12, 30)\n",
      "node_number 47 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 95 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 191 data shape (3, 30)\n",
      "node_number 192 data shape (3, 30)\n",
      "node_number 96 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 193 data shape (3, 30)\n",
      "node_number 194 data shape (3, 30)\n",
      "node_number 48 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 97 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 195 data shape (3, 30)\n",
      "node_number 196 data shape (3, 30)\n",
      "node_number 98 data shape (6, 30)\n",
      "node_number 24 data shape (24, 30)\n",
      "Left (12, 30)\n",
      "Right (12, 30)\n",
      "node_number 49 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 99 data shape (6, 30)\n",
      "node_number 100 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 201 data shape (3, 30)\n",
      "node_number 202 data shape (3, 30)\n",
      "node_number 50 data shape (12, 30)\n",
      "node_number 12 data shape (47, 30)\n",
      "Left (24, 30)\n",
      "Right (23, 30)\n",
      "node_number 25 data shape (24, 30)\n",
      "Left (12, 30)\n",
      "Right (12, 30)\n",
      "node_number 51 data shape (12, 30)\n",
      "node_number 52 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 105 data shape (6, 30)\n",
      "node_number 106 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 213 data shape (3, 30)\n",
      "node_number 214 data shape (3, 30)\n",
      "node_number 26 data shape (23, 30)\n",
      "Left (12, 30)\n",
      "Right (11, 30)\n",
      "node_number 53 data shape (12, 30)\n",
      "Left (6, 30)\n",
      "Right (6, 30)\n",
      "node_number 107 data shape (6, 30)\n",
      "node_number 108 data shape (6, 30)\n",
      "Left (3, 30)\n",
      "Right (3, 30)\n",
      "node_number 217 data shape (3, 30)\n",
      "node_number 218 data shape (3, 30)\n",
      "node_number 54 data shape (11, 30)\n",
      "node_number 6 data shape (95, 30)\n"
     ]
    }
   ],
   "source": [
    "node_number=0\n",
    "find_best_params(X_train,y_train,X_test,y_test,node_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee05eb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381, 30)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a756014d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.24428801638120348,\n",
       " 0.23127227036685324,\n",
       " 0.6146814615649228,\n",
       " 0.16733147349345334,\n",
       " 0.4009775089100112,\n",
       " 0.45319997609265716,\n",
       " None,\n",
       " 0.16537900055496985,\n",
       " 0.4537087604955242,\n",
       " 0.4986349366438745,\n",
       " 0.4290101711212903,\n",
       " 0.412073340523574,\n",
       " 0.4741836258647849,\n",
       " None,\n",
       " None,\n",
       " 0.2281778959350223,\n",
       " None,\n",
       " 0.3572148409678031,\n",
       " None,\n",
       " 0.6172038038154812,\n",
       " 0.6474969413289741,\n",
       " 0.4473000954262027,\n",
       " 0.3792205801056552,\n",
       " 0.42294857158158894,\n",
       " 0.4152127406994054,\n",
       " 0.3980086508905547,\n",
       " 0.6348443421154096,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 0.6854060424493489,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 0.5515689783046172,\n",
       " None,\n",
       " None,\n",
       " 0.48803868651986126,\n",
       " 0.46742227333610087,\n",
       " 0.5740619704927561,\n",
       " 0.8324187861043983,\n",
       " 0.6557257561104665,\n",
       " 0.26571876549906676,\n",
       " 0.43500497726067705,\n",
       " 0.6089185601926372,\n",
       " 0.5253402772931999,\n",
       " 0.6663410916378765,\n",
       " 0.47426992541256985,\n",
       " None,\n",
       " None,\n",
       " 0.4065232645874627,\n",
       " 0.2883484325847628,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 0.5924691929550584,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 0.7280488134837786,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 0.10387780836598466,\n",
       " 0.456333697491508,\n",
       " 0.525525603593435,\n",
       " 0.7903716225292632,\n",
       " 0.29677579082405825,\n",
       " None,\n",
       " 0.6612526212394471,\n",
       " 0.6904334451979106,\n",
       " 0.10088594827079045,\n",
       " 0.7376425690825347,\n",
       " 0.29539606181066624,\n",
       " 0.24739822413351725,\n",
       " 0.7513899878425307,\n",
       " None,\n",
       " 0.6549615628128951,\n",
       " None,\n",
       " 0.6177999041420122,\n",
       " 0.1948659428821004,\n",
       " 0.4850011980221758,\n",
       " None,\n",
       " None,\n",
       " 0.4147086300547907,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 0.639412717661108,\n",
       " None,\n",
       " 0.3634730810295307,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_optimized_thresh_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cac47dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh=all_optimized_thresh_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec809eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
