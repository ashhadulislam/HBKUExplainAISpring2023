{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rd5bv7jDdxvx"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'HBKU'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/Colab Notebooks\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mHBKU\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlib\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'HBKU'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks')\n",
    "\n",
    "import HBKU as lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "f9AxsNtgPPRw"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Od_BtSmSdPUX"
   },
   "outputs": [],
   "source": [
    "#load the breast cancer dataset \n",
    "bcan = datasets.load_breast_cancer()\n",
    "X = bcan.data\n",
    "y = bcan.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "print(\"Shapr of training data is \",X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdBkiM0Zegov"
   },
   "outputs": [],
   "source": [
    "# initialize the params1 and params2\n",
    "params1 = []\n",
    "params2 = []\n",
    "\n",
    "for i in range(100):\n",
    "    # call the initialize_weights function above\n",
    "    wts = initialize_weights(num_feats)\n",
    "    params1.append(wts)\n",
    "\n",
    "    # use the np.random.uniform() function\n",
    "    th = np.random.uniform(res.min(), res.max())\n",
    "    params2.append(th)\n",
    "\n",
    "# we have a list of 100 weight vectors (params1) and 100 thresholds (params2)\n",
    "# convert them to array\n",
    "params1 = np.array(params1)\n",
    "params2 = np.array(params2)\n",
    "\n",
    "print(\"Shape of params 1 (weights)\", params1.shape)\n",
    "print(\"Shape of params 2 (thresholds)\", params2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRggAR1yfoNm"
   },
   "outputs": [],
   "source": [
    "# Find the global minimum\n",
    "min_idx = np.argmin(z)\n",
    "param1_min = params1[min_idx]\n",
    "param2_min = params2[min_idx]\n",
    "\n",
    "print(\"param1_min\",param1_min,\"param2_min\",param2_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLCwW7vpf8hO"
   },
   "outputs": [],
   "source": [
    "# Hyper-parameter of the algorithm\n",
    "c1 = c2 = 0.1\n",
    "w1 = np.array([np.random.uniform() for i in range(X_train.shape[1])])\n",
    "w2 = 0.8 \n",
    "# Create particles\n",
    "n_particles = 20\n",
    "np.random.seed(100)\n",
    "params1 = [lib.initialize_weights(X_train.shape[1]) for _ in range(n_particles)]\n",
    "params2 = np.random.uniform(np.min(res), np.max(res), size=n_particles)\n",
    "\n",
    "params1 = np.array(params1)\n",
    "params2 = np.array(params2)\n",
    "\n",
    "print(\"params1 shape is \", params1.shape, \"params2 shape is \", params2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUg_3N1xgRc1"
   },
   "outputs": [],
   "source": [
    "# define velocity of each weight of every particle\n",
    "V_param1 = np.array([lib.initialize_weights(X_train.shape[1]) for i in range(n_particles)])\n",
    "\n",
    "#define velocity of each threshold of every particle\n",
    "V_param2 = np.random.uniform(size=n_particles)\n",
    "\n",
    "# Initialize objective values\n",
    "pbest = (params1,params2)\n",
    "pbest_obj = lib.objective_fn_vector(params1, params2, X_train, y_train)\n",
    "gbest=(params1[pbest_obj.argmin()],params2[pbest_obj.argmin()])\n",
    "gbest_obj = pbest_obj.min()\n",
    "\n",
    "print(\"pbest obj value for 20 particles are as follows\",pbest_obj)\n",
    "print(\"gbest obj value among all 20 particles is as follows\",gbest_obj)\n",
    "# note that gbest_obj should be the minimim of all pbest_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7dBxF6sg8uO"
   },
   "outputs": [],
   "source": [
    "def update():\n",
    "    \"Function to do one iteration of particle swarm optimization\"\n",
    "    global V_param1,V_param2, params1,params2, pbest, pbest_obj, gbest, gbest_obj\n",
    "    # these have been already initialized in the previous cells\n",
    "    \n",
    "    # Update params\n",
    "    r11,r12, r2 = np.random.rand(3)\n",
    "    V_param1 = w1 * V_param1 + c1 * r11 * (pbest[0] - params1) + c2 * r2 * (gbest[0] - params1)\n",
    "    V_param2 = w2 * V_param2 + c1 * r12 * (pbest[1] - params2) + c2 * r2 * (gbest[1] - params2)\n",
    "    params1 = params1 + V_param1\n",
    "    params2 = params2 + V_param2\n",
    "    \n",
    "    obj = lib.objective_fn_vector(params1, params2, X_train, y_train)\n",
    "    for i in range(pbest[0].shape[0]):\n",
    "        if pbest_obj[i] >= obj[i]:\n",
    "            pbest[0][i] = params1[i]  # update pbest[0][i] with value of params1[i]\n",
    "            pbest[1][i] = params2[i]  # update pbest[1][i]\n",
    "            pbest_obj[i] = obj[i]     # also update pbest_obj[i]\n",
    "\n",
    "    gbest = (params1[pbest_obj.argmin()], params2[pbest_obj.argmin()])  # update gbest to contain the best from params1 and params 2\n",
    "    gbest_obj = pbest_obj.min()  # update gbest to get the minimum of pbest_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0zvqmeshfB-"
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    update()\n",
    "print(\"PSO found best solution at f({})={}\".format(gbest, gbest_obj))\n",
    "print(\"Global optimal at f({})={}\".format([param1_min,param2_min], lib.objective_fn(param1_min, param2_min, X_train, y_train)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
